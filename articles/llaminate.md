# Llaminate

## Intuition

## Roadmap

## Next

With `tokun`, we saw the value of creating a model specialized in text encoding.

Here `llaminate` is designed to learn the mechanics of language and human interactions.

Still by compressing `llama3` from 8B to 33M parameters the model lost capabilities.
This might be due to the hacky training it received.

Following the modular architecture of the previous 2 projects, I think it'd be interesting to try and train yet another model.
This one would be dedicated to knowledge, storing particular facts in yet other embeddings.
