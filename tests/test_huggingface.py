import unittest

import tokun.huggingface

# BYTE TOKENIZER ###############################################################

class ByteTokenizerTest(unittest.TestCase):
    def setUp(self):
        # samples without special tokens
        self._texts = [
            'Lexical tokenization is conversion of a text into (semantically or syntactically) meaningful lexical tokens belonging to categories defined by a "lexer" program.',
            # Korean
            b'\xeb\x82\xb1\xeb\xa7\x90 \xeb\xb6\x84\xec\x84\x9d(Lexical analysis)\xec\x9d\x80 \xec\xbb\xb4\xed\x93\xa8\xed\x84\xb0 \xea\xb3\xbc\xed\x95\x99\xec\x97\x90\xec\x84\x9c \xed\x94\x84\xeb\xa1\x9c\xea\xb7\xb8\xeb\x9e\x98\xeb\xb0\x8d \xec\x96\xb8\xec\x96\xb4\xeb\xa5\xbc \xec\xb2\x98\xeb\xa6\xac\xed\x95\x98\xeb\x8a\x94 \xec\xb2\x98\xeb\xa6\xac\xed\x95\x98\xeb\x8a\x94 \xec\xb5\x9c\xec\xb4\x88\xec\x9d\x98 \xeb\x8b\xa8\xea\xb3\x84\xec\x9d\xb4\xeb\x8b\xa4.'.decode('utf-8'),
            # Russian
            b'\xd0\x92 \xd0\xb8\xd0\xbd\xd1\x84\xd0\xbe\xd1\x80\xd0\xbc\xd0\xb0\xd1\x82\xd0\xb8\xd0\xba\xd0\xb5 \xd0\xbb\xd0\xb5\xd0\xba\xd1\x81\xd0\xb8\xd1\x87\xd0\xb5\xd1\x81\xd0\xba\xd0\xb8\xd0\xb9 \xd0\xb0\xd0\xbd\xd0\xb0\xd0\xbb\xd0\xb8\xd0\xb7 (\xc2\xab\xd1\x82\xd0\xbe\xd0\xba\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb7\xd0\xb0\xd1\x86\xd0\xb8\xd1\x8f\xc2\xbb, \xd0\xbe\xd1\x82 \xd0\xb0\xd0\xbd\xd0\xb3\xd0\xbb. tokenizing) \xe2\x80\x94 \xd0\xbf\xd1\x80\xd0\xbe\xd1\x86\xd0\xb5\xd1\x81\xd1\x81 \xd0\xb0\xd0\xbd\xd0\xb0\xd0\xbb\xd0\xb8\xd1\x82\xd0\xb8\xd1\x87\xd0\xb5\xd1\x81\xd0\xba\xd0\xbe\xd0\xb3\xd0\xbe \xd1\x80\xd0\xb0\xd0\xb7\xd0\xb1\xd0\xbe\xd1\x80\xd0\xb0 \xd0\xb2\xd1\x85\xd0\xbe\xd0\xb4\xd0\xbd\xd0\xbe\xd0\xb9 \xd0\xbf\xd0\xbe\xd1\x81\xd0\xbb\xd0\xb5\xd0\xb4\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x82\xd0\xb5\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8 \xd1\x81\xd0\xb8\xd0\xbc\xd0\xb2\xd0\xbe\xd0\xbb\xd0\xbe\xd0\xb2 \xd0\xbd\xd0\xb0 \xd1\x80\xd0\xb0\xd1\x81\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xbd\xd0\xb0\xd0\xbd\xd0\xbd\xd1\x8b\xd0\xb5 \xd0\xb3\xd1\x80\xd1\x83\xd0\xbf\xd0\xbf\xd1\x8b \xe2\x80\x94 \xd0\xbb\xd0\xb5\xd0\xba\xd1\x81\xd0\xb5\xd0\xbc\xd1\x8b \xe2\x80\x94 \xd1\x81 \xd1\x86\xd0\xb5\xd0\xbb\xd1\x8c\xd1\x8e \xd0\xbf\xd0\xbe\xd0\xbb\xd1\x83\xd1\x87\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8f \xd0\xbd\xd0\xb0 \xd0\xb2\xd1\x8b\xd1\x85\xd0\xbe\xd0\xb4\xd0\xb5 \xd0\xb8\xd0\xb4\xd0\xb5\xd0\xbd\xd1\x82\xd0\xb8\xd1\x84\xd0\xb8\xd1\x86\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xbd\xd1\x8b\xd1\x85 \xd0\xbf\xd0\xbe\xd1\x81\xd0\xbb\xd0\xb5\xd0\xb4\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x82\xd0\xb5\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xbe\xd1\x81\xd1\x82\xd0\xb5\xd0\xb9, \xd0\xbd\xd0\xb0\xd0\xb7\xd1\x8b\xd0\xb2\xd0\xb0\xd0\xb5\xd0\xbc\xd1\x8b\xd1\x85 \xc2\xab\xd1\x82\xd0\xbe\xd0\xba\xd0\xb5\xd0\xbd\xd0\xb0\xd0\xbc\xd0\xb8\xc2\xbb (\xd0\xbf\xd0\xbe\xd0\xb4\xd0\xbe\xd0\xb1\xd0\xbd\xd0\xbe \xd0\xb3\xd1\x80\xd1\x83\xd0\xbf\xd0\xbf\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xba\xd0\xb5 \xd0\xb1\xd1\x83\xd0\xba\xd0\xb2 \xd0\xb2 \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x85).'.decode('utf-8'),]
        # differentiate according to the text encoding
        self._utf8 = tokun.huggingface.ByteTokenizer(encoding='utf-8')
        self._utf32be = tokun.huggingface.ByteTokenizer(encoding='utf-32-be')

    def test_special_tokens(self):
        # UTF-8
        self.assertEqual(self._utf8.tokenize(self._utf8.bos_token), ['\x02'])
        self.assertEqual(self._utf8.tokenize(self._utf8.eos_token), ['\x03'])
        self.assertEqual(self._utf8.tokenize(self._utf8.unk_token), ['\x00'])
        self.assertEqual(self._utf8.tokenize(self._utf8.sep_token), ['\x1e'])
        self.assertEqual(self._utf8.tokenize(self._utf8.pad_token), ['\xc2', '\x80'])
        self.assertEqual(self._utf8.tokenize(self._utf8.cls_token), ['\x1d'])
        self.assertEqual(self._utf8.tokenize(self._utf8.mask_token), ['\x1a'])
        # UTF-32-BE
        self.assertEqual(self._utf32be.tokenize(self._utf32be.bos_token), ['\x00', '\x00', '\x00', '\x02'])
        self.assertEqual(self._utf32be.tokenize(self._utf32be.eos_token), ['\x00', '\x00', '\x00', '\x03'])
        self.assertEqual(self._utf32be.tokenize(self._utf32be.unk_token), ['\x00', '\x00', '\x00', '\x00'])
        self.assertEqual(self._utf32be.tokenize(self._utf32be.sep_token), ['\x00', '\x00', '\x00', '\x1e'])
        self.assertEqual(self._utf32be.tokenize(self._utf32be.pad_token), ['\x00', '\x00', '\x00', '\x80'])
        self.assertEqual(self._utf32be.tokenize(self._utf32be.cls_token), ['\x00', '\x00', '\x00', '\x1d'])
        self.assertEqual(self._utf32be.tokenize(self._utf32be.mask_token), ['\x00', '\x00', '\x00', '\x1a'])

    def test_tokenization_length(self):
        for __text in self._texts:
            # UTF-8 encodings have between 1 and 4 bytes per character
            self.assertGreaterEqual(len(self._utf8.tokenize(__text)), len(__text))
            self.assertLessEqual(len(self._utf8.tokenize(__text)), 4 * len(__text))
            # UTF-32-BE encodings are exactly 4 bytes per character
            self.assertEqual(len(self._utf32be.tokenize(__text)), 4 * len(__text))

    def test_tokenization_range(self):
        for __text in self._texts:
            __utf8_ids = self._utf8(__text, add_special_tokens=False)['input_ids']
            __utf32be_ids = self._utf32be(__text, add_special_tokens=False)['input_ids']
            self.assertEqual(all(0<= __i < 256 for __i in __utf8_ids), True)
            self.assertEqual(all(0<= __i < 256 for __i in __utf32be_ids), True)

    def test_reversibility(self):
        for __text in self._texts:
            __utf8_tokens = self._utf8.tokenize(__text)
            __utf8_string = self._utf8.convert_tokens_to_string(__utf8_tokens)
            __utf32be_tokens = self._utf32be.tokenize(__text)
            __utf32be_string = self._utf32be.convert_tokens_to_string(__utf32be_tokens)
            self.assertEqual(__utf8_string, __text)
            self.assertEqual(__utf32be_string, __text)
