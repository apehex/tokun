{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "yd0W4Xjwm6vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INSTALL DEPS ################################################################\n",
        "\n",
        "!pip install mlable tokun"
      ],
      "metadata": {
        "id": "l5GL5MtSnAOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Login To HF"
      ],
      "metadata": {
        "id": "1KBh-UVBw5Le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LOGIN HF ####################################################################\n",
        "\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "DXWIjQn6wO53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Dependencies"
      ],
      "metadata": {
        "id": "go9Tvqy0w3cF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD DEPS ###################################################################\n",
        "\n",
        "import itertools\n",
        "import math\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "import huggingface_hub as hh\n",
        "import tensorflow as tf\n",
        "import transformers as ht\n",
        "\n",
        "import mlable.metrics\n",
        "\n",
        "import tokun.evaluation\n",
        "import tokun.huggingface\n",
        "import tokun.meta\n",
        "import tokun.model\n",
        "import tokun.pipeline"
      ],
      "metadata": {
        "id": "yo18wu1bm6ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tensorflow version ' + tf.__version__)"
      ],
      "metadata": {
        "id": "sbjr53W8ots9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Setup the GPU / TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFIMfPmgQa0h"
      },
      "outputs": [],
      "source": [
        "# DEVICES #####################################################################\n",
        "\n",
        "tf.debugging.set_log_device_placement(False)\n",
        "\n",
        "CPU = tf.config.list_logical_devices('CPU')\n",
        "GPU = tf.config.list_logical_devices('GPU')\n",
        "TPU = tf.config.list_logical_devices('TPU')\n",
        "\n",
        "if TPU:\n",
        "    RESOLVER = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(RESOLVER)\n",
        "    tf.tpu.experimental.initialize_tpu_system(RESOLVER)\n",
        "    DISTRIBUTION_STRATEGY = tf.distribute.TPUStrategy(RESOLVER)\n",
        "elif GPU:\n",
        "    DISTRIBUTION_STRATEGY = tf.distribute.MirroredStrategy(GPU)\n",
        "else:\n",
        "    DISTRIBUTION_STRATEGY = tf.distribute.MirroredStrategy(CPU)\n",
        "\n",
        "print(DISTRIBUTION_STRATEGY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t1jfsJlM3SX"
      },
      "source": [
        "## Defining The Metadata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TOGGLE ######################################################################\n",
        "\n",
        "BINARY = True"
      ],
      "metadata": {
        "id": "90f2FFr6D8gT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z74MlibMWnu"
      },
      "outputs": [],
      "source": [
        "# META ########################################################################\n",
        "\n",
        "N_SEQUENCE_AXIS = 1\n",
        "N_FEATURE_AXIS = -1\n",
        "\n",
        "N_TOKEN_DIM = [4, 4, 4] # G, for each block\n",
        "N_INPUT_DIM = 256 # U_i (bytes)\n",
        "N_OUTPUT_DIM = 8 if BINARY else 256 # U_o (8 bits)\n",
        "N_EMBEDDING_DIM = 256 # E\n",
        "N_SEQUENCE_DIM = 512\n",
        "\n",
        "OUTPUT = 'binary' if BINARY else 'categorical'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jm6y63XRBz07"
      },
      "outputs": [],
      "source": [
        "# DERIVED #####################################################################\n",
        "\n",
        "N_TOKEN_SIZES = list(itertools.accumulate(N_TOKEN_DIM, lambda x, y: x * y)) # in bytes\n",
        "\n",
        "VERSION = tokun.meta.version(token_dim=N_TOKEN_DIM, sequence_axis=N_SEQUENCE_AXIS, input_dim=N_INPUT_DIM, embed_dim=N_EMBEDDING_DIM, output_dim=N_OUTPUT_DIM)\n",
        "LABEL = '5.7'\n",
        "\n",
        "URL_IMPORT = 'https://github.com/apehex/tokun/raw/main/models/{}/{}/{}/{}.keras'.format(*VERSION, LABEL)\n",
        "\n",
        "PATH_IMPORT = 'model.keras'\n",
        "PATH_EXPORT = os.path.join('variants/', *VERSION[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download The Model"
      ],
      "metadata": {
        "id": "RtDWNP3mixBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOWNLOAD ####################################################################\n",
        "\n",
        "urllib.request.urlretrieve(URL_IMPORT, PATH_IMPORT)"
      ],
      "metadata": {
        "id": "4cELIEuGi0kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEyFtkcFNGe4"
      },
      "source": [
        "## Init"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TOKENIZER ###################################################################\n",
        "\n",
        "TOKENIZER = tokun.huggingface.ByteTokenizer(vocab_size=256, split_special_tokens=True)"
      ],
      "metadata": {
        "id": "Nxl39csfe4Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# METRICS #####################################################################\n",
        "\n",
        "_Accuracy = mlable.metrics.BinaryGroupAccuracy if BINARY else mlable.metrics.CategoricalGroupAccuracy\n",
        "_Loss = tf.keras.losses.BinaryCrossentropy if BINARY else tf.keras.losses.CategoricalCrossentropy"
      ],
      "metadata": {
        "id": "yl-ydAonEZ8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEpY1-vFIFX7"
      },
      "outputs": [],
      "source": [
        "# MODEL #######################################################################\n",
        "\n",
        "with DISTRIBUTION_STRATEGY.scope():\n",
        "    # metrics\n",
        "    byte_accuracy = _Accuracy(group=1, name='byte_accuracy')\n",
        "    character_accuracy = _Accuracy(group=4, name='character_accuracy')\n",
        "    token_accuracy = _Accuracy(group=N_TOKEN_SIZES[-1], name='token_accuracy')\n",
        "    # weights and config\n",
        "    MODEL = tf.keras.models.load_model(PATH_IMPORT, compile=False)\n",
        "    # compilation\n",
        "    MODEL.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        loss=_Loss(from_logits=False, label_smoothing=0., axis=-1, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE, name='loss'),\n",
        "        metrics=[byte_accuracy, character_accuracy, token_accuracy])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SPECIFY IO ##################################################################\n",
        "\n",
        "__inputs = tf.keras.layers.Input(shape=(math.prod(N_TOKEN_DIM) * N_SEQUENCE_DIM,), dtype=tf.int32)\n",
        "\n",
        "__outputs = MODEL._encoder(__inputs)\n",
        "__outputs = MODEL._decoder(__outputs)\n",
        "\n",
        "TOKUN = tf.keras.models.Model(__inputs, __outputs)"
      ],
      "metadata": {
        "id": "oVe3E5yicnir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuRwWdjpPQBM"
      },
      "outputs": [],
      "source": [
        "MODEL.summary()\n",
        "TOKUN.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check The Model And Tokenizer"
      ],
      "metadata": {
        "id": "8-Yeu740yumv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SAMPLE ######################################################################\n",
        "\n",
        "__s = \"\"\"위키백과, 우리 모두의 백과사전.\\nt-분포 확률적 임베딩(t-SNE)은 데이터의 차원 축소에 사용되는 기계 학습 알고리즘 중 하나로, 2002년 샘 로이스Sam Rowise와 제프리 힌튼에 의해 개발되었다.[1] t-SNE는 비선형 차원 축소 기법으로, 고차원 데이터를 특히 2, 3차원 등으로 줄여 가시화하는데에 유용하게 사용된다. 구체적으로 t-SNE는 비슷한 데이터는 근접한 2, 3차원의 지점으로, 다른 데이터는 멀리 떨어진 지점으로 맵핑한다.\"\"\""
      ],
      "metadata": {
        "id": "hQreBRH1iWKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UTF-32 TOKENIZATION #########################################################\n",
        "\n",
        "__x = TOKENIZER.batch_encode_plus(batch_text_or_text_pairs=[__s], padding='max_length', max_length=math.prod(N_TOKEN_DIM) * N_SEQUENCE_DIM, add_special_tokens=False)\n",
        "__x = tf.convert_to_tensor(__x['input_ids'])"
      ],
      "metadata": {
        "id": "CwYTJVDoiR_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST THE DERIVED MODEL ######################################################\n",
        "\n",
        "__e = TOKUN.layers[1](__x) # encoder\n",
        "__p = TOKUN.layers[2](__e) # decoder\n",
        "__y = tokun.pipeline.postprocess(__p, binary=BINARY, random=False)\n",
        "__o = tokun.pipeline.unpack(__y)"
      ],
      "metadata": {
        "id": "gt5aryDpywC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokun.evaluation.compare(__s, __o[0]))\n",
        "print(__s)\n",
        "print(__o[0])"
      ],
      "metadata": {
        "id": "lfpjp5cTy4l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export"
      ],
      "metadata": {
        "id": "tL_tnWQQuBMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INIT HF API #################################################################\n",
        "\n",
        "API = hh.HfApi()"
      ],
      "metadata": {
        "id": "ScImWFJBFBZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TOKENIZER ###################################################################\n",
        "\n",
        "TOKENIZER.save_pretrained(save_directory='tokenizer/')\n",
        "API.upload_folder(repo_id='apehex/tokun', folder_path='tokenizer/', path_in_repo='tokenizer/')"
      ],
      "metadata": {
        "id": "CBC3Mq6qest2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL #######################################################################\n",
        "\n",
        "hh.save_pretrained_keras(model=TOKUN, save_directory='model/', config=TOKUN.get_config())\n",
        "API.upload_folder(repo_id='apehex/tokun', folder_path='model/', path_in_repo=PATH_EXPORT)"
      ],
      "metadata": {
        "id": "5Fggv5mit_rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import And Check"
      ],
      "metadata": {
        "id": "PuGUU0eAlICU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOWNLOAD REPO ###############################################################\n",
        "\n",
        "API.snapshot_download(repo_id='apehex/tokun', local_dir='tokun/')"
      ],
      "metadata": {
        "id": "thqBCsE1QcMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL #######################################################################\n",
        "\n",
        "__tokun = hh.from_pretrained_keras(os.path.join('tokun/', PATH_EXPORT))"
      ],
      "metadata": {
        "id": "yeRIelSywF-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TOKENIZER ###################################################################\n",
        "\n",
        "__tokenizer = tokun.huggingface.ByteTokenizer()"
      ],
      "metadata": {
        "id": "5QJOseZomAAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PREDICT #####################################################################\n",
        "\n",
        "__s = \"\"\"위키백과, 우리 모두의 백과사전.\\nt-분포 확률적 임베딩(t-SNE)은 데이터의 차원 축소에 사용되는 기계 학습 알고리즘 중 하나로, 2002년 샘 로이스Sam Rowise와 제프리 힌튼에 의해 개발되었다.[1] t-SNE는 비선형 차원 축소 기법으로, 고차원 데이터를 특히 2, 3차원 등으로 줄여 가시화하는데에 유용하게 사용된다. 구체적으로 t-SNE는 비슷한 데이터는 근접한 2, 3차원의 지점으로, 다른 데이터는 멀리 떨어진 지점으로 맵핑한다.\"\"\"\n",
        "\n",
        "__x = __tokenizer.batch_encode_plus(batch_text_or_text_pairs=[__s], padding='max_length', max_length=math.prod(N_TOKEN_DIM) * N_SEQUENCE_DIM, add_special_tokens=False)\n",
        "__x = tf.convert_to_tensor(__x['input_ids'])\n",
        "\n",
        "__p = __tokun(__x)\n",
        "__y = tokun.pipeline.postprocess(__p, binary=BINARY, random=False)\n",
        "__o = tokun.pipeline.unpack(__y)"
      ],
      "metadata": {
        "id": "Tn_tkm-dwR3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokun.evaluation.compare(__s, __o[0]))\n",
        "print(__s)\n",
        "print(__o[0])"
      ],
      "metadata": {
        "id": "iStrt6zuy1FK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}