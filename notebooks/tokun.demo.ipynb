{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import deps"
      ],
      "metadata": {
        "id": "yd0W4Xjwm6vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokun\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5GL5MtSnAOC",
        "outputId": "c7399cc4-44d3-44d8-d7ad-4834b55b34b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokun\n",
            "  Downloading tokun-0.3.6-py3-none-any.whl (12 kB)\n",
            "INFO: pip is looking at multiple versions of tokun to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading tokun-0.3.5-py3-none-any.whl (12 kB)\n",
            "  Downloading tokun-0.3.4-py3-none-any.whl (12 kB)\n",
            "Collecting mlable>=0.1.3 (from tokun)\n",
            "  Downloading mlable-0.1.3-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.14 in /usr/local/lib/python3.10/dist-packages (from tokun) (2.15.0)\n",
            "Requirement already satisfied: tensorflow-datasets>=4.9 in /usr/local/lib/python3.10/dist-packages (from tokun) (4.9.4)\n",
            "Requirement already satisfied: torch>=2.2 in /usr/local/lib/python3.10/dist-packages (from tokun) (2.2.0+cpu)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (1.63.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (2.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=4.9->tokun) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=4.9->tokun) (0.1.8)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=4.9->tokun) (1.7.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=4.9->tokun) (2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=4.9->tokun) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=4.9->tokun) (2.31.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=4.9->tokun) (1.15.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=4.9->tokun) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=4.9->tokun) (4.66.4)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=4.9->tokun) (0.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.2->tokun) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.2->tokun) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.2->tokun) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2->tokun) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.2->tokun) (2024.3.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.14->tokun) (0.43.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets>=4.9->tokun) (6.4.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets>=4.9->tokun) (3.18.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets>=4.9->tokun) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets>=4.9->tokun) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets>=4.9->tokun) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets>=4.9->tokun) (2024.2.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.14->tokun) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.14->tokun) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.14->tokun) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.14->tokun) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.14->tokun) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.2->tokun) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.2->tokun) (1.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.14->tokun) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.14->tokun) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.14->tokun) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.14->tokun) (2.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.14->tokun) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.14->tokun) (3.2.2)\n",
            "Installing collected packages: mlable, tokun\n",
            "Successfully installed mlable-0.1.3 tokun-0.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import functools\n",
        "import itertools\n",
        "import math\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import mlable.tensorflow.io\n",
        "import tokun.meta\n",
        "import tokun.model\n",
        "import tokun.pipeline"
      ],
      "metadata": {
        "id": "yo18wu1bm6ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tensorflow version \" + tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbjr53W8ots9",
        "outputId": "7cafdcba-4fe8-4a2e-cc51-5e3e5c7865ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version 2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Setup the GPU / TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFIMfPmgQa0h",
        "outputId": "f4df3967-af9e-4f35-a5b0-92d40a00173a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tensorflow.python.distribute.tpu_strategy.TPUStrategyV2 object at 0x7f8361e5ba60>\n"
          ]
        }
      ],
      "source": [
        "tf.debugging.set_log_device_placement(False)\n",
        "\n",
        "GPU = tf.config.list_logical_devices('GPU')\n",
        "TPU = tf.config.list_logical_devices('TPU')\n",
        "\n",
        "if TPU:\n",
        "    RESOLVER = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(RESOLVER)\n",
        "    tf.tpu.experimental.initialize_tpu_system(RESOLVER)\n",
        "    DISTRIBUTION_STRATEGY = tf.distribute.TPUStrategy(RESOLVER)\n",
        "elif GPU:\n",
        "    DISTRIBUTION_STRATEGY = tf.distribute.MirroredStrategy(GPU)\n",
        "\n",
        "print(DISTRIBUTION_STRATEGY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t1jfsJlM3SX"
      },
      "source": [
        "## Defining The Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z74MlibMWnu"
      },
      "outputs": [],
      "source": [
        "# PARAMETERS ##################################################################\n",
        "\n",
        "ACTIVATION = 'silu'\n",
        "ATTENTION = True\n",
        "NORMALIZATION = True\n",
        "\n",
        "N_TOKEN_DIM = [4, 4] # G, for each block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jm6y63XRBz07"
      },
      "outputs": [],
      "source": [
        "# DERIVED #####################################################################\n",
        "\n",
        "TOKEN_SIZES = list(itertools.accumulate(N_TOKEN_DIM, lambda x, y: x * y)) # in bytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfIZb86Fg0dQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3040b022-2a14-4913-f614-79ad3ecbdaf5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('model.keras', <http.client.HTTPMessage at 0x7f7d087b9f90>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# IMPORT ######################################################################\n",
        "\n",
        "VERSION = tokun.meta.version(groups=N_TOKEN_DIM, activation=ACTIVATION, attention=ATTENTION, normalization=NORMALIZATION)\n",
        "LABEL = '8.5'\n",
        "\n",
        "URL_IMPORT = 'https://github.com/apehex/tokun/raw/main/models/{}/{}/{}/{}/{}.keras'.format(*VERSION, LABEL)\n",
        "PATH_IMPORT = 'model.keras'\n",
        "\n",
        "urllib.request.urlretrieve(URL_IMPORT, PATH_IMPORT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-iGpXb15m83"
      },
      "outputs": [],
      "source": [
        "# SAMPLES #####################################################################\n",
        "\n",
        "SAMPLES = [\n",
        "    \"\"\"A variational autoencoder is a generative model with a prior and noise distribution respectively. Usually such models are trained using the expectation-maximization meta-algorithm (e.g. probabilistic PCA, (spike & slab) sparse coding). Such a scheme optimizes a lower bound of the data likelihood, which is usually intractable, and in doing so requires the discovery of q-distributions, or variational posteriors. These q-distributions are normally parameterized for each individual data point in a separate optimization process. However, variational autoencoders use a neural network as an amortized approach to jointly optimize across data points. This neural network takes as input the data points themselves, and outputs parameters for the variational distribution. As it maps from a known input space to the low-dimensional latent space, it is called the encoder.\"\"\",\n",
        "    \"\"\"Une unitÃ© lexicale ou token lexical ou plus simplement token est un couple composÃ© d'un nom et d'une valeur optionnelle (e.g. 135677).\"\"\",\n",
        "    \"\"\"class AutoEncoder(tf.keras.models.Model):\\n    def __init__(self, token_dim: int, encoding_dim: int, embedding_dim: int, latent_dim: int, batch_dim: int=None, **kwargs) -> None:\\n        super(AutoEncoder, self).__init__(**kwargs)\\n        self._encoder = Encoder(token_dim=token_dim, encoding_dim=encoding_dim, embedding_dim=embedding_dim, latent_dim=latent_dim, batch_dim=batch_dim)\\n        self._decoder = Decoder(token_dim=token_dim, encoding_dim=encoding_dim, embedding_dim=embedding_dim, latent_dim=latent_dim, batch_dim=batch_dim)\\n\\n    def call(self, x: tf.Tensor) -> tf.Tensor:\\n        return self._decoder(self._encoder(x))\"\"\",\n",
        "    \"\"\"class AutoEncoder(tf.keras.models.Model):\\n  def __init__(self, token_dim: int, encoding_dim: int, embedding_dim: int, latent_dim: int, batch_dim: int=None, **kwargs) -> None:\\n    super(AutoEncoder, self).__init__(**kwargs)\\n    self._encoder = Encoder(token_dim=token_dim, encoding_dim=encoding_dim, embedding_dim=embedding_dim, latent_dim=latent_dim, batch_dim=batch_dim)\\n    self._decoder = Decoder(token_dim=token_dim, encoding_dim=encoding_dim, embedding_dim=embedding_dim, latent_dim=latent_dim, batch_dim=batch_dim)\\n\\n  def call(self, x: tf.Tensor) -> tf.Tensor:\\n    return self._decoder(self._encoder(x))\"\"\",\n",
        "    \"\"\"ìœ„í‚¤ë°±ê³¼, ìš°ë¦¬ ëª¨ë‘ì˜ ë°±ê³¼ì‚¬ì „.\\nt-ë¶„í¬ í™•ë¥ ì  ì„ë² ë”©(t-SNE)ì€ ë°ì´í„°ì˜ ì°¨ì› ì¶•ì†Œì— ì‚¬ìš©ë˜ëŠ” ê¸°ê³„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜ë¡œ, 2002ë…„ ìƒ˜ ë¡œì´ìŠ¤Sam Rowiseì™€ ì œí”„ë¦¬ íŒíŠ¼ì— ì˜í•´ ê°œë°œë˜ì—ˆë‹¤.[1] t-SNEëŠ” ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ ê¸°ë²•ìœ¼ë¡œ, ê³ ì°¨ì› ë°ì´í„°ë¥¼ íŠ¹íˆ 2, 3ì°¨ì› ë“±ìœ¼ë¡œ ì¤„ì—¬ ê°€ì‹œí™”í•˜ëŠ”ë°ì— ìœ ìš©í•˜ê²Œ ì‚¬ìš©ëœë‹¤. êµ¬ì²´ì ìœ¼ë¡œ t-SNEëŠ” ë¹„ìŠ·í•œ ë°ì´í„°ëŠ” ê·¼ì ‘í•œ 2, 3ì°¨ì›ì˜ ì§€ì ìœ¼ë¡œ, ë‹¤ë¥¸ ë°ì´í„°ëŠ” ë©€ë¦¬ ë–¨ì–´ì§„ ì§€ì ìœ¼ë¡œ ë§µí•‘í•œë‹¤.\"\"\",\n",
        "    \"\"\"Auto-encodeur variationnel\\n\\nArticle\\nDiscussion\\nLire\\nModifier\\nModifier le code\\nVoir lâ€™historique\\n\\nOutils\\nEn apprentissage automatique, un auto-encodeur variationnel (ou VAE de l'anglais variational auto encoder)1, est une architecture de rÃ©seau de neurones artificiels introduite en 2013 par D. Kingma et M. Welling, appartenant aux familles des modÃ¨les graphiques probabilistes et des mÃ©thodes bayÃ©siennes variationnelles.\\n\\nLes VAE sont souvent rapprochÃ©s des autoencodeurs2,3 en raison de leur architectures similaires. Leur utilisation et leur formulation mathÃ©matiques sont cependant diffÃ©rentes.\\n\\nLes auto-encodeurs variationnels permettent de formuler un problÃ¨me d'infÃ©rence statistique (par exemple, dÃ©duire la valeur d'une variable alÃ©atoire Ã  partir d'une autre variable alÃ©atoire) en un problÃ¨me d'optimisation statistique (c'est-Ã -dire trouver les valeurs de paramÃ¨tres qui minimisent une fonction objectif)4. Ils reprÃ©sentent une fonction associant Ã  une valeur d'entrÃ©e une distribution latente multivariÃ©e, qui n'est pas directement observÃ©e mais dÃ©duite depuis un modÃ¨le mathÃ©matique Ã  partir de la distribution d'autres variables. Bien que ce type de modÃ¨le ait Ã©tÃ© initialement conÃ§u pour l'apprentissage non supervisÃ©5, son efficacitÃ© a Ã©tÃ© prouvÃ©e pour l'apprentissage semi-supervisÃ©6,7 et l'apprentissage supervisÃ©8.\\n\\nArchitecture\\nDans un VAE, les donnÃ©es d'entrÃ©e sont Ã©chantillonnÃ©es Ã  partir d'une distribution paramÃ©trÃ©e (la distribution a priori, en termes d'infÃ©rence bayÃ©sienne), et l'encodeur et le dÃ©codeur sont entraÃ®nÃ©s conjointement de sorte que la sortie minimise une erreur de reconstruction dans le sens de la divergence de Kullback-Leibler entre la distribution paramÃ©trique postÃ©rieure et la vraie distribution a posteriori9,10.\\n\\nFormulation\\n\\nLe schÃ©ma de base d'un auto-encodeur variationnel. Le modÃ¨le reÃ§oit \\nğ‘¥\\n{\\displaystyle \\mathbf {x} } comme entrÃ©e. L'encodeur le comprime dans l'espace latent. Le dÃ©codeur reÃ§oit en entrÃ©e les informations prÃ©levÃ©es dans l'espace latent et produit \\nğ‘¥\\nâ€²\\n{\\displaystyle \\mathbf {x'} } aussi semblable que possible Ã  \\nğ‘¥\\n{\\displaystyle \\mathbf {x} } .\\nOn note \\nğ‘¥\\n{\\displaystyle \\mathbf {x} } le vecteur contenant l'ensemble des variables observÃ©es que l'on souhaite modÃ©liser. Ce vecteur est une variable alÃ©atoire, caractÃ©risÃ© par une distribution de probabilitÃ© inconnue \\nğ‘ƒ\\n(\\nğ‘¥\\n)\\n{\\displaystyle P(\\mathbf {x} )}, que l'on souhaite approximer par une distribution paramÃ©trÃ©e \\nğ‘\\nğœƒ{\\displaystyle p_{\\theta }} ayant pour paramÃ¨tres \\nğœƒ{\\displaystyle \\theta }.\\n\\nOn introduit alors un vecteur alÃ©atoire \\nğ‘§\\n{\\displaystyle \\mathbf {z} } distribuÃ© conjointement avec \\nğ‘¥\\n{\\displaystyle \\mathbf {x} } (c'est-Ã -dire dont la loi de probabilitÃ© n'est pas indÃ©pendante de celle de \\nğ‘¥\\n{\\displaystyle \\mathbf {x} }). Ce vecteur \\nğ‘§\\n{\\displaystyle \\mathbf {z} } reprÃ©sente un encodage latent de \\nğ‘¥\\n{\\displaystyle \\mathbf {x} }, que l'on ne peut observer directement.\\n\\nOn exprime alors la distribution \\nğ‘\\nğœƒ{\\displaystyle p_{\\theta }} via la loi de probablitiÃ© marginale sur \\nğ‘§\\n{\\displaystyle \\mathbf {z} }, ce qui donne alors:\\n\\nğ‘\\nğœƒ\\n(\\nğ‘¥\\n)\\n=\\nâˆ«\\nğ‘§\\nğ‘\\nğœƒ\\n(\\nğ‘¥\\n,\\nğ‘§\\n)\\nğ‘‘\\nğ‘§\\n,\\n{\\displaystyle p_{\\theta }(\\mathbf {x} )=\\int _{\\mathbf {z} }p_{\\theta }(\\mathbf {x,z} )\\,d\\mathbf {z} ,}\\noÃ¹ \\nğ‘\\nğœƒ\\n(\\nğ‘¥\\n,\\nğ‘§\\n)\\n{\\displaystyle p_{\\theta }(\\mathbf {x,z} )} reprÃ©sente la distribution conjointe sous \\nğ‘\\nğœƒ{\\displaystyle p_{\\theta }} des donnÃ©es observables \\nğ‘¥\\n{\\displaystyle \\mathbf {x} } et de leur reprÃ©sentation latente \\nğ‘§\\n{\\displaystyle \\mathbf {z} }. Selon la formule des probabilitÃ©s composÃ©es, l'Ã©quation peut Ãªtre rÃ©Ã©crite comme\\n\\nğ‘\\nğœƒ\\n(\\nğ‘¥\\n)\\n=\\nâˆ«\\nğ‘§\\nğ‘\\nğœƒ\\n(\\nğ‘¥\\nâˆ£\\nğ‘§\\n)\\nğ‘\\nğœƒ\\n(\\nğ‘§\\n)\\nğ‘‘\\nğ‘§\\n{\\displaystyle p_{\\theta }(\\mathbf {x} )=\\int _{\\mathbf {z} }p_{\\theta }(\\mathbf {x\\mid z} )p_{\\theta }(\\mathbf {z} )\\,d\\mathbf {z} }\\nDans l'auto-encodeur variationnel classique, on fait l'hypothÃ¨se que \\nğ‘§\\n{\\displaystyle \\mathbf {z} } est un vecteur Ã  valeur rÃ©elles de dimension finie, et \\nğ‘\\nğœƒ\\n(\\nğ‘¥\\n|\\nğ‘§\\n)\\n{\\displaystyle p_{\\theta }(\\mathbf {x|z} )} suit une loi normale. Par consÃ©quent, \\nğ‘\\nğœƒ\\n(\\nğ‘¥\\n)\\n{\\displaystyle p_{\\theta }(\\mathbf {x} )} est un mÃ©lange de distributions gaussiennes.\\n\\nOn peut voir les relations entre les donnÃ©es d'entrÃ©e et leur reprÃ©sentation latente comme un problÃ¨me d'infÃ©rence bayÃ©sienne avec\\n\\nğ‘\\nğœƒ\\n(\\nğ‘§\\n)\\n{\\displaystyle p_{\\theta }(\\mathbf {z} )} reprÃ©sente la distribution probabilitÃ© a priori dans l'espace latent\\nğ‘\\nğœƒ\\n(\\nğ‘¥\\nâˆ£\\nğ‘§\\n)\\n{\\displaystyle p_{\\theta }(\\mathbf {x} \\mid \\mathbf {z} )} reprÃ©sente la vraisemblance\\nğ‘\\nğœƒ\\n(\\nğ‘§\\nâˆ£\\nğ‘¥\\n)\\n{\\displaystyle p_{\\theta }(\\mathbf {z} \\mid \\mathbf {x} )} reprÃ©sente la distribution de probabilitÃ© a posteriori\\nMalheureusement, le calcul de \\nğ‘\\nğœƒ\\n(\\nğ‘¥\\n)\\n{\\displaystyle p_{\\theta }(\\mathbf {x} )} est au mieux coÃ»teux, et dans la plupart des cas, impossible. Pour rÃ©soudre ce problÃ¨me, il est nÃ©cessaire d'introduire une autre fonction \\nğ‘\\nÎ¦{\\displaystyle q_{\\Phi }} pour approximer la distribution a posteriori :\\n\\nğ‘\\nÎ¦\\n(\\nğ‘§\\nâˆ£\\nğ‘¥\\n)\\nâ‰ˆ\\nğ‘\\nğœƒ\\n(\\nğ‘§\\nâˆ£\\nğ‘¥\\n)\\n{\\displaystyle q_{\\Phi }(\\mathbf {z\\mid x} )\\approx p_{\\theta }(\\mathbf {z\\mid x} )}\\noÃ¹ \\nÎ¦{\\displaystyle \\Phi } est l'ensemble des paramÃ¨tres de \\nğ‘\\n{\\displaystyle q} .\\n\\nAinsi le problÃ¨me est formulÃ© pour pouvoir Ãªtre appliquÃ© dans une architecture de rÃ©seau de neurones auto-encodeur, dans lequel la distribution de vraisemblance conditionnelle \\nğ‘\\nğœƒ\\n(\\nğ‘¥\\nâˆ£\\nğ‘§\\n)\\n{\\displaystyle p_{\\theta }(\\mathbf {x} \\mid \\mathbf {z} )} est reprÃ©sentÃ©e par un dÃ©codeur probabiliste, tandis que la distribution a posteriori approchÃ©e \\nğ‘\\nÎ¦\\n(\\nğ‘§\\nâˆ£\\nğ‘¥\\n)\\n{\\displaystyle q_{\\Phi }(\\mathbf {z} \\mid \\mathbf {x} )} est reprÃ©sentÃ©e par un codeur probabiliste. La mise en Å“uvre d'un VAE consistera donc Ã  calculer les valeurs optimales des paramÃ¨tres \\nğœƒ{\\displaystyle \\theta } et \\nÎ¦{\\displaystyle \\Phi } par un apprentissage automatique.\\n\\nFonction de perte ELBO\\nComme dans tout problÃ¨me d'apprentissage profond, il est nÃ©cessaire de dÃ©finir une fonction de perte diffÃ©rentiable afin de mettre Ã  jour les poids du rÃ©seau par rÃ©tropropagation lors de l'apprentissage.\\n\\nPour les auto-encodeurs variationnels, l'idÃ©e est de minimiser conjointement les paramÃ¨tres du modÃ¨le gÃ©nÃ©ratif \\nğœƒ{\\displaystyle \\theta } pour rÃ©duire l'erreur de reconstruction entre l'entrÃ©e et la sortie, et \\nÎ¦{\\displaystyle \\Phi } pour avoir \\nğ‘\\nÎ¦\\n(\\nğ‘§\\nâˆ£\\nğ‘¥\\n)\\n{\\displaystyle q_{\\Phi }(\\mathbf {z\\mid x} )}, la distribution postÃ©rieure approchÃ©e, le plus prÃ¨s possible de \\nğ‘\\nğœƒ\\n(\\nğ‘§\\nâˆ£\\nğ‘¥\\n)\\n{\\displaystyle p_{\\theta }(\\mathbf {z} \\mid \\mathbf {x} )}, la vraie distribution de probabilitÃ© a posteriori.\\n\\nComme fonction de coÃ»t pour la reconstruction, l'erreur quadratique moyenne et l'entropie croisÃ©e sont souvent utilisÃ©es.\\n\\nPour la fonction de coÃ»t de distance entre les deux distributions, la divergence inverse de Kullback â€“ Leibler \\nğ·\\nğ¾\\nğ¿\\n(\\nğ‘\\nÎ¦\\n(\\nğ‘§\\nâˆ£\\nğ‘¥\\n)\\nâˆ¥\\nğ‘\\nğœƒ\\n(\\nğ‘§\\nâˆ£\\nğ‘¥\\n)\\n)\\n{\\displaystyle D_{KL}(q_{\\Phi }(\\mathbf {z\\mid x} )\\parallel p_{\\theta }(\\mathbf {z\\mid x} ))} est un bon choix pour pousser \\nğ‘\\nÎ¦\\n(\\nğ‘§\\nâˆ£\\nğ‘¥\\n)\\n{\\displaystyle q_{\\Phi }(\\mathbf {z\\mid x} )} en dessous de \\nğ‘\\nğœƒ\\n(\\nğ‘§\\nâˆ£\\nğ‘¥\\n)\\n{\\displaystyle p_{\\theta }(\\mathbf {z} \\mid \\mathbf {x} )}1,11.\\n\\nReparamÃ©trisation\\n\\nLe schÃ©ma de l'astuce de reparamÃ©trisation. La variable alÃ©atoire \\nğœ€{\\displaystyle \\mathbf {\\varepsilon } } est injectÃ© dans l'espace latent, non observÃ©, \\nğ‘§\\n{\\displaystyle \\mathbf {z} } comme entrÃ©e externe. De cette maniÃ¨re, il est possible de rÃ©tropropager le gradient sans impliquer de variable stochastique lors de la mise Ã  jour.\\nPour rendre la formulation ELBO adaptÃ©e Ã  des fins d'apprentissage, il est nÃ©cessaire de modifier lÃ©gÃ¨rement la formulation du problÃ¨me et la structure du VAE12.\\n\\nL'Ã©chantillonnage stochastique est l'opÃ©ration non diffÃ©rentiable par laquelle il est possible d'Ã©chantillonner Ã  partir de l'espace latent et d'alimenter le dÃ©codeur probabiliste.\\n\\nL'hypothÃ¨se principale sur l'espace latent est qu'il peut Ãªtre considÃ©rÃ© comme un ensemble de distributions gaussiennes multivariÃ©es, et peut donc Ãªtre dÃ©crit comme\\n\\nğ‘§\\nâˆ¼\\nğ‘\\nÎ¦\\n(\\nğ‘§\\nâˆ£\\nğ‘¥\\n)\\n=\\nğ‘\\n(\\nğœ‡\\n,\\nğœ\\n2\\n)\\n{\\displaystyle \\mathbf {z} \\sim q_{\\Phi }(\\mathbf {z} \\mid \\mathbf {x} )={\\mathcal {N}}({\\boldsymbol {\\mu }},{\\boldsymbol {\\sigma }}^{2})} .\\n\\nLe schÃ©ma d'un auto-encodeur variationnel aprÃ¨s l'astuce de reparamÃ©trisation.\\nEtant donnÃ© \\nğœ€\\nâˆ¼\\nğ‘\\n(\\n0\\n,\\nğ¼\\n)\\n{\\displaystyle {\\boldsymbol {\\varepsilon }}\\sim {\\mathcal {N}}(0,{\\boldsymbol {I}})} et \\nâŠ™{\\displaystyle \\odot } dÃ©fini comme le produit Ã©lÃ©ment par Ã©lÃ©ment, l'astuce de reparamÃ©trisation modifie l'Ã©quation ci-dessus comme\\n\\nğ‘§\\n=\\nğœ‡\\n+\\nğœ\\nâŠ™\\nğœ€\\n.\\n{\\displaystyle \\mathbf {z} ={\\boldsymbol {\\mu }}+{\\boldsymbol {\\sigma }}\\odot {\\boldsymbol {\\varepsilon }}.}\\nGrÃ¢ce Ã  cette transformation (qui peut Ãªtre Ã©tendue Ã  des distributions non gaussiennes), le VAE devient entraÃ®nable et le codeur probabiliste doit apprendre Ã  mapper une reprÃ©sentation compressÃ©e de l'entrÃ©e dans les deux vecteurs latents \\nğœ‡{\\displaystyle {\\boldsymbol {\\mu }}} et \\nğœ{\\displaystyle {\\boldsymbol {\\sigma }}}, tandis que la stochasticitÃ© reste exclue du processus de mise Ã  jour et est injectÃ©e dans l'espace latent en tant qu'entrÃ©e externe via le vecteur alÃ©atoire \\nğœ€{\\displaystyle {\\boldsymbol {\\varepsilon }}} .\\n\\nRÃ©fÃ©rences\\nDiederik P. Kingma et Max Welling, Â« Auto-Encoding Variational Bayes Â», arXiv:1312.6114 [cs, stat],\\x200e 1er mai 2014 (lire en ligne [archive], consultÃ© le 1er juillet 2022)\\n(en) Kramer, Â« Nonlinear principal component analysis using autoassociative neural networks Â», AIChE Journal, vol. 37, no 2,\\x200e 1991, p. 233â€“243 (DOI 10.1002/aic.690370209, lire en ligne [archive])\\n(en) Hinton et Salakhutdinov, Â« Reducing the Dimensionality of Data with Neural Networks Â», Science, vol. 313, no 5786,\\x200e 28 juillet 2006, p. 504â€“507 (PMID 16873662, DOI 10.1126/science.1127647, Bibcode 2006Sci...313..504H, S2CID 1658773, lire en ligne [archive])\\n(en) Â« A Beginner's Guide to Variational Methods: Mean-Field Approximation [archive] Â», Eric Jang, 8 juillet 2016\\nWei-Ning Hsu, Yu Zhang et James Glass, 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), dÃ©cembre 2017, 16â€“23 p. (ISBN 978-1-5090-4788-8, DOI 10.1109/ASRU.2017.8268911, arXiv 1707.06265, S2CID 22681625), Â« Unsupervised domain adaptation for robust speech recognition via variational autoencoder-based data augmentation Â»\\nM. Ehsan Abbasnejad, Anthony Dick et Anton van den Hengel, Infinite Variational Autoencoder for Semi-Supervised Learning, 2017, 5888â€“5897 p. (lire en ligne [archive])\\n(en) Xu, Sun, Deng et Tan, Â« Variational Autoencoder for Semi-Supervised Text Classification Â», Proceedings of the AAAI Conference on Artificial Intelligence, vol. 31, no 1,\\x200e 12 fÃ©vrier 2017 (lire en ligne [archive])\\nKameoka, Li, Inoue et Makino, Â« Supervised Determined Source Separation with Multichannel Variational Autoencoder Â», Neural Computation, vol. 31, no 9,\\x200e 1er septembre 2019, p. 1891â€“1914 (PMID 31335290, DOI 10.1162/neco_a_01217, S2CID 198168155, lire en ligne [archive])\\nAn, J., & Cho, S. (2015). Variational autoencoder based anomaly detection using reconstruction probability. Special Lecture on IE, 2(1).\\nKingma et Welling, Â« An Introduction to Variational Autoencoders Â», Foundations and Trends in Machine Learning, vol. 12, no 4,\\x200e 2019, p. 307â€“392 (ISSN 1935-8237, DOI 10.1561/2200000056, arXiv 1906.02691, S2CID 174802445)\\n(en) Â« From Autoencoder to Beta-VAE [archive] Â», Lil'Log, 12 aoÃ»t 2018\\nBengio, Courville et Vincent, Â« Representation Learning: A Review and New Perspectives Â», IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no 8,\\x200e 2013, p. 1798â€“1828 (ISSN 1939-3539, PMID 23787338, DOI 10.1109/TPAMI.2013.50, arXiv 1206.5538, S2CID 393948, lire en ligne [archive])\"\"\",]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEyFtkcFNGe4"
      },
      "source": [
        "## Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEpY1-vFIFX7"
      },
      "outputs": [],
      "source": [
        "# INIT ########################################################################\n",
        "\n",
        "with DISTRIBUTION_STRATEGY.scope():\n",
        "    MODEL = tf.keras.models.load_model(PATH_IMPORT)\n",
        "    MODEL.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=0., axis=-1, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE, name='loss'),\n",
        "        metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuRwWdjpPQBM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b93305df-dcc8-4187-9929-dc8cefb079bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"auto_encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder (Encoder)           multiple                  593408    \n",
            "                                                                 \n",
            " decoder (Decoder)           multiple                  595200    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1188608 (4.53 MB)\n",
            "Trainable params: 1188608 (4.53 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "MODEL.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHtROW1K1R7c"
      },
      "source": [
        "## Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ENCODE ######################################################################\n",
        "\n",
        "__s = SAMPLES[1]\n",
        "__x = tokun.pipeline.preprocess(text=__s, groups=N_TOKEN_DIM, flatten=True)\n",
        "__e = MODEL._encoder(__x) # final embedding = input for another model"
      ],
      "metadata": {
        "id": "gw_bhpP6JX8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VIEW ########################################################################\n",
        "\n",
        "print(len(__s)) # original text length, roughly S\n",
        "print(__x.shape) # 4 * S, with padding\n",
        "print(__e.shape) # 4 * S // 64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5DINgnlMCwC",
        "outputId": "3569e1b1-5fff-42f6-b8b6-867f391cc915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "134\n",
            "(544, 256)\n",
            "(34, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detokenize"
      ],
      "metadata": {
        "id": "maFagKtSLgPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DECODE ######################################################################\n",
        "\n",
        "__p = MODEL._decoder(__e)\n",
        "__y = tokun.pipeline.postprocess(__p)"
      ],
      "metadata": {
        "id": "cLuBORzbMNcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('# INPUT ################################################################\\n\\n' + __s)\n",
        "print('\\n# OUTPUT ###############################################################\\n\\n' + __y)\n",
        "print('\\n# SCORE ################################################################\\n\\n' + str(tokun.pipeline.compare(__s, __y)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZkdNGWoMalz",
        "outputId": "2cacc8ac-7574-498b-9898-129c568eb1a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# INPUT ################################################################\n",
            "\n",
            "Une unitÃ© lexicale ou token lexical ou plus simplement token est un couple composÃ© d'un nom et d'une valeur optionnelle (e.g. 135677).\n",
            "\n",
            "# OUTPUT ###############################################################\n",
            "\n",
            "Une unitÃ© lexicale ou token lexical ou plus simplement token est un couple composÃ© d'un nom et d'une valeur optionnelle (e.g. 135677).\u0000\u0000\n",
            "\n",
            "# SCORE ################################################################\n",
            "\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Robustness"
      ],
      "metadata": {
        "id": "dLjTuH3IMyza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SAMPLE ######################################################################\n",
        "\n",
        "__x = tokun.pipeline.preprocess(\"\"\"Une unitÃ© lexicale ou token lexical ou plus simplement token est un couple composÃ© d'un nom et d'une valeur optionnelle (e.g. 135677).\"\"\", groups=N_TOKEN_DIM, flatten=True)\n",
        "__e = MODEL._encoder(__x)"
      ],
      "metadata": {
        "id": "-NRq3hZHM_6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ysd903RYTt8"
      },
      "outputs": [],
      "source": [
        "# NOISE #######################################################################\n",
        "\n",
        "__std = tf.math.reduce_std(__e, axis=0)\n",
        "__noise = tf.random.normal(shape=(256,), mean=0., stddev=tf.math.reduce_mean(__std).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DETOKENIZE ##################################################################\n",
        "\n",
        "print(tokun.pipeline.postprocess(MODEL._decoder(__e))) # original\n",
        "print(tokun.pipeline.postprocess(MODEL._decoder(__e + 0.8 * __std))) # with structured noise\n",
        "print(tokun.pipeline.postprocess(MODEL._decoder(__e + 1. * __noise))) # with random noise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKyqASAJs2xl",
        "outputId": "aeb66aa4-c44d-46e2-de03-c699bc6b9336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Une unitÃ© lexicale ou token lexical ou plus simplement token est un couple composÃ© d'un nom et d'une valeur optionnelle (e.g. 135677).\u0000\u0000\n",
            "Une unitÃ© lexicale ou token lexical ou plus simple\u0000ent token est un couple composÃ© d'un nom et d'une valeur optionnelle (e.g. 135677).\u0000\u0000\n",
            "Une unitÃ© lexicale ou token lexical ou plus simplement token est un couple composÃ© d'un nom et d'une vaÉ¬eur optionnelle (e.g. 135677).\u0000e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oa0x3nJKYbR2"
      },
      "outputs": [],
      "source": [
        "# ENCODE ######################################################################\n",
        "\n",
        "__s = SAMPLES[0]\n",
        "__x = tokun.pipeline.preprocess(text=__s, groups=N_TOKEN_DIM, flatten=True)\n",
        "__e = MODEL._encoder(__x) # final embedding = input for another model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DECODE ######################################################################\n",
        "\n",
        "__p = MODEL._decoder(__e)\n",
        "__y = tokun.pipeline.postprocess(__p)"
      ],
      "metadata": {
        "id": "ZJMxA5jE2TX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(__s)\n",
        "print(__y)\n",
        "print(tokun.pipeline.compare(__s, __y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkQ3QKAu72G1",
        "outputId": "e3ddec15-728b-400f-c14d-394a45582303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auto-encodeur variationnel\n",
            "\n",
            "Article\n",
            "Discussion\n",
            "Lire\n",
            "Modifier\n",
            "Modifier le code\n",
            "Voir lâ€™historique\n",
            "\n",
            "Outils\n",
            "En apprentissage automatique, un auto-encodeur variationnel (ou VAE de l'anglais variational auto encoder)1, est une architecture de rÃ©seau de neurones artificiels introduite en 2013 par D. Kingma et M. Welling, appartenant aux familles des modÃ¨les graphiques probabilistes et des mÃ©thodes bayÃ©siennes variationnelles.\n",
            "\n",
            "Les VAE sont souvent rapprochÃ©s des autoencodeurs2,3 en raison de leur architectures similaires. Leur utilisation et leur formulation mathÃ©matiques sont cependant diffÃ©rentes.\n",
            "\n",
            "Les auto-encodeurs variationnels permettent de formuler un problÃ¨me d'infÃ©rence statistique (par exemple, dÃ©duire la valeur d'une variable alÃ©atoire Ã  partir d'une autre variable alÃ©atoire) en un problÃ¨me d'optimisation statistique (c'est-Ã -dire trouver les valeurs de paramÃ¨tres qui minimisent une fonction objectif)4. Ils reprÃ©sentent une fonction associant Ã  une valeur d'entrÃ©e une distribution latente multivariÃ©e, qui n'est pas directement observÃ©e mais dÃ©duite depuis un modÃ¨le mathÃ©matique Ã  partir de la distribution d'autres variables. Bien que ce type de modÃ¨le ait Ã©tÃ© initialement conÃ§u pour l'apprentissage non supervisÃ©5, son efficacitÃ© a Ã©tÃ© prouvÃ©e pour l'apprentissage semi-supervisÃ©6,7 et l'apprentissage supervisÃ©8.\n",
            "\n",
            "Architecture\n",
            "Dans un VAE, les donnÃ©es d'entrÃ©e sont Ã©chantillonnÃ©es Ã  partir d'une distribution paramÃ©trÃ©e (la distribution a priori, en termes d'infÃ©rence bayÃ©sienne), et l'encodeur et le dÃ©codeur sont entraÃ®nÃ©s conjointement de sorte que la sortie minimise une erreur de reconstruction dans le sens de la divergence de Kullback-Leibler entre la distribution paramÃ©trique postÃ©rieure et la vraie distribution a posteriori9,10.\n",
            "\n",
            "Formulation\n",
            "\n",
            "Le schÃ©ma de base d'un auto-encodeur variationnel. Le modÃ¨le reÃ§oit \n",
            "ğ‘¥\n",
            "{\\displaystyle \\mathbf {x} } comme entrÃ©e. L'encodeur le comprime dans l'espace latent. Le dÃ©codeur reÃ§oit en entrÃ©e les informations prÃ©levÃ©es dans l'espace latent et produit \n",
            "ğ‘¥\n",
            "â€²\n",
            "{\\displaystyle \\mathbf {x'} } aussi semblable que possible Ã  \n",
            "ğ‘¥\n",
            "{\\displaystyle \\mathbf {x} } .\n",
            "On note \n",
            "ğ‘¥\n",
            "{\\displaystyle \\mathbf {x} } le vecteur contenant l'ensemble des variables observÃ©es que l'on souhaite modÃ©liser. Ce vecteur est une variable alÃ©atoire, caractÃ©risÃ© par une distribution de probabilitÃ© inconnue \n",
            "ğ‘ƒ\n",
            "(\n",
            "ğ‘¥\n",
            ")\n",
            "{\\displaystyle P(\\mathbf {x} )}, que l'on souhaite approximer par une distribution paramÃ©trÃ©e \n",
            "ğ‘\n",
            "ğœƒ{\\displaystyle p_{\theta }} ayant pour paramÃ¨tres \n",
            "ğœƒ{\\displaystyle \theta }.\n",
            "\n",
            "On introduit alors un vecteur alÃ©atoire \n",
            "ğ‘§\n",
            "{\\displaystyle \\mathbf {z} } distribuÃ© conjointement avec \n",
            "ğ‘¥\n",
            "{\\displaystyle \\mathbf {x} } (c'est-Ã -dire dont la loi de probabilitÃ© n'est pas indÃ©pendante de celle de \n",
            "ğ‘¥\n",
            "{\\displaystyle \\mathbf {x} }). Ce vecteur \n",
            "ğ‘§\n",
            "{\\displaystyle \\mathbf {z} } reprÃ©sente un encodage latent de \n",
            "ğ‘¥\n",
            "{\\displaystyle \\mathbf {x} }, que l'on ne peut observer directement.\n",
            "\n",
            "On exprime alors la distribution \n",
            "ğ‘\n",
            "ğœƒ{\\displaystyle p_{\theta }} via la loi de probablitiÃ© marginale sur \n",
            "ğ‘§\n",
            "{\\displaystyle \\mathbf {z} }, ce qui donne alors:\n",
            "\n",
            "ğ‘\n",
            "ğœƒ\n",
            "(\n",
            "ğ‘¥\n",
            ")\n",
            "=\n",
            "âˆ«\n",
            "ğ‘§\n",
            "ğ‘\n",
            "ğœƒ\n",
            "(\n",
            "ğ‘¥\n",
            ",\n",
            "ğ‘§\n",
            ")\n",
            "ğ‘‘\n",
            "ğ‘§\n",
            ",\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} )=\\int _{\\mathbf {z} }p_{\theta }(\\mathbf {x,z} )\\,d\\mathbf {z} ,}\n",
            "oÃ¹ \n",
            "ğ‘\n",
            "ğœƒ\n",
            "(\n",
            "ğ‘¥\n",
            ",\n",
            "ğ‘§\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x,z} )} reprÃ©sente la distribution conjointe sous \n",
            "ğ‘\n",
            "ğœƒ{\\displaystyle p_{\theta }} des donnÃ©es observables \n",
            "ğ‘¥\n",
            "{\\displaystyle \\mathbf {x} } et de leur reprÃ©sentation latente \n",
            "ğ‘§\n",
            "{\\displaystyle \\mathbf {z} }. Selon la formule des probabilitÃ©s composÃ©es, l'Ã©quation peut Ãªtre rÃ©Ã©crite comme\n",
            "\n",
            "ğ‘\n",
            "ğœƒ\n",
            "(\n",
            "ğ‘¥\n",
            ")\n",
            "=\n",
            "âˆ«\n",
            "ğ‘§\n",
            "ğ‘\n",
            "ğœƒ\n",
            "(\n",
            "ğ‘¥\n",
            "âˆ£\n",
            "ğ‘§\n",
            ")\n",
            "ğ‘\n",
            "ğœƒ\n",
            "(\n",
            "ğ‘§\n",
            ")\n",
            "ğ‘‘\n",
            "ğ‘§\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} )=\\int _{\\mathbf {z} }p_{\theta }(\\mathbf {x\\mid z} )p_{\theta }(\\mathbf {z} )\\,d\\mathbf {z} }\n",
            "Dans l'auto-encodeur variationnel classique, on fait l'hypothÃ¨se que \n",
            "ğ‘§\n",
            "{\\displaystyle \\mathbf {z} } est un vecteur Ã  valeur rÃ©elles de dimension finie, et \n",
            "ğ‘\n",
            "ğœƒ\n",
            "(\n",
            "ğ‘¥\n",
            "|\n",
            "ğ‘§\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x|z} )} suit une loi normale. Par consÃ©quent, \n",
            "ğ‘\n",
            "ğœƒ\n",
            "(\n",
            "ğ‘¥\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} )} est un mÃ©lange de distributions gaussiennes.\n",
            "\n",
            "On peut voir les relations entre les donnÃ©es d'entrÃ©e et leur reprÃ©sentation latente comme un problÃ¨me d'infÃ©rence bayÃ©sienne avec\n",
            "\n",
            "ğ‘\n",
            "ğœƒ\n",
            "(\n",
            "ğ‘§\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {z} )} reprÃ©sente la distribution probabilitÃ© a priori dans l'espace latent\n",
            "ğ‘\n",
            "ğœƒ\n",
            "(\n",
            "ğ‘¥\n",
            "âˆ£\n",
            "ğ‘§\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} \\mid \\mathbf {z} )} reprÃ©sente la vraisemblance\n",
            "ğ‘\n",
            "ğœƒ\n",
            "(\n",
            "ğ‘§\n",
            "âˆ£\n",
            "ğ‘¥\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {z} \\mid \\mathbf {x} )} reprÃ©sente la distribution de probabilitÃ© a posteriori\n",
            "Malheureusement, le calcul de \n",
            "ğ‘\n",
            "ğœƒ\n",
            "(\n",
            "ğ‘¥\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} )} est au mieux coÃ»teux, et dans la plupart des cas, impossible. Pour rÃ©soudre ce problÃ¨me, il est nÃ©cessaire d'introduire une autre fonction \n",
            "ğ‘\n",
            "Î¦{\\displaystyle q_{\\Phi }} pour approximer la distribution a posteriori :\n",
            "\n",
            "ğ‘\n",
            "Î¦\n",
            "(\n",
            "ğ‘§\n",
            "âˆ£\n",
            "ğ‘¥\n",
            ")\n",
            "â‰ˆ\n",
            "ğ‘\n",
            "ğœƒ\n",
            "(\n",
            "ğ‘§\n",
            "âˆ£\n",
            "ğ‘¥\n",
            ")\n",
            "{\\displaystyle q_{\\Phi }(\\mathbf {z\\mid x} )\u0007pprox p_{\theta }(\\mathbf {z\\mid x} )}\n",
            "oÃ¹ \n",
            "Î¦{\\displaystyle \\Phi } est l'ensemble des paramÃ¨tres de \n",
            "ğ‘\n",
            "{\\displaystyle q} .\n",
            "\n",
            "Ainsi le problÃ¨me est formulÃ© pour pouvoir Ãªtre appliquÃ© dans une architecture de rÃ©seau de neurones auto-encodeur, dans lequel la distribution de vraisemblance conditionnelle \n",
            "ğ‘\n",
            "ğœƒ\n",
            "(\n",
            "ğ‘¥\n",
            "âˆ£\n",
            "ğ‘§\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} \\mid \\mathbf {z} )} est reprÃ©sentÃ©e par un dÃ©codeur probabiliste, tandis que la distribution a posteriori approchÃ©e \n",
            "ğ‘\n",
            "Î¦\n",
            "(\n",
            "ğ‘§\n",
            "âˆ£\n",
            "ğ‘¥\n",
            ")\n",
            "{\\displaystyle q_{\\Phi }(\\mathbf {z} \\mid \\mathbf {x} )} est reprÃ©sentÃ©e par un codeur probabiliste. La mise en Å“uvre d'un VAE consistera donc Ã  calculer les valeurs optimales des paramÃ¨tres \n",
            "ğœƒ{\\displaystyle \theta } et \n",
            "Î¦{\\displaystyle \\Phi } par un apprentissage automatique.\n",
            "\n",
            "Fonction de perte ELBO\n",
            "Comme dans tout problÃ¨me d'apprentissage profond, il est nÃ©cessaire de dÃ©finir une fonction de perte diffÃ©rentiable afin de mettre Ã  jour les poids du rÃ©seau par rÃ©tropropagation lors de l'apprentissage.\n",
            "\n",
            "Pour les auto-encodeurs variationnels, l'idÃ©e est de minimiser conjointement les paramÃ¨tres du modÃ¨le gÃ©nÃ©ratif \n",
            "ğœƒ{\\displaystyle \theta } pour rÃ©duire l'erreur de reconstruction entre l'entrÃ©e et la sortie, et \n",
            "Î¦{\\displaystyle \\Phi } pour avoir \n",
            "ğ‘\n",
            "Î¦\n",
            "(\n",
            "ğ‘§\n",
            "âˆ£\n",
            "ğ‘¥\n",
            ")\n",
            "{\\displaystyle q_{\\Phi }(\\mathbf {z\\mid x} )}, la distribution postÃ©rieure approchÃ©e, le plus prÃ¨s possible de \n",
            "ğ‘\n",
            "ğœƒ\n",
            "(\n",
            "ğ‘§\n",
            "âˆ£\n",
            "ğ‘¥\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {z} \\mid \\mathbf {x} )}, la vraie distribution de probabilitÃ© a posteriori.\n",
            "\n",
            "Comme fonction de coÃ»t pour la reconstruction, l'erreur quadratique moyenne et l'entropie croisÃ©e sont souvent utilisÃ©es.\n",
            "\n",
            "Pour la fonction de coÃ»t de distance entre les deux distributions, la divergence inverse de Kullback â€“ Leibler \n",
            "ğ·\n",
            "ğ¾\n",
            "ğ¿\n",
            "(\n",
            "ğ‘\n",
            "Î¦\n",
            "(\n",
            "ğ‘§\n",
            "âˆ£\n",
            "ğ‘¥\n",
            ")\n",
            "âˆ¥\n",
            "ğ‘\n",
            "ğœƒ\n",
            "(\n",
            "ğ‘§\n",
            "âˆ£\n",
            "ğ‘¥\n",
            ")\n",
            ")\n",
            "{\\displaystyle D_{KL}(q_{\\Phi }(\\mathbf {z\\mid x} )\\parallel p_{\theta }(\\mathbf {z\\mid x} ))} est un bon choix pour pousser \n",
            "ğ‘\n",
            "Î¦\n",
            "(\n",
            "ğ‘§\n",
            "âˆ£\n",
            "ğ‘¥\n",
            ")\n",
            "{\\displaystyle q_{\\Phi }(\\mathbf {z\\mid x} )} en dessous de \n",
            "ğ‘\n",
            "ğœƒ\n",
            "(\n",
            "ğ‘§\n",
            "âˆ£\n",
            "ğ‘¥\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {z} \\mid \\mathbf {x} )}1,11.\n",
            "\n",
            "ReparamÃ©trisation\n",
            "\n",
            "Le schÃ©ma de l'astuce de reparamÃ©trisation. La variable alÃ©atoire \n",
            "ğœ€{\\displaystyle \\mathbf {\u000barepsilon } } est injectÃ© dans l'espace latent, non observÃ©, \n",
            "ğ‘§\n",
            "{\\displaystyle \\mathbf {z} } comme entrÃ©e externe. De cette maniÃ¨re, il est possible de rÃ©tropropager le gradient sans impliquer de variable stochastique lors de la mise Ã  jour.\n",
            "Pour rendre la formulation ELBO adaptÃ©e Ã  des fins d'apprentissage, il est nÃ©cessaire de modifier lÃ©gÃ¨rement la formulation du problÃ¨me et la structure du VAE12.\n",
            "\n",
            "L'Ã©chantillonnage stochastique est l'opÃ©ration non diffÃ©rentiable par laquelle il est possible d'Ã©chantillonner Ã  partir de l'espace latent et d'alimenter le dÃ©codeur probabiliste.\n",
            "\n",
            "L'hypothÃ¨se principale sur l'espace latent est qu'il peut Ãªtre considÃ©rÃ© comme un ensemble de distributions gaussiennes multivariÃ©es, et peut donc Ãªtre dÃ©crit comme\n",
            "\n",
            "ğ‘§\n",
            "âˆ¼\n",
            "ğ‘\n",
            "Î¦\n",
            "(\n",
            "ğ‘§\n",
            "âˆ£\n",
            "ğ‘¥\n",
            ")\n",
            "=\n",
            "ğ‘\n",
            "(\n",
            "ğœ‡\n",
            ",\n",
            "ğœ\n",
            "2\n",
            ")\n",
            "{\\displaystyle \\mathbf {z} \\sim q_{\\Phi }(\\mathbf {z} \\mid \\mathbf {x} )={\\mathcal {N}}({\boldsymbol {\\mu }},{\boldsymbol {\\sigma }}^{2})} .\n",
            "\n",
            "Le schÃ©ma d'un auto-encodeur variationnel aprÃ¨s l'astuce de reparamÃ©trisation.\n",
            "Etant donnÃ© \n",
            "ğœ€\n",
            "âˆ¼\n",
            "ğ‘\n",
            "(\n",
            "0\n",
            ",\n",
            "ğ¼\n",
            ")\n",
            "{\\displaystyle {\boldsymbol {\u000barepsilon }}\\sim {\\mathcal {N}}(0,{\boldsymbol {I}})} et \n",
            "âŠ™{\\displaystyle \\odot } dÃ©fini comme le produit Ã©lÃ©ment par Ã©lÃ©ment, l'astuce de reparamÃ©trisation modifie l'Ã©quation ci-dessus comme\n",
            "\n",
            "ğ‘§\n",
            "=\n",
            "ğœ‡\n",
            "+\n",
            "ğœ\n",
            "âŠ™\n",
            "ğœ€\n",
            ".\n",
            "{\\displaystyle \\mathbf {z} ={\boldsymbol {\\mu }}+{\boldsymbol {\\sigma }}\\odot {\boldsymbol {\u000barepsilon }}.}\n",
            "GrÃ¢ce Ã  cette transformation (qui peut Ãªtre Ã©tendue Ã  des distributions non gaussiennes), le VAE devient entraÃ®nable et le codeur probabiliste doit apprendre Ã  mapper une reprÃ©sentation compressÃ©e de l'entrÃ©e dans les deux vecteurs latents \n",
            "ğœ‡{\\displaystyle {\boldsymbol {\\mu }}} et \n",
            "ğœ{\\displaystyle {\boldsymbol {\\sigma }}}, tandis que la stochasticitÃ© reste exclue du processus de mise Ã  jour et est injectÃ©e dans l'espace latent en tant qu'entrÃ©e externe via le vecteur alÃ©atoire \n",
            "ğœ€{\\displaystyle {\boldsymbol {\u000barepsilon }}} .\n",
            "\n",
            "RÃ©fÃ©rences\n",
            "Diederik P. Kingma et Max Welling, Â« Auto-Encoding Variational Bayes Â», arXiv:1312.6114 [cs, stat], 0e 1er mai 2014 (lire en ligne [archive], consultÃ© le 1er juillet 2022)\n",
            "(en) Kramer, Â« Nonlinear principal component analysis using autoassociative neural networks Â», AIChE Journal, vol. 37, no 2, 0e 1991, p. 233â€“243 (DOI 10.1002/aic.690370209, lire en ligne [archive])\n",
            "(en) Hinton et Salakhutdinov, Â« Reducing the Dimensionality of Data with Neural Networks Â», Science, vol. 313, no 5786, 0e 28 juillet 2006, p. 504â€“507 (PMID 16873662, DOI 10.1126/science.1127647, Bibcode 2006Sci...313..504H, S2CID 1658773, lire en ligne [archive])\n",
            "(en) Â« A Beginner's Guide to Variational Methods: Mean-Field Approximation [archive] Â», Eric Jang, 8 juillet 2016\n",
            "Wei-Ning Hsu, Yu Zhang et James Glass, 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), dÃ©cembre 2017, 16â€“23 p. (ISBN 978-1-5090-4788-8, DOI 10.1109/ASRU.2017.8268911, arXiv 1707.06265, S2CID 22681625), Â« Unsupervised domain adaptation for robust speech recognition via variational autoencoder-based data augmentation Â»\n",
            "M. Ehsan Abbasnejad, Anthony Dick et Anton van den Hengel, Infinite Variational Autoencoder for Semi-Supervised Learning, 2017, 5888â€“5897 p. (lire en ligne [archive])\n",
            "(en) Xu, Sun, Deng et Tan, Â« Variational Autoencoder for Semi-Supervised Text Classification Â», Proceedings of the AAAI Conference on Artificial Intelligence, vol. 31, no 1, 0e 12 fÃ©vrier 2017 (lire en ligne [archive])\n",
            "Kameoka, Li, Inoue et Makino, Â« Supervised Determined Source Separation with Multichannel Variational Autoencoder Â», Neural Computation, vol. 31, no 9, 0e 1er septembre 2019, p. 1891â€“1914 (PMID 31335290, DOI 10.1162/neco_a_01217, S2CID 198168155, lire en ligne [archive])\n",
            "An, J., & Cho, S. (2015). Variational autoencoder based anomaly detection using reconstruction probability. Special Lecture on IE, 2(1).\n",
            "Kingma et Welling, Â« An Introduction to Variational Autoencoders Â», Foundations and Trends in Machine Learning, vol. 12, no 4, 0e 2019, p. 307â€“392 (ISSN 1935-8237, DOI 10.1561/2200000056, arXiv 1906.02691, S2CID 174802445)\n",
            "(en) Â« From Autoencoder to Beta-VAE [archive] Â», Lil'Log, 12 aoÃ»t 2018\n",
            "Bengio, Courville et Vincent, Â« Representation Learning: A Review and New Perspectives Â», IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no 8, 0e 2013, p. 1798â€“1828 (ISSN 1939-3539, PMID 23787338, DOI 10.1109/TPAMI.2013.50, arXiv 1206.5538, S2CID 393948, lire en ligne [archive])\n",
            "Auto-encodeur variationnel\n",
            "\n",
            "Article\n",
            "Discussion\n",
            "Lire\n",
            "Modifier\n",
            "Modifier le code\n",
            "Voir lâ€™historique\n",
            "\n",
            "Outils\n",
            "En apprentissage automatique, un auto-encodeur variationnel (ou VAE de l'anglais variational auto encoder)1, est une architecture de rÃ©seau de neurones artificiels introduite en 2013 par D. Kingma et M. Welling, appartenant aux familles des modÃ¨les graphiques probabilistes et des mÃ©thodes bayÃ©siennes variationnelles.\n",
            "\n",
            "Les VAE sont souvent rapprochÃ©s des autoencodeurs2,3 en raison de leur architectures similaires. Leur utilisation et leur formulation mathÃ©matiques sont cependant diffÃ©rentes.\n",
            "\n",
            "Les auto-encodeurs variationnels permettent de formuler un problÃ¨me d'infÃ©rence statistique (par exemple, dÃ©duire la valeur d'une variable alÃ©atoire Ã  partir d'une autre variable alÃ©atoire) en un problÃ¨me d'optimisation statistique (c'est-Ã -dire trouver les valeurs de paramÃ¨tres qui minimisent une fonction objectif)4. Ils reprÃ©sentent une fonction associant Ã  une valeur d'entrÃ©e une distribution latente multivariÃ©e, qui n'est pas directement observÃ©e mais dÃ©duite depuis un modÃ¨le mathÃ©matique Ã  partir de la distribution d'autres variables. Bien que ce type de modÃ¨le ait Ã©tÃ© initialement conÃ§u pour l'apprentissage non supervisÃ©5, son efficacitÃ© a Ã©tÃ© prouvÃ©e pour l'apprentissage semi-supervisÃ©6,7 et l'apprentissage supervisÃ©8.\n",
            "\n",
            "Architecture\n",
            "Dans un VAE, les donnÃ©es d'entrÃ©e sont Ã©chantillonnÃ©es Ã  partir d'une distribution paramÃ©trÃ©e (la distribution a priori, en termes d'infÃ©rence bayÃ©sienne), et l'encodeur et le dÃ©codeur sont entraÃ®nÃ©s conjointement de sorte que la sortie minimise une erreur de reconstruction dans le sens de la divergence de Kullback-Leibler entre la distribution paramÃ©trique postÃ©rieure et la vraie distribution a posteriori9,10.\n",
            "\n",
            "Formulation\n",
            "\n",
            "Le schÃ©ma de base d'un auto-encodeur variationnel. Le modÃ¨le reÃ§oit \n",
            "è¥\n",
            "{\\displaystyle \\mathbf {x} } comme entrÃ©e. L'encodeur le comprime dans l'espace latent. Le dÃ©codeur reÃ§oit en entrÃ©e les informations prÃ©levÃ©es dans l'espace latent et produit \n",
            "è¥\n",
            "â€²\n",
            "{\\displaystyle \\mathbf {x'} } aussi semblable que possible Ã  \n",
            "è¥\n",
            "{\\displaystyle \\mathbf {x} } .\n",
            "On note \n",
            "è¥\n",
            "{\\displaystyle \\mathbf {x} } le vecteur contenant l'ensemble des variables observÃ©es que l'on souhaite modÃ©liser. Ce vecteur est une variable alÃ©atoire, caractÃ©risÃ© par une distribution de probabilitÃ© inconnue \n",
            "èƒ\n",
            "(\n",
            "è¥\n",
            ")\n",
            "{\\displaystyle P(\\mathbf {x} )}, que l'on souhaite approximer par une distribution paramÃ©trÃ©e \n",
            "åµ\n",
            "æ¤ƒ{\\displaystyle p_{\theta }} ayant pour paramÃ¨tres \n",
            "è˜ƒ{\\displaystyle \theta }.\n",
            "\n",
            "On introduit alors un vecteur alÃ©atoire \n",
            "è§\n",
            "{\\displaystyle \\mathbf {z} } distribuÃ© conjointement avec \n",
            "è¥\n",
            "{\\displaystyle \\mathbf {x} } (c'est-Ã -dire dont la loi de probabilitÃ© n'est pas indÃ©pendante de celle de \n",
            "è¥\n",
            "{\\displaystyle \\mathbf {x} }). Ce vecteur \n",
            "è§\n",
            "{\\displaystyle \\mathbf {z} } reprÃ©sente un encodage latent de \n",
            "ä¹¥\n",
            "{\\displaystyle \\mathbf {x} }, que l'on ne peut observer directement.\n",
            "\n",
            "On exprime alors la distribution \n",
            "è\n",
            "æ¤ƒ{\\displaystyle p_{\theta }} via la loi de probablitiÃ© marginale sur \n",
            "è§\n",
            "{\\displaystyle \\mathbf {z} }, ce qui donne alors:\n",
            "\n",
            "è\n",
            "æ¤ƒ\n",
            "(\n",
            "ä¹¥\n",
            ")\n",
            "=\n",
            "ç«\n",
            "ä¹§\n",
            "è\n",
            "æ¤ƒ\n",
            "(\n",
            "ä¹¥\n",
            ",\n",
            "ä¹§\n",
            ")\n",
            "ä¹‘\n",
            "è§\n",
            ",\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} )=\\int _{\\mathbf {z} }p_{\theta }(\\mathbf {x,z} )\\,d\\mathbf {z} ,}\n",
            "oÃ¹ \n",
            "è\n",
            "æ¤ƒ\n",
            "(\n",
            "ä¹¥\n",
            ",\n",
            "ä¹§\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x,z} )} reprÃ©sente la distribution conjointe sous \n",
            "è\n",
            "æ¤ƒ{\\displaystyle p_{\theta }} des donnÃ©es observables \n",
            "è¥\n",
            "{\\displaystyle \\mathbf {x} } et de leur reprÃ©sentation latente \n",
            "è§\n",
            "{\\displaystyle \\mathbf {z} }. Selon la formule des probabilitÃ©s composÃ©es, l'Ã©quation peut Ãªtre rÃ©Ã©crite comme\n",
            "\n",
            "è\n",
            "æ¤ƒ\n",
            "(\n",
            "è¥\n",
            ")\n",
            "=\n",
            "âˆ«\n",
            "è§\n",
            "è\n",
            "æ¤ƒ\n",
            "(\n",
            "è¥\n",
            "âˆ£\n",
            "è§\n",
            ")\n",
            "ä¹\n",
            "é¤ƒ\n",
            "(\n",
            "è§\n",
            ")\n",
            "é™‘\n",
            "ä¹§\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} )=\\int _{\\mathbf {z} }p_{\theta }(\\mathbf {x\\mid z} )p_{\theta }(\\mathbf {z} )\\,d\\mathbf {z} }\n",
            "Dans l'auto-encodeur variationnel classique, on fait l'hypothÃ¨se que \n",
            "è§\n",
            "{\\displaystyle \\mathbf {z} } est un vecteur Ã  valeur rÃ©elles de dimension finie, et \n",
            "è\n",
            "æ¤ƒ\n",
            "(\n",
            "è¥\n",
            "|\n",
            "ä¹§\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x|z} )} suit une loi normale. Par consÃ©quent, \n",
            "è\n",
            "æ¤ƒ\n",
            "(\n",
            "è¥\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} )} est un mÃ©lange de distributions gaussiennes.\n",
            "\n",
            "On peut voir les relations entre les donnÃ©es d'entrÃ©e et leur reprÃ©sentation latente comme un problÃ¨me d'infÃ©rence bayÃ©sienne avec\n",
            "\n",
            "è\n",
            "æ¤ƒ\n",
            "(\n",
            "ä¹§\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {z} )} reprÃ©sente la distribution probabilitÃ© a priori dans l'espace latent\n",
            "è\n",
            "æ¤ƒ\n",
            "(\n",
            "ä¹¥\n",
            "âˆ£\n",
            "ä¹§\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} \\mid \\mathbf {z} )} reprÃ©sente la vraisemblance\n",
            "è\n",
            "æ¤ƒ\n",
            "(\n",
            "ä¹§\n",
            "âˆ£\n",
            "ä¹¥\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {z} \\mid \\mathbf {x} )} reprÃ©sente la distribution de probabilitÃ© a posteriori\n",
            "Malheureusement, le calcul de \n",
            "ä¹\n",
            "å¤ƒ\n",
            "(\n",
            "è¥\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} )} est au mieux coÃ»teux, et dans la plupart des cas, impossible. Pour rÃ©soudre ce problÃ¨me, il est nÃ©cessaire d'introduire une autre fonction \n",
            "è\n",
            "Î¦{\\displaystyle q_{\\Phi }} pour approximer la distribution a posteriori :\n",
            "\n",
            "è\n",
            "Î¦\n",
            "(\n",
            "è§\n",
            "âˆ£\n",
            "è¥\n",
            ")\n",
            "â‰ˆ\n",
            "ä¹\n",
            "å¤ƒ\n",
            "(\n",
            "è§\n",
            "âˆ£\n",
            "è¥\n",
            ")\n",
            "{\\displaystyle q_{\\Phi }(\\mathbf {z\\mid x} )\u0007pprox p_{\theta }(\\mathbf {z\\mid x} )}\n",
            "oÃ¹ \n",
            "Î¦{\\displaystyle \\Phi } est l'ensemble des paramÃ¨tres de \n",
            "è\n",
            "{\\displaystyle q} .\n",
            "\n",
            "Ainsi le problÃ¨me est formulÃ© pour pouvoir Ãªtre appliquÃ© dans une architecture de rÃ©seau de neurones auto-encodeur, dans lequel la distribution de vraisemblance conditionnelle \n",
            "è\n",
            "æ¤ƒ\n",
            "(\n",
            "ä¹¥\n",
            "âˆ£\n",
            "ä¹§\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} \\mid \\mathbf {z} )} est reprÃ©sentÃ©e par un dÃ©codeur probabiliste, tandis que la distribution a posteriori approchÃ©e \n",
            "è\n",
            "Î¦\n",
            "(\n",
            "è§\n",
            "âˆ£\n",
            "è¥\n",
            ")\n",
            "{\\displaystyle q_{\\Phi }(\\mathbf {z} \\mid \\mathbf {x} )} est reprÃ©sentÃ©e par un codeur probabiliste. La mise en Å“uvre d'un VAE consistera donc Ã  calculer les valeurs optimales des paramÃ¨tres \n",
            "æ¤ƒ{\\displaystyle \theta } et \n",
            "Î¦{\\displaystyle \\Phi } par un apprentissage automatique.\n",
            "\n",
            "Fonction de perte ELBO\n",
            "Comme dans tout problÃ¨me d'apprentissage profond, il est nÃ©cessaire de dÃ©finir une fonction de perte diffÃ©rentiable afin de mettre Ã  jour les poids du rÃ©seau par rÃ©tropropagation lors de l'apprentissage.\n",
            "\n",
            "Pour les auto-encodeurs variationnels, l'idÃ©e est de minimiser conjointement les paramÃ¨tres du modÃ¨le gÃ©nÃ©ratif \n",
            "é¤ƒ{\\displaystyle \theta } pour rÃ©duire l'erreur de reconstruction entre l'entrÃ©e et la sortie, et \n",
            "Î¦{\\displaystyle \\Phi } pour avoir \n",
            "è\n",
            "Î¦\n",
            "(\n",
            "è§\n",
            "âˆ£\n",
            "è¥\n",
            ")\n",
            "{\\displaystyle q_{\\Phi }(\\mathbf {z\\mid x} )}, la distribution postÃ©rieure approchÃ©e, le plus prÃ¨s possible de \n",
            "è\n",
            "æ¤ƒ\n",
            "(\n",
            "ä¹§\n",
            "âˆ£\n",
            "ä¹¥\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {z} \\mid \\mathbf {x} )}, la vraie distribution de probabilitÃ© a posteriori.\n",
            "\n",
            "Comme fonction de coÃ»t pour la reconstruction, l'erreur quadratique moyenne et l'entropie croisÃ©e sont souvent utilisÃ©es.\n",
            "\n",
            "Pour la fonction de coÃ»t de distance entre les deux distributions, la divergence inverse de Kullback â€“ Leibler \n",
            "è€·\n",
            "ä¸¾\n",
            "è€¿\n",
            "(\n",
            "è\n",
            "Î¦\n",
            "(\n",
            "ä¹§\n",
            "âˆ£\n",
            "ä¹¥\n",
            ")\n",
            "âˆ¥\n",
            "è\n",
            "æ¤ƒ\n",
            "(\n",
            "ä¹§\n",
            "âˆ£\n",
            "ä¹¥\n",
            ")\n",
            ")\n",
            "{\\displaystyle D_{KL}(q_{\\Phi }(\\mathbf {z\\mid x} )\\parallel p_{\theta }(\\mathbf {z\\mid x} ))} est un bon choix pour pousser \n",
            "è\n",
            "Î¦\n",
            "(\n",
            "è§\n",
            "âˆ£\n",
            "è¥\n",
            ")\n",
            "{\\displaystyle q_{\\Phi }(\\mathbf {z\\mid x} )} en dessous de \n",
            "è\n",
            "æ¤ƒ\n",
            "(\n",
            "ä¹§\n",
            "âˆ£\n",
            "ä¹¥\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {z} \\mid \\mathbf {x} )}1,11.\n",
            "\n",
            "ReparamÃ©trisation\n",
            "\n",
            "Le schÃ©ma de l'astuce de reparamÃ©trisation. La variable alÃ©atoire \n",
            "æœš{\\displaystyle \\mathbf {\u000barepsilon } } est injectÃ© dans l'espace latent, non observÃ©, \n",
            "è§\n",
            "{\\displaystyle \\mathbf {z} } comme entrÃ©e externe. De cette maniÃ¨re, il est possible de rÃ©tropropager le gradient sans impliquer de variable stochastique lors de la mise Ã  jour.\n",
            "Pour rendre la formulation ELBO adaptÃ©e Ã  des fins d'apprentissage, il est nÃ©cessaire de modifier lÃ©gÃ¨rement la formulation du problÃ¨me et la structure du VAE12.\n",
            "\n",
            "L'Ã©chantillonnage stochastique est l'opÃ©ration non diffÃ©rentiable par laquelle il est possible d'Ã©chantillonner Ã  partir de l'espace latent et d'alimenter le dÃ©codeur probabiliste.\n",
            "\n",
            "L'hypothÃ¨se principale sur l'espace latent est qu'il peut Ãªtre considÃ©rÃ© comme un ensemble de distributions gaussiennes multivariÃ©es, et peut donc Ãªtre dÃ©crit comme\n",
            "\n",
            "è¹§\n",
            "âˆ¼\n",
            "è\n",
            "Î¦\n",
            "(\n",
            "è§\n",
            "âˆ£\n",
            "è¥\n",
            ")\n",
            "=\n",
            "é™\n",
            "(\n",
            "ç¨‡\n",
            ",\n",
            "æ¤\n",
            "2\n",
            ")\n",
            "{\\displaystyle \\mathbf {z} \\sim q_{\\Phi }(\\mathbf {z} \\mid \\mathbf {x} )={\\mathcal {N}}({\boldsymbol {\\mu }},{\boldsymbol {\\sigma }}^{2})} .\n",
            "\n",
            "Le schÃ©ma d'un auto-encodeur variationnel aprÃ¨s l'astuce de reparamÃ©trisation.\n",
            "Etant donnÃ© \n",
            "æ¦¤\n",
            "âˆ¼\n",
            "è\n",
            "(\n",
            "0\n",
            ",\n",
            "è€¼\n",
            ")\n",
            "{\\displaystyle {\boldsymbol {\u000barepsilon }}\\sim {\\mathcal {N}}(0,{\boldsymbol {I}})} et \n",
            "âŠ™{\\displaystyle \\odot } dÃ©fini comme le produit Ã©lÃ©ment par Ã©lÃ©ment, l'astuce de reparamÃ©trisation modifie l'Ã©quation ci-dessus comme\n",
            "\n",
            "ä¹§\n",
            "=\n",
            "ä¸‡\n",
            "+\n",
            "æ¤\n",
            "âŠ™\n",
            "ä»‹\n",
            ".\n",
            "{\\displaystyle \\mathbf {z} ={\boldsymbol {\\mu }}+{\boldsymbol {\\sigma }}\\odot {\boldsymbol {\u000barepsilon }}.}\n",
            "GrÃ¢ce Ã  cette transformation (qui peut Ãªtre Ã©tendue Ã  des distributions non gaussiennes), le VAE devient entraÃ®nable et le codeur probabiliste doit apprendre Ã  mapper une reprÃ©sentation compressÃ©e de l'entrÃ©e dans les deux vecteurs latents \n",
            "ä¸‡{\\displaystyle {\boldsymbol {\\mu }}} et \n",
            "æ¤{\\displaystyle {\boldsymbol {\\sigma }}}, tandis que la stochasticitÃ© reste exclue du processus de mise Ã  jour et est injectÃ©e dans l'espace latent en tant qu'entrÃ©e externe via le vecteur alÃ©atoire \n",
            "æœš{\\displaystyle {\boldsymbol {\u000barepsilon }}} .\n",
            "\n",
            "RÃ©fÃ©rences\n",
            "Diederik P. Kingma et Max Welling, Â« Auto-Encoding Variational Bayes Â», arXiv:1312.6114 [cs, stat], 0e 1er mai 2014 (lire en ligne [archive], consultÃ© le 1er juillet 2022)\n",
            "(en) Kramer, Â« Nonlinear principal component analysis using autoassociative neural networks Â», AIChE Journal, vol. 37, no 2, 0e 1991, p. 233â€“243 (DOI 10.1002/aic.690370209, lire en ligne [archive])\n",
            "(en) Hinton et Salakhutdinov, Â« Reducing the Dimensionality of Data with Neural Networks Â», Science, vol. 313, no 5786, 0e 28 juillet 2006, p. 504â€“507 (PMID 16873662, DOI 10.1126/science.1127647, Bibcode 2006Sci...313..504H, S2CID 1658773, lire en ligne [archive])\n",
            "(en) Â« A Beginner's Guide to Variational Methods: Mean-Field Approximation [archive] Â», Eric Jang, 8 juillet 2016\n",
            "Wei-Ning Hsu, Yu Zhang et James Glass, 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), dÃ©cembre 2017, 16â€“23 p. (ISBN 978-1-5090-4788-8, DOI 10.1109/ASRU.2017.8268911, arXiv 1707.06265, S2CID 22681625), Â« Unsupervised domain adaptation for robust speech recognition via variational autoencoder-based data augmentation Â»\n",
            "M. Ehsan Abbasnejad, Anthony Dick et Anton van den Hengel, Infinite Variational Autoencoder for Semi-Supervised Learning, 2017, 5888â€“5897 p. (lire en ligne [archive])\n",
            "(en) Xu, Sun, Deng et Tan, Â« Variational Autoencoder for Semi-Supervised Text Classification Â», Proceedings of the AAAI Conference on Artificial Intelligence, vol. 31, no 1, 0e 12 fÃ©vrier 2017 (lire en ligne [archive])\n",
            "Kameoka, Li, Inoue et Makino, Â« Supervised Determined Source Separation with Multichannel Variational Autoencoder Â», Neural Computation, vol. 31, no 9, 0e 1er septembre 2019, p. 1891â€“1914 (PMID 31335290, DOI 10.1162/neco_a_01217, S2CID 198168155, lire en ligne [archive])\n",
            "An, J., & Cho, S. (2015). Variational autoencoder based anomaly detection using reconstruction probability. Special Lecture on IE, 2(1).\n",
            "Kingma et Welling, Â« An Introduction to Variational Autoencoders Â», Foundations and Trends in Machine Learning, vol. 12, no 4, 0e 2019, p. 307â€“392 (ISSN 1935-8237, DOI 10.1561/2200000056, arXiv 1906.02691, S2CID 174802445)\n",
            "(en) Â« From Autoencoder to Beta-VAE [archive] Â», Lil'Log, 12 aoÃ»t 2018\n",
            "Bengio, Courville et Vincent, Â« Representation Learning: A Review and New Perspectives Â», IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no 8, 0e 2013, p. 1798â€“1828 (ISSN 1939-3539, PMID 23787338, DOI 10.1109/TPAMI.2013.50, arXiv 1206.5538, S2CID 393948, lire en ligne [archive])\u0000\n",
            "0.9885127491079976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(__s))\n",
        "print(__e.shape)\n",
        "tf.print(__e[:4, :8], summarize=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAKttpNX2M8_",
        "outputId": "bbcc5f7c-e109-45e1-bc20-b99e86be792d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "134\n",
            "(34, 256)\n",
            "[[3.17276168 1.53056908 2.41119337 0.0258403085 1.5207386 1.66698301 2.24263883 2.11223722]\n",
            " [2.65205669 1.68546355 2.01416564 0.655108571 2.3957293 1.70228446 2.12328672 2.04205203]\n",
            " [2.4943645 0.441500723 1.79073346 2.31724644 1.87132716 1.36434507 3.37104845 2.3522613]\n",
            " [2.87078524 1.11898732 2.12827492 0.995271683 0.403087556 0.974042118 1.82035911 2.90426946]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "__t = list(set(tokun.pipeline.chunk(seq=SAMPLES[-1], size=4, repeats=False)))\n",
        "__x = tokun.pipeline.preprocess(text=''.join(__t), groups=N_TOKEN_DIM, flatten=True)\n",
        "__e = MODEL._encoder(__x)"
      ],
      "metadata": {
        "id": "dKFlLJbz3IBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlable.io.write(data=__t, path='./metadata.tsv', tsv=False)\n",
        "mlable.io.write(data=__e.numpy(), path='./embeddings.tsv', tsv=True)"
      ],
      "metadata": {
        "id": "Ttnox9Df6AEQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}