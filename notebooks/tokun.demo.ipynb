{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import deps"
      ],
      "metadata": {
        "id": "yd0W4Xjwm6vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlable tokun\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "l5GL5MtSnAOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import functools\n",
        "import itertools\n",
        "import math\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import mlable.metrics\n",
        "import mlable.io\n",
        "\n",
        "import tokun.evaluation\n",
        "import tokun.meta\n",
        "import tokun.model\n",
        "import tokun.pipeline"
      ],
      "metadata": {
        "id": "yo18wu1bm6ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tensorflow version \" + tf.__version__)"
      ],
      "metadata": {
        "id": "sbjr53W8ots9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Setup the GPU / TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFIMfPmgQa0h"
      },
      "outputs": [],
      "source": [
        "# DEVICES #####################################################################\n",
        "\n",
        "tf.debugging.set_log_device_placement(False)\n",
        "\n",
        "CPU = tf.config.list_logical_devices('CPU')\n",
        "GPU = tf.config.list_logical_devices('GPU')\n",
        "TPU = tf.config.list_logical_devices('TPU')\n",
        "\n",
        "if TPU:\n",
        "    RESOLVER = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(RESOLVER)\n",
        "    tf.tpu.experimental.initialize_tpu_system(RESOLVER)\n",
        "    DISTRIBUTION_STRATEGY = tf.distribute.TPUStrategy(RESOLVER)\n",
        "elif GPU:\n",
        "    DISTRIBUTION_STRATEGY = tf.distribute.MirroredStrategy(GPU)\n",
        "else:\n",
        "    DISTRIBUTION_STRATEGY = tf.distribute.MirroredStrategy(CPU)\n",
        "\n",
        "print(DISTRIBUTION_STRATEGY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t1jfsJlM3SX"
      },
      "source": [
        "## Defining The Metadata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TOGGLE ######################################################################\n",
        "\n",
        "BINARY = True"
      ],
      "metadata": {
        "id": "u6fxCXsWF2w4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z74MlibMWnu"
      },
      "outputs": [],
      "source": [
        "# PARAMETERS ##################################################################\n",
        "\n",
        "N_SEQUENCE_AXIS = 1\n",
        "N_FEATURE_AXIS = -1\n",
        "\n",
        "N_TOKEN_DIM = [4, 16] # G, for each block\n",
        "N_INPUT_DIM = 256 # U_i (bytes)\n",
        "N_EMBED_DIM = 256 # E\n",
        "N_OUTPUT_DIM = 8 if BINARY else 256 # U_o (8 bits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jm6y63XRBz07"
      },
      "outputs": [],
      "source": [
        "# DERIVED #####################################################################\n",
        "\n",
        "N_TOKEN_SIZES = list(itertools.accumulate(N_TOKEN_DIM, lambda x, y: x * y)) # in bytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfIZb86Fg0dQ"
      },
      "outputs": [],
      "source": [
        "# IMPORT ######################################################################\n",
        "\n",
        "LABEL = '8.6'\n",
        "VERSION = tokun.meta.version(token_dim=N_TOKEN_DIM, sequence_axis=N_SEQUENCE_AXIS, input_dim=N_INPUT_DIM, embed_dim=N_EMBED_DIM, output_dim=N_OUTPUT_DIM)\n",
        "\n",
        "URL_IMPORT = 'https://github.com/apehex/tokun/raw/main/models/{}/{}/{}/{}.keras'.format(*VERSION, LABEL)\n",
        "PATH_IMPORT = 'model.keras'\n",
        "\n",
        "urllib.request.urlretrieve(URL_IMPORT, PATH_IMPORT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-iGpXb15m83"
      },
      "outputs": [],
      "source": [
        "# SAMPLES #####################################################################\n",
        "\n",
        "SAMPLES = [\n",
        "    \"\"\"A variational autoencoder is a generative model with a prior and noise distribution respectively. Usually such models are trained using the expectation-maximization meta-algorithm (e.g. probabilistic PCA, (spike & slab) sparse coding). Such a scheme optimizes a lower bound of the data likelihood, which is usually intractable, and in doing so requires the discovery of q-distributions, or variational posteriors. These q-distributions are normally parameterized for each individual data point in a separate optimization process. However, variational autoencoders use a neural network as an amortized approach to jointly optimize across data points. This neural network takes as input the data points themselves, and outputs parameters for the variational distribution. As it maps from a known input space to the low-dimensional latent space, it is called the encoder.\"\"\",\n",
        "    \"\"\"Une unité lexicale ou token lexical ou plus simplement token est un couple composé d'un nom et d'une valeur optionnelle (e.g. 135677).\"\"\",\n",
        "    \"\"\"class AutoEncoder(tf.keras.models.Model):\\n    def __init__(self, token_dim: int, encoding_dim: int, embedding_dim: int, batch_dim: int=None, **kwargs) -> None:\\n        super(AutoEncoder, self).__init__(**kwargs)\\n        self._encoder = Encoder(token_dim=token_dim, encoding_dim=encoding_dim, embedding_dim=embedding_dim, batch_dim=batch_dim)\\n        self._decoder = Decoder(token_dim=token_dim, encoding_dim=encoding_dim, embedding_dim=embedding_dim, batch_dim=batch_dim)\\n\\n    def call(self, x: tf.Tensor) -> tf.Tensor:\\n        return self._decoder(self._encoder(x))\"\"\",\n",
        "    \"\"\"class AutoEncoder(tf.keras.models.Model):\\n  def __init__(self, token_dim: int, encoding_dim: int, embedding_dim: int, batch_dim: int=None, **kwargs) -> None:\\n    super(AutoEncoder, self).__init__(**kwargs)\\n    self._encoder = Encoder(token_dim=token_dim, encoding_dim=encoding_dim, embedding_dim=embedding_dim, batch_dim=batch_dim)\\n    self._decoder = Decoder(token_dim=token_dim, encoding_dim=encoding_dim, embedding_dim=embedding_dim, batch_dim=batch_dim)\\n\\n  def call(self, x: tf.Tensor) -> tf.Tensor:\\n    return self._decoder(self._encoder(x))\"\"\",\n",
        "    \"\"\"위키백과, 우리 모두의 백과사전.\\nt-분포 확률적 임베딩(t-SNE)은 데이터의 차원 축소에 사용되는 기계 학습 알고리즘 중 하나로, 2002년 샘 로이스Sam Rowise와 제프리 힌튼에 의해 개발되었다.[1] t-SNE는 비선형 차원 축소 기법으로, 고차원 데이터를 특히 2, 3차원 등으로 줄여 가시화하는데에 유용하게 사용된다. 구체적으로 t-SNE는 비슷한 데이터는 근접한 2, 3차원의 지점으로, 다른 데이터는 멀리 떨어진 지점으로 맵핑한다.\"\"\",]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEyFtkcFNGe4"
      },
      "source": [
        "## Init"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# METRICS #####################################################################\n",
        "\n",
        "_Accuracy = mlable.metrics.BinaryGroupAccuracy if BINARY else mlable.metrics.CategoricalGroupAccuracy\n",
        "_Loss = tf.keras.losses.BinaryCrossentropy if BINARY else tf.keras.losses.CategoricalCrossentropy"
      ],
      "metadata": {
        "id": "HOhNK7hiYaaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEpY1-vFIFX7"
      },
      "outputs": [],
      "source": [
        "# INIT ########################################################################\n",
        "\n",
        "with DISTRIBUTION_STRATEGY.scope():\n",
        "    # metrics\n",
        "    byte_accuracy = _Accuracy(group=1, name='byte_accuracy')\n",
        "    character_accuracy = _Accuracy(group=4, name='character_accuracy')\n",
        "    token_accuracy = _Accuracy(group=N_TOKEN_SIZES[-1], name='token_accuracy')\n",
        "    # weights\n",
        "    MODEL = tf.keras.models.load_model(PATH_IMPORT, compile=False)\n",
        "    # compilation\n",
        "    MODEL.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        loss=_Loss(from_logits=False, label_smoothing=0., axis=-1, reduction='sum_over_batch_size', name='ce_loss'),\n",
        "        metrics=[byte_accuracy, character_accuracy, token_accuracy])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuRwWdjpPQBM"
      },
      "outputs": [],
      "source": [
        "MODEL.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHtROW1K1R7c"
      },
      "source": [
        "## Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ENCODE ######################################################################\n",
        "\n",
        "__s = SAMPLES[1]\n",
        "__x = tokun.pipeline.preprocess(text=__s, token_size=math.prod(N_TOKEN_DIM), expand=N_SEQUENCE_AXIS * [1])\n",
        "__e = MODEL._encoder(__x) # final embedding = input for another model"
      ],
      "metadata": {
        "id": "gw_bhpP6JX8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VIEW ########################################################################\n",
        "\n",
        "print(len(__s)) # original text length, roughly S\n",
        "print(__x.shape) # 4 * S, with padding\n",
        "print(__e.shape) # 4 * S // 64"
      ],
      "metadata": {
        "id": "M5DINgnlMCwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detokenize"
      ],
      "metadata": {
        "id": "maFagKtSLgPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DECODE ######################################################################\n",
        "\n",
        "__p = MODEL._decoder(__e)\n",
        "__y = tokun.pipeline.postprocess(__p, binary=BINARY, random=False)\n",
        "__o = tokun.pipeline.unpack(__y)"
      ],
      "metadata": {
        "id": "cLuBORzbMNcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('# INPUT ################################################################\\n\\n' + __s)\n",
        "print('\\n# OUTPUT ###############################################################\\n\\n' + __o[0])\n",
        "print('\\n# SCORE ################################################################\\n\\n' + str(tokun.evaluation.compare(__s, __o[0])))"
      ],
      "metadata": {
        "id": "mZkdNGWoMalz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Robustness"
      ],
      "metadata": {
        "id": "dLjTuH3IMyza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SAMPLE ######################################################################\n",
        "\n",
        "__x = tokun.pipeline.preprocess(\"\"\"Une unité lexicale ou token lexical ou plus simplement token est un couple composé d'un nom et d'une valeur optionnelle (e.g. 135677).\"\"\", token_size=math.prod(N_TOKEN_DIM), expand=N_SEQUENCE_AXIS * [1])\n",
        "__e = MODEL._encoder(__x)"
      ],
      "metadata": {
        "id": "-NRq3hZHM_6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ysd903RYTt8"
      },
      "outputs": [],
      "source": [
        "# NOISE #######################################################################\n",
        "\n",
        "__std = tf.math.reduce_std(__e, axis=1)\n",
        "__noise = tf.random.normal(shape=(256,), mean=0., stddev=tf.math.reduce_mean(__std).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DETOKENIZE ##################################################################\n",
        "\n",
        "print(tokun.pipeline.postprocess(MODEL._decoder(__e), binary=BINARY, random=False)) # original\n",
        "print(tokun.pipeline.postprocess(MODEL._decoder(__e + 0.8 * __std), binary=BINARY, random=False)) # with structured noise\n",
        "print(tokun.pipeline.postprocess(MODEL._decoder(__e + 1. * __noise), binary=BINARY, random=False)) # with random noise"
      ],
      "metadata": {
        "id": "nKyqASAJs2xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Similarity"
      ],
      "metadata": {
        "id": "y7yyDMCmmbYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SAMPLES #####################################################################\n",
        "\n",
        "REFERENCE = \"\"\"A couple is enjoying fresh sushis at home.\"\"\"\n",
        "CANDIDATES = [\n",
        "    \"\"\"A couple is cooking fresh sushis at home.\"\"\",\n",
        "    \"\"\"Ana and Bob are enjoying sushis while watching TV.\"\"\",\n",
        "    \"\"\"A couple is enjoying california rolls.\"\"\",\n",
        "    \"\"\"You can order fresh sushis from home.\"\"\",\n",
        "    \"\"\"Enjoying fresh sushis at home a couple is.\"\"\",\n",
        "    \"\"\"Sushis are the best!\"\"\",]"
      ],
      "metadata": {
        "id": "khdOpTtlnLqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# METRIC ######################################################################\n",
        "\n",
        "__f = tf.keras.losses.CosineSimilarity(axis=-1, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
        "\n",
        "# REFERENCE ###################################################################\n",
        "\n",
        "__xr, __er, __pr, __yr = tokun.pipeline.sample(model=MODEL, text=REFERENCE, token_size=math.prod(N_TOKEN_DIM), expand=N_SEQUENCE_AXIS * [1])\n",
        "\n",
        "# SHAPE #######################################################################\n",
        "\n",
        "__sr = list(__er.shape) # shape\n",
        "__rr = len(__sr) # rank"
      ],
      "metadata": {
        "id": "bpKV8Qvhwtvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DISTANCES ###################################################################\n",
        "\n",
        "DISTANCES = {}\n",
        "\n",
        "for __c in CANDIDATES:\n",
        "  __xl, __el, __pl, __yl, __ol = tokun.pipeline.sample(model=MODEL, text=__c, token_size=math.prod(__sr[1:2] + N_TOKEN_DIM), expand=N_SEQUENCE_AXIS * [1], binary=BINARY, random=False)\n",
        "  __el = tf.slice(__el, begin=__rr * [0], size=__sr) # match reference shape\n",
        "  DISTANCES[__c] = __f(y_true=__er, y_pred=__el).numpy()"
      ],
      "metadata": {
        "id": "SNHPvydHlsGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SORT ########################################################################\n",
        "\n",
        "DISTANCES = dict(sorted(DISTANCES.items(), key=lambda __x: __x[1]))"
      ],
      "metadata": {
        "id": "Vj90AWFUy2no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DISTANCES"
      ],
      "metadata": {
        "id": "sKTTjT5KxDUK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}