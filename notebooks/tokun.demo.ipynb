{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import deps"
      ],
      "metadata": {
        "id": "yd0W4Xjwm6vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokun\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5GL5MtSnAOC",
        "outputId": "c7399cc4-44d3-44d8-d7ad-4834b55b34b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokun\n",
            "  Downloading tokun-0.3.6-py3-none-any.whl (12 kB)\n",
            "INFO: pip is looking at multiple versions of tokun to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading tokun-0.3.5-py3-none-any.whl (12 kB)\n",
            "  Downloading tokun-0.3.4-py3-none-any.whl (12 kB)\n",
            "Collecting mlable>=0.1.3 (from tokun)\n",
            "  Downloading mlable-0.1.3-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.14 in /usr/local/lib/python3.10/dist-packages (from tokun) (2.15.0)\n",
            "Requirement already satisfied: tensorflow-datasets>=4.9 in /usr/local/lib/python3.10/dist-packages (from tokun) (4.9.4)\n",
            "Requirement already satisfied: torch>=2.2 in /usr/local/lib/python3.10/dist-packages (from tokun) (2.2.0+cpu)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (1.63.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.14->tokun) (2.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=4.9->tokun) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=4.9->tokun) (0.1.8)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=4.9->tokun) (1.7.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=4.9->tokun) (2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=4.9->tokun) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=4.9->tokun) (2.31.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=4.9->tokun) (1.15.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=4.9->tokun) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=4.9->tokun) (4.66.4)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets>=4.9->tokun) (0.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.2->tokun) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.2->tokun) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.2->tokun) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2->tokun) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.2->tokun) (2024.3.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.14->tokun) (0.43.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets>=4.9->tokun) (6.4.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets>=4.9->tokun) (3.18.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets>=4.9->tokun) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets>=4.9->tokun) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets>=4.9->tokun) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets>=4.9->tokun) (2024.2.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.14->tokun) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.14->tokun) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.14->tokun) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.14->tokun) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.14->tokun) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.2->tokun) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.2->tokun) (1.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.14->tokun) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.14->tokun) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.14->tokun) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.14->tokun) (2.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.14->tokun) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.14->tokun) (3.2.2)\n",
            "Installing collected packages: mlable, tokun\n",
            "Successfully installed mlable-0.1.3 tokun-0.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import functools\n",
        "import itertools\n",
        "import math\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import mlable.tensorflow.io\n",
        "import tokun.meta\n",
        "import tokun.model\n",
        "import tokun.pipeline"
      ],
      "metadata": {
        "id": "yo18wu1bm6ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tensorflow version \" + tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbjr53W8ots9",
        "outputId": "7cafdcba-4fe8-4a2e-cc51-5e3e5c7865ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version 2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Setup the GPU / TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFIMfPmgQa0h",
        "outputId": "f4df3967-af9e-4f35-a5b0-92d40a00173a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tensorflow.python.distribute.tpu_strategy.TPUStrategyV2 object at 0x7f8361e5ba60>\n"
          ]
        }
      ],
      "source": [
        "tf.debugging.set_log_device_placement(False)\n",
        "\n",
        "GPU = tf.config.list_logical_devices('GPU')\n",
        "TPU = tf.config.list_logical_devices('TPU')\n",
        "\n",
        "if TPU:\n",
        "    RESOLVER = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(RESOLVER)\n",
        "    tf.tpu.experimental.initialize_tpu_system(RESOLVER)\n",
        "    DISTRIBUTION_STRATEGY = tf.distribute.TPUStrategy(RESOLVER)\n",
        "elif GPU:\n",
        "    DISTRIBUTION_STRATEGY = tf.distribute.MirroredStrategy(GPU)\n",
        "\n",
        "print(DISTRIBUTION_STRATEGY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t1jfsJlM3SX"
      },
      "source": [
        "## Defining The Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z74MlibMWnu"
      },
      "outputs": [],
      "source": [
        "# PARAMETERS ##################################################################\n",
        "\n",
        "ACTIVATION = 'silu'\n",
        "ATTENTION = True\n",
        "NORMALIZATION = True\n",
        "\n",
        "N_TOKEN_DIM = [4, 4] # G, for each block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jm6y63XRBz07"
      },
      "outputs": [],
      "source": [
        "# DERIVED #####################################################################\n",
        "\n",
        "TOKEN_SIZES = list(itertools.accumulate(N_TOKEN_DIM, lambda x, y: x * y)) # in bytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfIZb86Fg0dQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3040b022-2a14-4913-f614-79ad3ecbdaf5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('model.keras', <http.client.HTTPMessage at 0x7f7d087b9f90>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# IMPORT ######################################################################\n",
        "\n",
        "VERSION = tokun.meta.version(groups=N_TOKEN_DIM, activation=ACTIVATION, attention=ATTENTION, normalization=NORMALIZATION)\n",
        "LABEL = '8.5'\n",
        "\n",
        "URL_IMPORT = 'https://github.com/apehex/tokun/raw/main/models/{}/{}/{}/{}/{}.keras'.format(*VERSION, LABEL)\n",
        "PATH_IMPORT = 'model.keras'\n",
        "\n",
        "urllib.request.urlretrieve(URL_IMPORT, PATH_IMPORT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-iGpXb15m83"
      },
      "outputs": [],
      "source": [
        "# SAMPLES #####################################################################\n",
        "\n",
        "SAMPLES = [\n",
        "    \"\"\"A variational autoencoder is a generative model with a prior and noise distribution respectively. Usually such models are trained using the expectation-maximization meta-algorithm (e.g. probabilistic PCA, (spike & slab) sparse coding). Such a scheme optimizes a lower bound of the data likelihood, which is usually intractable, and in doing so requires the discovery of q-distributions, or variational posteriors. These q-distributions are normally parameterized for each individual data point in a separate optimization process. However, variational autoencoders use a neural network as an amortized approach to jointly optimize across data points. This neural network takes as input the data points themselves, and outputs parameters for the variational distribution. As it maps from a known input space to the low-dimensional latent space, it is called the encoder.\"\"\",\n",
        "    \"\"\"Une unité lexicale ou token lexical ou plus simplement token est un couple composé d'un nom et d'une valeur optionnelle (e.g. 135677).\"\"\",\n",
        "    \"\"\"class AutoEncoder(tf.keras.models.Model):\\n    def __init__(self, token_dim: int, encoding_dim: int, embedding_dim: int, latent_dim: int, batch_dim: int=None, **kwargs) -> None:\\n        super(AutoEncoder, self).__init__(**kwargs)\\n        self._encoder = Encoder(token_dim=token_dim, encoding_dim=encoding_dim, embedding_dim=embedding_dim, latent_dim=latent_dim, batch_dim=batch_dim)\\n        self._decoder = Decoder(token_dim=token_dim, encoding_dim=encoding_dim, embedding_dim=embedding_dim, latent_dim=latent_dim, batch_dim=batch_dim)\\n\\n    def call(self, x: tf.Tensor) -> tf.Tensor:\\n        return self._decoder(self._encoder(x))\"\"\",\n",
        "    \"\"\"class AutoEncoder(tf.keras.models.Model):\\n  def __init__(self, token_dim: int, encoding_dim: int, embedding_dim: int, latent_dim: int, batch_dim: int=None, **kwargs) -> None:\\n    super(AutoEncoder, self).__init__(**kwargs)\\n    self._encoder = Encoder(token_dim=token_dim, encoding_dim=encoding_dim, embedding_dim=embedding_dim, latent_dim=latent_dim, batch_dim=batch_dim)\\n    self._decoder = Decoder(token_dim=token_dim, encoding_dim=encoding_dim, embedding_dim=embedding_dim, latent_dim=latent_dim, batch_dim=batch_dim)\\n\\n  def call(self, x: tf.Tensor) -> tf.Tensor:\\n    return self._decoder(self._encoder(x))\"\"\",\n",
        "    \"\"\"위키백과, 우리 모두의 백과사전.\\nt-분포 확률적 임베딩(t-SNE)은 데이터의 차원 축소에 사용되는 기계 학습 알고리즘 중 하나로, 2002년 샘 로이스Sam Rowise와 제프리 힌튼에 의해 개발되었다.[1] t-SNE는 비선형 차원 축소 기법으로, 고차원 데이터를 특히 2, 3차원 등으로 줄여 가시화하는데에 유용하게 사용된다. 구체적으로 t-SNE는 비슷한 데이터는 근접한 2, 3차원의 지점으로, 다른 데이터는 멀리 떨어진 지점으로 맵핑한다.\"\"\",\n",
        "    \"\"\"Auto-encodeur variationnel\\n\\nArticle\\nDiscussion\\nLire\\nModifier\\nModifier le code\\nVoir l’historique\\n\\nOutils\\nEn apprentissage automatique, un auto-encodeur variationnel (ou VAE de l'anglais variational auto encoder)1, est une architecture de réseau de neurones artificiels introduite en 2013 par D. Kingma et M. Welling, appartenant aux familles des modèles graphiques probabilistes et des méthodes bayésiennes variationnelles.\\n\\nLes VAE sont souvent rapprochés des autoencodeurs2,3 en raison de leur architectures similaires. Leur utilisation et leur formulation mathématiques sont cependant différentes.\\n\\nLes auto-encodeurs variationnels permettent de formuler un problème d'inférence statistique (par exemple, déduire la valeur d'une variable aléatoire à partir d'une autre variable aléatoire) en un problème d'optimisation statistique (c'est-à-dire trouver les valeurs de paramètres qui minimisent une fonction objectif)4. Ils représentent une fonction associant à une valeur d'entrée une distribution latente multivariée, qui n'est pas directement observée mais déduite depuis un modèle mathématique à partir de la distribution d'autres variables. Bien que ce type de modèle ait été initialement conçu pour l'apprentissage non supervisé5, son efficacité a été prouvée pour l'apprentissage semi-supervisé6,7 et l'apprentissage supervisé8.\\n\\nArchitecture\\nDans un VAE, les données d'entrée sont échantillonnées à partir d'une distribution paramétrée (la distribution a priori, en termes d'inférence bayésienne), et l'encodeur et le décodeur sont entraînés conjointement de sorte que la sortie minimise une erreur de reconstruction dans le sens de la divergence de Kullback-Leibler entre la distribution paramétrique postérieure et la vraie distribution a posteriori9,10.\\n\\nFormulation\\n\\nLe schéma de base d'un auto-encodeur variationnel. Le modèle reçoit \\n𝑥\\n{\\displaystyle \\mathbf {x} } comme entrée. L'encodeur le comprime dans l'espace latent. Le décodeur reçoit en entrée les informations prélevées dans l'espace latent et produit \\n𝑥\\n′\\n{\\displaystyle \\mathbf {x'} } aussi semblable que possible à \\n𝑥\\n{\\displaystyle \\mathbf {x} } .\\nOn note \\n𝑥\\n{\\displaystyle \\mathbf {x} } le vecteur contenant l'ensemble des variables observées que l'on souhaite modéliser. Ce vecteur est une variable aléatoire, caractérisé par une distribution de probabilité inconnue \\n𝑃\\n(\\n𝑥\\n)\\n{\\displaystyle P(\\mathbf {x} )}, que l'on souhaite approximer par une distribution paramétrée \\n𝑝\\n𝜃{\\displaystyle p_{\\theta }} ayant pour paramètres \\n𝜃{\\displaystyle \\theta }.\\n\\nOn introduit alors un vecteur aléatoire \\n𝑧\\n{\\displaystyle \\mathbf {z} } distribué conjointement avec \\n𝑥\\n{\\displaystyle \\mathbf {x} } (c'est-à-dire dont la loi de probabilité n'est pas indépendante de celle de \\n𝑥\\n{\\displaystyle \\mathbf {x} }). Ce vecteur \\n𝑧\\n{\\displaystyle \\mathbf {z} } représente un encodage latent de \\n𝑥\\n{\\displaystyle \\mathbf {x} }, que l'on ne peut observer directement.\\n\\nOn exprime alors la distribution \\n𝑝\\n𝜃{\\displaystyle p_{\\theta }} via la loi de probablitié marginale sur \\n𝑧\\n{\\displaystyle \\mathbf {z} }, ce qui donne alors:\\n\\n𝑝\\n𝜃\\n(\\n𝑥\\n)\\n=\\n∫\\n𝑧\\n𝑝\\n𝜃\\n(\\n𝑥\\n,\\n𝑧\\n)\\n𝑑\\n𝑧\\n,\\n{\\displaystyle p_{\\theta }(\\mathbf {x} )=\\int _{\\mathbf {z} }p_{\\theta }(\\mathbf {x,z} )\\,d\\mathbf {z} ,}\\noù \\n𝑝\\n𝜃\\n(\\n𝑥\\n,\\n𝑧\\n)\\n{\\displaystyle p_{\\theta }(\\mathbf {x,z} )} représente la distribution conjointe sous \\n𝑝\\n𝜃{\\displaystyle p_{\\theta }} des données observables \\n𝑥\\n{\\displaystyle \\mathbf {x} } et de leur représentation latente \\n𝑧\\n{\\displaystyle \\mathbf {z} }. Selon la formule des probabilités composées, l'équation peut être réécrite comme\\n\\n𝑝\\n𝜃\\n(\\n𝑥\\n)\\n=\\n∫\\n𝑧\\n𝑝\\n𝜃\\n(\\n𝑥\\n∣\\n𝑧\\n)\\n𝑝\\n𝜃\\n(\\n𝑧\\n)\\n𝑑\\n𝑧\\n{\\displaystyle p_{\\theta }(\\mathbf {x} )=\\int _{\\mathbf {z} }p_{\\theta }(\\mathbf {x\\mid z} )p_{\\theta }(\\mathbf {z} )\\,d\\mathbf {z} }\\nDans l'auto-encodeur variationnel classique, on fait l'hypothèse que \\n𝑧\\n{\\displaystyle \\mathbf {z} } est un vecteur à valeur réelles de dimension finie, et \\n𝑝\\n𝜃\\n(\\n𝑥\\n|\\n𝑧\\n)\\n{\\displaystyle p_{\\theta }(\\mathbf {x|z} )} suit une loi normale. Par conséquent, \\n𝑝\\n𝜃\\n(\\n𝑥\\n)\\n{\\displaystyle p_{\\theta }(\\mathbf {x} )} est un mélange de distributions gaussiennes.\\n\\nOn peut voir les relations entre les données d'entrée et leur représentation latente comme un problème d'inférence bayésienne avec\\n\\n𝑝\\n𝜃\\n(\\n𝑧\\n)\\n{\\displaystyle p_{\\theta }(\\mathbf {z} )} représente la distribution probabilité a priori dans l'espace latent\\n𝑝\\n𝜃\\n(\\n𝑥\\n∣\\n𝑧\\n)\\n{\\displaystyle p_{\\theta }(\\mathbf {x} \\mid \\mathbf {z} )} représente la vraisemblance\\n𝑝\\n𝜃\\n(\\n𝑧\\n∣\\n𝑥\\n)\\n{\\displaystyle p_{\\theta }(\\mathbf {z} \\mid \\mathbf {x} )} représente la distribution de probabilité a posteriori\\nMalheureusement, le calcul de \\n𝑝\\n𝜃\\n(\\n𝑥\\n)\\n{\\displaystyle p_{\\theta }(\\mathbf {x} )} est au mieux coûteux, et dans la plupart des cas, impossible. Pour résoudre ce problème, il est nécessaire d'introduire une autre fonction \\n𝑞\\nΦ{\\displaystyle q_{\\Phi }} pour approximer la distribution a posteriori :\\n\\n𝑞\\nΦ\\n(\\n𝑧\\n∣\\n𝑥\\n)\\n≈\\n𝑝\\n𝜃\\n(\\n𝑧\\n∣\\n𝑥\\n)\\n{\\displaystyle q_{\\Phi }(\\mathbf {z\\mid x} )\\approx p_{\\theta }(\\mathbf {z\\mid x} )}\\noù \\nΦ{\\displaystyle \\Phi } est l'ensemble des paramètres de \\n𝑞\\n{\\displaystyle q} .\\n\\nAinsi le problème est formulé pour pouvoir être appliqué dans une architecture de réseau de neurones auto-encodeur, dans lequel la distribution de vraisemblance conditionnelle \\n𝑝\\n𝜃\\n(\\n𝑥\\n∣\\n𝑧\\n)\\n{\\displaystyle p_{\\theta }(\\mathbf {x} \\mid \\mathbf {z} )} est représentée par un décodeur probabiliste, tandis que la distribution a posteriori approchée \\n𝑞\\nΦ\\n(\\n𝑧\\n∣\\n𝑥\\n)\\n{\\displaystyle q_{\\Phi }(\\mathbf {z} \\mid \\mathbf {x} )} est représentée par un codeur probabiliste. La mise en œuvre d'un VAE consistera donc à calculer les valeurs optimales des paramètres \\n𝜃{\\displaystyle \\theta } et \\nΦ{\\displaystyle \\Phi } par un apprentissage automatique.\\n\\nFonction de perte ELBO\\nComme dans tout problème d'apprentissage profond, il est nécessaire de définir une fonction de perte différentiable afin de mettre à jour les poids du réseau par rétropropagation lors de l'apprentissage.\\n\\nPour les auto-encodeurs variationnels, l'idée est de minimiser conjointement les paramètres du modèle génératif \\n𝜃{\\displaystyle \\theta } pour réduire l'erreur de reconstruction entre l'entrée et la sortie, et \\nΦ{\\displaystyle \\Phi } pour avoir \\n𝑞\\nΦ\\n(\\n𝑧\\n∣\\n𝑥\\n)\\n{\\displaystyle q_{\\Phi }(\\mathbf {z\\mid x} )}, la distribution postérieure approchée, le plus près possible de \\n𝑝\\n𝜃\\n(\\n𝑧\\n∣\\n𝑥\\n)\\n{\\displaystyle p_{\\theta }(\\mathbf {z} \\mid \\mathbf {x} )}, la vraie distribution de probabilité a posteriori.\\n\\nComme fonction de coût pour la reconstruction, l'erreur quadratique moyenne et l'entropie croisée sont souvent utilisées.\\n\\nPour la fonction de coût de distance entre les deux distributions, la divergence inverse de Kullback – Leibler \\n𝐷\\n𝐾\\n𝐿\\n(\\n𝑞\\nΦ\\n(\\n𝑧\\n∣\\n𝑥\\n)\\n∥\\n𝑝\\n𝜃\\n(\\n𝑧\\n∣\\n𝑥\\n)\\n)\\n{\\displaystyle D_{KL}(q_{\\Phi }(\\mathbf {z\\mid x} )\\parallel p_{\\theta }(\\mathbf {z\\mid x} ))} est un bon choix pour pousser \\n𝑞\\nΦ\\n(\\n𝑧\\n∣\\n𝑥\\n)\\n{\\displaystyle q_{\\Phi }(\\mathbf {z\\mid x} )} en dessous de \\n𝑝\\n𝜃\\n(\\n𝑧\\n∣\\n𝑥\\n)\\n{\\displaystyle p_{\\theta }(\\mathbf {z} \\mid \\mathbf {x} )}1,11.\\n\\nReparamétrisation\\n\\nLe schéma de l'astuce de reparamétrisation. La variable aléatoire \\n𝜀{\\displaystyle \\mathbf {\\varepsilon } } est injecté dans l'espace latent, non observé, \\n𝑧\\n{\\displaystyle \\mathbf {z} } comme entrée externe. De cette manière, il est possible de rétropropager le gradient sans impliquer de variable stochastique lors de la mise à jour.\\nPour rendre la formulation ELBO adaptée à des fins d'apprentissage, il est nécessaire de modifier légèrement la formulation du problème et la structure du VAE12.\\n\\nL'échantillonnage stochastique est l'opération non différentiable par laquelle il est possible d'échantillonner à partir de l'espace latent et d'alimenter le décodeur probabiliste.\\n\\nL'hypothèse principale sur l'espace latent est qu'il peut être considéré comme un ensemble de distributions gaussiennes multivariées, et peut donc être décrit comme\\n\\n𝑧\\n∼\\n𝑞\\nΦ\\n(\\n𝑧\\n∣\\n𝑥\\n)\\n=\\n𝑁\\n(\\n𝜇\\n,\\n𝜎\\n2\\n)\\n{\\displaystyle \\mathbf {z} \\sim q_{\\Phi }(\\mathbf {z} \\mid \\mathbf {x} )={\\mathcal {N}}({\\boldsymbol {\\mu }},{\\boldsymbol {\\sigma }}^{2})} .\\n\\nLe schéma d'un auto-encodeur variationnel après l'astuce de reparamétrisation.\\nEtant donné \\n𝜀\\n∼\\n𝑁\\n(\\n0\\n,\\n𝐼\\n)\\n{\\displaystyle {\\boldsymbol {\\varepsilon }}\\sim {\\mathcal {N}}(0,{\\boldsymbol {I}})} et \\n⊙{\\displaystyle \\odot } défini comme le produit élément par élément, l'astuce de reparamétrisation modifie l'équation ci-dessus comme\\n\\n𝑧\\n=\\n𝜇\\n+\\n𝜎\\n⊙\\n𝜀\\n.\\n{\\displaystyle \\mathbf {z} ={\\boldsymbol {\\mu }}+{\\boldsymbol {\\sigma }}\\odot {\\boldsymbol {\\varepsilon }}.}\\nGrâce à cette transformation (qui peut être étendue à des distributions non gaussiennes), le VAE devient entraînable et le codeur probabiliste doit apprendre à mapper une représentation compressée de l'entrée dans les deux vecteurs latents \\n𝜇{\\displaystyle {\\boldsymbol {\\mu }}} et \\n𝜎{\\displaystyle {\\boldsymbol {\\sigma }}}, tandis que la stochasticité reste exclue du processus de mise à jour et est injectée dans l'espace latent en tant qu'entrée externe via le vecteur aléatoire \\n𝜀{\\displaystyle {\\boldsymbol {\\varepsilon }}} .\\n\\nRéférences\\nDiederik P. Kingma et Max Welling, « Auto-Encoding Variational Bayes », arXiv:1312.6114 [cs, stat],\\x200e 1er mai 2014 (lire en ligne [archive], consulté le 1er juillet 2022)\\n(en) Kramer, « Nonlinear principal component analysis using autoassociative neural networks », AIChE Journal, vol. 37, no 2,\\x200e 1991, p. 233–243 (DOI 10.1002/aic.690370209, lire en ligne [archive])\\n(en) Hinton et Salakhutdinov, « Reducing the Dimensionality of Data with Neural Networks », Science, vol. 313, no 5786,\\x200e 28 juillet 2006, p. 504–507 (PMID 16873662, DOI 10.1126/science.1127647, Bibcode 2006Sci...313..504H, S2CID 1658773, lire en ligne [archive])\\n(en) « A Beginner's Guide to Variational Methods: Mean-Field Approximation [archive] », Eric Jang, 8 juillet 2016\\nWei-Ning Hsu, Yu Zhang et James Glass, 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), décembre 2017, 16–23 p. (ISBN 978-1-5090-4788-8, DOI 10.1109/ASRU.2017.8268911, arXiv 1707.06265, S2CID 22681625), « Unsupervised domain adaptation for robust speech recognition via variational autoencoder-based data augmentation »\\nM. Ehsan Abbasnejad, Anthony Dick et Anton van den Hengel, Infinite Variational Autoencoder for Semi-Supervised Learning, 2017, 5888–5897 p. (lire en ligne [archive])\\n(en) Xu, Sun, Deng et Tan, « Variational Autoencoder for Semi-Supervised Text Classification », Proceedings of the AAAI Conference on Artificial Intelligence, vol. 31, no 1,\\x200e 12 février 2017 (lire en ligne [archive])\\nKameoka, Li, Inoue et Makino, « Supervised Determined Source Separation with Multichannel Variational Autoencoder », Neural Computation, vol. 31, no 9,\\x200e 1er septembre 2019, p. 1891–1914 (PMID 31335290, DOI 10.1162/neco_a_01217, S2CID 198168155, lire en ligne [archive])\\nAn, J., & Cho, S. (2015). Variational autoencoder based anomaly detection using reconstruction probability. Special Lecture on IE, 2(1).\\nKingma et Welling, « An Introduction to Variational Autoencoders », Foundations and Trends in Machine Learning, vol. 12, no 4,\\x200e 2019, p. 307–392 (ISSN 1935-8237, DOI 10.1561/2200000056, arXiv 1906.02691, S2CID 174802445)\\n(en) « From Autoencoder to Beta-VAE [archive] », Lil'Log, 12 août 2018\\nBengio, Courville et Vincent, « Representation Learning: A Review and New Perspectives », IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no 8,\\x200e 2013, p. 1798–1828 (ISSN 1939-3539, PMID 23787338, DOI 10.1109/TPAMI.2013.50, arXiv 1206.5538, S2CID 393948, lire en ligne [archive])\"\"\",]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEyFtkcFNGe4"
      },
      "source": [
        "## Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEpY1-vFIFX7"
      },
      "outputs": [],
      "source": [
        "# INIT ########################################################################\n",
        "\n",
        "with DISTRIBUTION_STRATEGY.scope():\n",
        "    MODEL = tf.keras.models.load_model(PATH_IMPORT)\n",
        "    MODEL.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=0., axis=-1, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE, name='loss'),\n",
        "        metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuRwWdjpPQBM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b93305df-dcc8-4187-9929-dc8cefb079bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"auto_encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder (Encoder)           multiple                  593408    \n",
            "                                                                 \n",
            " decoder (Decoder)           multiple                  595200    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1188608 (4.53 MB)\n",
            "Trainable params: 1188608 (4.53 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "MODEL.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHtROW1K1R7c"
      },
      "source": [
        "## Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ENCODE ######################################################################\n",
        "\n",
        "__s = SAMPLES[1]\n",
        "__x = tokun.pipeline.preprocess(text=__s, groups=N_TOKEN_DIM, flatten=True)\n",
        "__e = MODEL._encoder(__x) # final embedding = input for another model"
      ],
      "metadata": {
        "id": "gw_bhpP6JX8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VIEW ########################################################################\n",
        "\n",
        "print(len(__s)) # original text length, roughly S\n",
        "print(__x.shape) # 4 * S, with padding\n",
        "print(__e.shape) # 4 * S // 64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5DINgnlMCwC",
        "outputId": "3569e1b1-5fff-42f6-b8b6-867f391cc915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "134\n",
            "(544, 256)\n",
            "(34, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detokenize"
      ],
      "metadata": {
        "id": "maFagKtSLgPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DECODE ######################################################################\n",
        "\n",
        "__p = MODEL._decoder(__e)\n",
        "__y = tokun.pipeline.postprocess(__p)"
      ],
      "metadata": {
        "id": "cLuBORzbMNcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('# INPUT ################################################################\\n\\n' + __s)\n",
        "print('\\n# OUTPUT ###############################################################\\n\\n' + __y)\n",
        "print('\\n# SCORE ################################################################\\n\\n' + str(tokun.pipeline.compare(__s, __y)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZkdNGWoMalz",
        "outputId": "2cacc8ac-7574-498b-9898-129c568eb1a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# INPUT ################################################################\n",
            "\n",
            "Une unité lexicale ou token lexical ou plus simplement token est un couple composé d'un nom et d'une valeur optionnelle (e.g. 135677).\n",
            "\n",
            "# OUTPUT ###############################################################\n",
            "\n",
            "Une unité lexicale ou token lexical ou plus simplement token est un couple composé d'un nom et d'une valeur optionnelle (e.g. 135677).\u0000\u0000\n",
            "\n",
            "# SCORE ################################################################\n",
            "\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Robustness"
      ],
      "metadata": {
        "id": "dLjTuH3IMyza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SAMPLE ######################################################################\n",
        "\n",
        "__x = tokun.pipeline.preprocess(\"\"\"Une unité lexicale ou token lexical ou plus simplement token est un couple composé d'un nom et d'une valeur optionnelle (e.g. 135677).\"\"\", groups=N_TOKEN_DIM, flatten=True)\n",
        "__e = MODEL._encoder(__x)"
      ],
      "metadata": {
        "id": "-NRq3hZHM_6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ysd903RYTt8"
      },
      "outputs": [],
      "source": [
        "# NOISE #######################################################################\n",
        "\n",
        "__std = tf.math.reduce_std(__e, axis=0)\n",
        "__noise = tf.random.normal(shape=(256,), mean=0., stddev=tf.math.reduce_mean(__std).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DETOKENIZE ##################################################################\n",
        "\n",
        "print(tokun.pipeline.postprocess(MODEL._decoder(__e))) # original\n",
        "print(tokun.pipeline.postprocess(MODEL._decoder(__e + 0.8 * __std))) # with structured noise\n",
        "print(tokun.pipeline.postprocess(MODEL._decoder(__e + 1. * __noise))) # with random noise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKyqASAJs2xl",
        "outputId": "aeb66aa4-c44d-46e2-de03-c699bc6b9336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Une unité lexicale ou token lexical ou plus simplement token est un couple composé d'un nom et d'une valeur optionnelle (e.g. 135677).\u0000\u0000\n",
            "Une unité lexicale ou token lexical ou plus simple\u0000ent token est un couple composé d'un nom et d'une valeur optionnelle (e.g. 135677).\u0000\u0000\n",
            "Une unité lexicale ou token lexical ou plus simplement token est un couple composé d'un nom et d'une vaɬeur optionnelle (e.g. 135677).\u0000e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oa0x3nJKYbR2"
      },
      "outputs": [],
      "source": [
        "# ENCODE ######################################################################\n",
        "\n",
        "__s = SAMPLES[0]\n",
        "__x = tokun.pipeline.preprocess(text=__s, groups=N_TOKEN_DIM, flatten=True)\n",
        "__e = MODEL._encoder(__x) # final embedding = input for another model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DECODE ######################################################################\n",
        "\n",
        "__p = MODEL._decoder(__e)\n",
        "__y = tokun.pipeline.postprocess(__p)"
      ],
      "metadata": {
        "id": "ZJMxA5jE2TX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(__s)\n",
        "print(__y)\n",
        "print(tokun.pipeline.compare(__s, __y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkQ3QKAu72G1",
        "outputId": "e3ddec15-728b-400f-c14d-394a45582303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auto-encodeur variationnel\n",
            "\n",
            "Article\n",
            "Discussion\n",
            "Lire\n",
            "Modifier\n",
            "Modifier le code\n",
            "Voir l’historique\n",
            "\n",
            "Outils\n",
            "En apprentissage automatique, un auto-encodeur variationnel (ou VAE de l'anglais variational auto encoder)1, est une architecture de réseau de neurones artificiels introduite en 2013 par D. Kingma et M. Welling, appartenant aux familles des modèles graphiques probabilistes et des méthodes bayésiennes variationnelles.\n",
            "\n",
            "Les VAE sont souvent rapprochés des autoencodeurs2,3 en raison de leur architectures similaires. Leur utilisation et leur formulation mathématiques sont cependant différentes.\n",
            "\n",
            "Les auto-encodeurs variationnels permettent de formuler un problème d'inférence statistique (par exemple, déduire la valeur d'une variable aléatoire à partir d'une autre variable aléatoire) en un problème d'optimisation statistique (c'est-à-dire trouver les valeurs de paramètres qui minimisent une fonction objectif)4. Ils représentent une fonction associant à une valeur d'entrée une distribution latente multivariée, qui n'est pas directement observée mais déduite depuis un modèle mathématique à partir de la distribution d'autres variables. Bien que ce type de modèle ait été initialement conçu pour l'apprentissage non supervisé5, son efficacité a été prouvée pour l'apprentissage semi-supervisé6,7 et l'apprentissage supervisé8.\n",
            "\n",
            "Architecture\n",
            "Dans un VAE, les données d'entrée sont échantillonnées à partir d'une distribution paramétrée (la distribution a priori, en termes d'inférence bayésienne), et l'encodeur et le décodeur sont entraînés conjointement de sorte que la sortie minimise une erreur de reconstruction dans le sens de la divergence de Kullback-Leibler entre la distribution paramétrique postérieure et la vraie distribution a posteriori9,10.\n",
            "\n",
            "Formulation\n",
            "\n",
            "Le schéma de base d'un auto-encodeur variationnel. Le modèle reçoit \n",
            "𝑥\n",
            "{\\displaystyle \\mathbf {x} } comme entrée. L'encodeur le comprime dans l'espace latent. Le décodeur reçoit en entrée les informations prélevées dans l'espace latent et produit \n",
            "𝑥\n",
            "′\n",
            "{\\displaystyle \\mathbf {x'} } aussi semblable que possible à \n",
            "𝑥\n",
            "{\\displaystyle \\mathbf {x} } .\n",
            "On note \n",
            "𝑥\n",
            "{\\displaystyle \\mathbf {x} } le vecteur contenant l'ensemble des variables observées que l'on souhaite modéliser. Ce vecteur est une variable aléatoire, caractérisé par une distribution de probabilité inconnue \n",
            "𝑃\n",
            "(\n",
            "𝑥\n",
            ")\n",
            "{\\displaystyle P(\\mathbf {x} )}, que l'on souhaite approximer par une distribution paramétrée \n",
            "𝑝\n",
            "𝜃{\\displaystyle p_{\theta }} ayant pour paramètres \n",
            "𝜃{\\displaystyle \theta }.\n",
            "\n",
            "On introduit alors un vecteur aléatoire \n",
            "𝑧\n",
            "{\\displaystyle \\mathbf {z} } distribué conjointement avec \n",
            "𝑥\n",
            "{\\displaystyle \\mathbf {x} } (c'est-à-dire dont la loi de probabilité n'est pas indépendante de celle de \n",
            "𝑥\n",
            "{\\displaystyle \\mathbf {x} }). Ce vecteur \n",
            "𝑧\n",
            "{\\displaystyle \\mathbf {z} } représente un encodage latent de \n",
            "𝑥\n",
            "{\\displaystyle \\mathbf {x} }, que l'on ne peut observer directement.\n",
            "\n",
            "On exprime alors la distribution \n",
            "𝑝\n",
            "𝜃{\\displaystyle p_{\theta }} via la loi de probablitié marginale sur \n",
            "𝑧\n",
            "{\\displaystyle \\mathbf {z} }, ce qui donne alors:\n",
            "\n",
            "𝑝\n",
            "𝜃\n",
            "(\n",
            "𝑥\n",
            ")\n",
            "=\n",
            "∫\n",
            "𝑧\n",
            "𝑝\n",
            "𝜃\n",
            "(\n",
            "𝑥\n",
            ",\n",
            "𝑧\n",
            ")\n",
            "𝑑\n",
            "𝑧\n",
            ",\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} )=\\int _{\\mathbf {z} }p_{\theta }(\\mathbf {x,z} )\\,d\\mathbf {z} ,}\n",
            "où \n",
            "𝑝\n",
            "𝜃\n",
            "(\n",
            "𝑥\n",
            ",\n",
            "𝑧\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x,z} )} représente la distribution conjointe sous \n",
            "𝑝\n",
            "𝜃{\\displaystyle p_{\theta }} des données observables \n",
            "𝑥\n",
            "{\\displaystyle \\mathbf {x} } et de leur représentation latente \n",
            "𝑧\n",
            "{\\displaystyle \\mathbf {z} }. Selon la formule des probabilités composées, l'équation peut être réécrite comme\n",
            "\n",
            "𝑝\n",
            "𝜃\n",
            "(\n",
            "𝑥\n",
            ")\n",
            "=\n",
            "∫\n",
            "𝑧\n",
            "𝑝\n",
            "𝜃\n",
            "(\n",
            "𝑥\n",
            "∣\n",
            "𝑧\n",
            ")\n",
            "𝑝\n",
            "𝜃\n",
            "(\n",
            "𝑧\n",
            ")\n",
            "𝑑\n",
            "𝑧\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} )=\\int _{\\mathbf {z} }p_{\theta }(\\mathbf {x\\mid z} )p_{\theta }(\\mathbf {z} )\\,d\\mathbf {z} }\n",
            "Dans l'auto-encodeur variationnel classique, on fait l'hypothèse que \n",
            "𝑧\n",
            "{\\displaystyle \\mathbf {z} } est un vecteur à valeur réelles de dimension finie, et \n",
            "𝑝\n",
            "𝜃\n",
            "(\n",
            "𝑥\n",
            "|\n",
            "𝑧\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x|z} )} suit une loi normale. Par conséquent, \n",
            "𝑝\n",
            "𝜃\n",
            "(\n",
            "𝑥\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} )} est un mélange de distributions gaussiennes.\n",
            "\n",
            "On peut voir les relations entre les données d'entrée et leur représentation latente comme un problème d'inférence bayésienne avec\n",
            "\n",
            "𝑝\n",
            "𝜃\n",
            "(\n",
            "𝑧\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {z} )} représente la distribution probabilité a priori dans l'espace latent\n",
            "𝑝\n",
            "𝜃\n",
            "(\n",
            "𝑥\n",
            "∣\n",
            "𝑧\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} \\mid \\mathbf {z} )} représente la vraisemblance\n",
            "𝑝\n",
            "𝜃\n",
            "(\n",
            "𝑧\n",
            "∣\n",
            "𝑥\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {z} \\mid \\mathbf {x} )} représente la distribution de probabilité a posteriori\n",
            "Malheureusement, le calcul de \n",
            "𝑝\n",
            "𝜃\n",
            "(\n",
            "𝑥\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} )} est au mieux coûteux, et dans la plupart des cas, impossible. Pour résoudre ce problème, il est nécessaire d'introduire une autre fonction \n",
            "𝑞\n",
            "Φ{\\displaystyle q_{\\Phi }} pour approximer la distribution a posteriori :\n",
            "\n",
            "𝑞\n",
            "Φ\n",
            "(\n",
            "𝑧\n",
            "∣\n",
            "𝑥\n",
            ")\n",
            "≈\n",
            "𝑝\n",
            "𝜃\n",
            "(\n",
            "𝑧\n",
            "∣\n",
            "𝑥\n",
            ")\n",
            "{\\displaystyle q_{\\Phi }(\\mathbf {z\\mid x} )\u0007pprox p_{\theta }(\\mathbf {z\\mid x} )}\n",
            "où \n",
            "Φ{\\displaystyle \\Phi } est l'ensemble des paramètres de \n",
            "𝑞\n",
            "{\\displaystyle q} .\n",
            "\n",
            "Ainsi le problème est formulé pour pouvoir être appliqué dans une architecture de réseau de neurones auto-encodeur, dans lequel la distribution de vraisemblance conditionnelle \n",
            "𝑝\n",
            "𝜃\n",
            "(\n",
            "𝑥\n",
            "∣\n",
            "𝑧\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} \\mid \\mathbf {z} )} est représentée par un décodeur probabiliste, tandis que la distribution a posteriori approchée \n",
            "𝑞\n",
            "Φ\n",
            "(\n",
            "𝑧\n",
            "∣\n",
            "𝑥\n",
            ")\n",
            "{\\displaystyle q_{\\Phi }(\\mathbf {z} \\mid \\mathbf {x} )} est représentée par un codeur probabiliste. La mise en œuvre d'un VAE consistera donc à calculer les valeurs optimales des paramètres \n",
            "𝜃{\\displaystyle \theta } et \n",
            "Φ{\\displaystyle \\Phi } par un apprentissage automatique.\n",
            "\n",
            "Fonction de perte ELBO\n",
            "Comme dans tout problème d'apprentissage profond, il est nécessaire de définir une fonction de perte différentiable afin de mettre à jour les poids du réseau par rétropropagation lors de l'apprentissage.\n",
            "\n",
            "Pour les auto-encodeurs variationnels, l'idée est de minimiser conjointement les paramètres du modèle génératif \n",
            "𝜃{\\displaystyle \theta } pour réduire l'erreur de reconstruction entre l'entrée et la sortie, et \n",
            "Φ{\\displaystyle \\Phi } pour avoir \n",
            "𝑞\n",
            "Φ\n",
            "(\n",
            "𝑧\n",
            "∣\n",
            "𝑥\n",
            ")\n",
            "{\\displaystyle q_{\\Phi }(\\mathbf {z\\mid x} )}, la distribution postérieure approchée, le plus près possible de \n",
            "𝑝\n",
            "𝜃\n",
            "(\n",
            "𝑧\n",
            "∣\n",
            "𝑥\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {z} \\mid \\mathbf {x} )}, la vraie distribution de probabilité a posteriori.\n",
            "\n",
            "Comme fonction de coût pour la reconstruction, l'erreur quadratique moyenne et l'entropie croisée sont souvent utilisées.\n",
            "\n",
            "Pour la fonction de coût de distance entre les deux distributions, la divergence inverse de Kullback – Leibler \n",
            "𝐷\n",
            "𝐾\n",
            "𝐿\n",
            "(\n",
            "𝑞\n",
            "Φ\n",
            "(\n",
            "𝑧\n",
            "∣\n",
            "𝑥\n",
            ")\n",
            "∥\n",
            "𝑝\n",
            "𝜃\n",
            "(\n",
            "𝑧\n",
            "∣\n",
            "𝑥\n",
            ")\n",
            ")\n",
            "{\\displaystyle D_{KL}(q_{\\Phi }(\\mathbf {z\\mid x} )\\parallel p_{\theta }(\\mathbf {z\\mid x} ))} est un bon choix pour pousser \n",
            "𝑞\n",
            "Φ\n",
            "(\n",
            "𝑧\n",
            "∣\n",
            "𝑥\n",
            ")\n",
            "{\\displaystyle q_{\\Phi }(\\mathbf {z\\mid x} )} en dessous de \n",
            "𝑝\n",
            "𝜃\n",
            "(\n",
            "𝑧\n",
            "∣\n",
            "𝑥\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {z} \\mid \\mathbf {x} )}1,11.\n",
            "\n",
            "Reparamétrisation\n",
            "\n",
            "Le schéma de l'astuce de reparamétrisation. La variable aléatoire \n",
            "𝜀{\\displaystyle \\mathbf {\u000barepsilon } } est injecté dans l'espace latent, non observé, \n",
            "𝑧\n",
            "{\\displaystyle \\mathbf {z} } comme entrée externe. De cette manière, il est possible de rétropropager le gradient sans impliquer de variable stochastique lors de la mise à jour.\n",
            "Pour rendre la formulation ELBO adaptée à des fins d'apprentissage, il est nécessaire de modifier légèrement la formulation du problème et la structure du VAE12.\n",
            "\n",
            "L'échantillonnage stochastique est l'opération non différentiable par laquelle il est possible d'échantillonner à partir de l'espace latent et d'alimenter le décodeur probabiliste.\n",
            "\n",
            "L'hypothèse principale sur l'espace latent est qu'il peut être considéré comme un ensemble de distributions gaussiennes multivariées, et peut donc être décrit comme\n",
            "\n",
            "𝑧\n",
            "∼\n",
            "𝑞\n",
            "Φ\n",
            "(\n",
            "𝑧\n",
            "∣\n",
            "𝑥\n",
            ")\n",
            "=\n",
            "𝑁\n",
            "(\n",
            "𝜇\n",
            ",\n",
            "𝜎\n",
            "2\n",
            ")\n",
            "{\\displaystyle \\mathbf {z} \\sim q_{\\Phi }(\\mathbf {z} \\mid \\mathbf {x} )={\\mathcal {N}}({\boldsymbol {\\mu }},{\boldsymbol {\\sigma }}^{2})} .\n",
            "\n",
            "Le schéma d'un auto-encodeur variationnel après l'astuce de reparamétrisation.\n",
            "Etant donné \n",
            "𝜀\n",
            "∼\n",
            "𝑁\n",
            "(\n",
            "0\n",
            ",\n",
            "𝐼\n",
            ")\n",
            "{\\displaystyle {\boldsymbol {\u000barepsilon }}\\sim {\\mathcal {N}}(0,{\boldsymbol {I}})} et \n",
            "⊙{\\displaystyle \\odot } défini comme le produit élément par élément, l'astuce de reparamétrisation modifie l'équation ci-dessus comme\n",
            "\n",
            "𝑧\n",
            "=\n",
            "𝜇\n",
            "+\n",
            "𝜎\n",
            "⊙\n",
            "𝜀\n",
            ".\n",
            "{\\displaystyle \\mathbf {z} ={\boldsymbol {\\mu }}+{\boldsymbol {\\sigma }}\\odot {\boldsymbol {\u000barepsilon }}.}\n",
            "Grâce à cette transformation (qui peut être étendue à des distributions non gaussiennes), le VAE devient entraînable et le codeur probabiliste doit apprendre à mapper une représentation compressée de l'entrée dans les deux vecteurs latents \n",
            "𝜇{\\displaystyle {\boldsymbol {\\mu }}} et \n",
            "𝜎{\\displaystyle {\boldsymbol {\\sigma }}}, tandis que la stochasticité reste exclue du processus de mise à jour et est injectée dans l'espace latent en tant qu'entrée externe via le vecteur aléatoire \n",
            "𝜀{\\displaystyle {\boldsymbol {\u000barepsilon }}} .\n",
            "\n",
            "Références\n",
            "Diederik P. Kingma et Max Welling, « Auto-Encoding Variational Bayes », arXiv:1312.6114 [cs, stat], 0e 1er mai 2014 (lire en ligne [archive], consulté le 1er juillet 2022)\n",
            "(en) Kramer, « Nonlinear principal component analysis using autoassociative neural networks », AIChE Journal, vol. 37, no 2, 0e 1991, p. 233–243 (DOI 10.1002/aic.690370209, lire en ligne [archive])\n",
            "(en) Hinton et Salakhutdinov, « Reducing the Dimensionality of Data with Neural Networks », Science, vol. 313, no 5786, 0e 28 juillet 2006, p. 504–507 (PMID 16873662, DOI 10.1126/science.1127647, Bibcode 2006Sci...313..504H, S2CID 1658773, lire en ligne [archive])\n",
            "(en) « A Beginner's Guide to Variational Methods: Mean-Field Approximation [archive] », Eric Jang, 8 juillet 2016\n",
            "Wei-Ning Hsu, Yu Zhang et James Glass, 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), décembre 2017, 16–23 p. (ISBN 978-1-5090-4788-8, DOI 10.1109/ASRU.2017.8268911, arXiv 1707.06265, S2CID 22681625), « Unsupervised domain adaptation for robust speech recognition via variational autoencoder-based data augmentation »\n",
            "M. Ehsan Abbasnejad, Anthony Dick et Anton van den Hengel, Infinite Variational Autoencoder for Semi-Supervised Learning, 2017, 5888–5897 p. (lire en ligne [archive])\n",
            "(en) Xu, Sun, Deng et Tan, « Variational Autoencoder for Semi-Supervised Text Classification », Proceedings of the AAAI Conference on Artificial Intelligence, vol. 31, no 1, 0e 12 février 2017 (lire en ligne [archive])\n",
            "Kameoka, Li, Inoue et Makino, « Supervised Determined Source Separation with Multichannel Variational Autoencoder », Neural Computation, vol. 31, no 9, 0e 1er septembre 2019, p. 1891–1914 (PMID 31335290, DOI 10.1162/neco_a_01217, S2CID 198168155, lire en ligne [archive])\n",
            "An, J., & Cho, S. (2015). Variational autoencoder based anomaly detection using reconstruction probability. Special Lecture on IE, 2(1).\n",
            "Kingma et Welling, « An Introduction to Variational Autoencoders », Foundations and Trends in Machine Learning, vol. 12, no 4, 0e 2019, p. 307–392 (ISSN 1935-8237, DOI 10.1561/2200000056, arXiv 1906.02691, S2CID 174802445)\n",
            "(en) « From Autoencoder to Beta-VAE [archive] », Lil'Log, 12 août 2018\n",
            "Bengio, Courville et Vincent, « Representation Learning: A Review and New Perspectives », IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no 8, 0e 2013, p. 1798–1828 (ISSN 1939-3539, PMID 23787338, DOI 10.1109/TPAMI.2013.50, arXiv 1206.5538, S2CID 393948, lire en ligne [archive])\n",
            "Auto-encodeur variationnel\n",
            "\n",
            "Article\n",
            "Discussion\n",
            "Lire\n",
            "Modifier\n",
            "Modifier le code\n",
            "Voir l’historique\n",
            "\n",
            "Outils\n",
            "En apprentissage automatique, un auto-encodeur variationnel (ou VAE de l'anglais variational auto encoder)1, est une architecture de réseau de neurones artificiels introduite en 2013 par D. Kingma et M. Welling, appartenant aux familles des modèles graphiques probabilistes et des méthodes bayésiennes variationnelles.\n",
            "\n",
            "Les VAE sont souvent rapprochés des autoencodeurs2,3 en raison de leur architectures similaires. Leur utilisation et leur formulation mathématiques sont cependant différentes.\n",
            "\n",
            "Les auto-encodeurs variationnels permettent de formuler un problème d'inférence statistique (par exemple, déduire la valeur d'une variable aléatoire à partir d'une autre variable aléatoire) en un problème d'optimisation statistique (c'est-à-dire trouver les valeurs de paramètres qui minimisent une fonction objectif)4. Ils représentent une fonction associant à une valeur d'entrée une distribution latente multivariée, qui n'est pas directement observée mais déduite depuis un modèle mathématique à partir de la distribution d'autres variables. Bien que ce type de modèle ait été initialement conçu pour l'apprentissage non supervisé5, son efficacité a été prouvée pour l'apprentissage semi-supervisé6,7 et l'apprentissage supervisé8.\n",
            "\n",
            "Architecture\n",
            "Dans un VAE, les données d'entrée sont échantillonnées à partir d'une distribution paramétrée (la distribution a priori, en termes d'inférence bayésienne), et l'encodeur et le décodeur sont entraînés conjointement de sorte que la sortie minimise une erreur de reconstruction dans le sens de la divergence de Kullback-Leibler entre la distribution paramétrique postérieure et la vraie distribution a posteriori9,10.\n",
            "\n",
            "Formulation\n",
            "\n",
            "Le schéma de base d'un auto-encodeur variationnel. Le modèle reçoit \n",
            "聥\n",
            "{\\displaystyle \\mathbf {x} } comme entrée. L'encodeur le comprime dans l'espace latent. Le décodeur reçoit en entrée les informations prélevées dans l'espace latent et produit \n",
            "聥\n",
            "′\n",
            "{\\displaystyle \\mathbf {x'} } aussi semblable que possible à \n",
            "聥\n",
            "{\\displaystyle \\mathbf {x} } .\n",
            "On note \n",
            "聥\n",
            "{\\displaystyle \\mathbf {x} } le vecteur contenant l'ensemble des variables observées que l'on souhaite modéliser. Ce vecteur est une variable aléatoire, caractérisé par une distribution de probabilité inconnue \n",
            "聃\n",
            "(\n",
            "聥\n",
            ")\n",
            "{\\displaystyle P(\\mathbf {x} )}, que l'on souhaite approximer par une distribution paramétrée \n",
            "嵝\n",
            "椃{\\displaystyle p_{\theta }} ayant pour paramètres \n",
            "蘃{\\displaystyle \theta }.\n",
            "\n",
            "On introduit alors un vecteur aléatoire \n",
            "聧\n",
            "{\\displaystyle \\mathbf {z} } distribué conjointement avec \n",
            "聥\n",
            "{\\displaystyle \\mathbf {x} } (c'est-à-dire dont la loi de probabilité n'est pas indépendante de celle de \n",
            "聥\n",
            "{\\displaystyle \\mathbf {x} }). Ce vecteur \n",
            "聧\n",
            "{\\displaystyle \\mathbf {z} } représente un encodage latent de \n",
            "乥\n",
            "{\\displaystyle \\mathbf {x} }, que l'on ne peut observer directement.\n",
            "\n",
            "On exprime alors la distribution \n",
            "聝\n",
            "椃{\\displaystyle p_{\theta }} via la loi de probablitié marginale sur \n",
            "聧\n",
            "{\\displaystyle \\mathbf {z} }, ce qui donne alors:\n",
            "\n",
            "聝\n",
            "椃\n",
            "(\n",
            "乥\n",
            ")\n",
            "=\n",
            "琫\n",
            "乧\n",
            "聝\n",
            "椃\n",
            "(\n",
            "乥\n",
            ",\n",
            "乧\n",
            ")\n",
            "乑\n",
            "聧\n",
            ",\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} )=\\int _{\\mathbf {z} }p_{\theta }(\\mathbf {x,z} )\\,d\\mathbf {z} ,}\n",
            "où \n",
            "聝\n",
            "椃\n",
            "(\n",
            "乥\n",
            ",\n",
            "乧\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x,z} )} représente la distribution conjointe sous \n",
            "聝\n",
            "椃{\\displaystyle p_{\theta }} des données observables \n",
            "聥\n",
            "{\\displaystyle \\mathbf {x} } et de leur représentation latente \n",
            "聧\n",
            "{\\displaystyle \\mathbf {z} }. Selon la formule des probabilités composées, l'équation peut être réécrite comme\n",
            "\n",
            "聝\n",
            "椃\n",
            "(\n",
            "聥\n",
            ")\n",
            "=\n",
            "∫\n",
            "聧\n",
            "聝\n",
            "椃\n",
            "(\n",
            "聥\n",
            "∣\n",
            "聧\n",
            ")\n",
            "九\n",
            "餃\n",
            "(\n",
            "聧\n",
            ")\n",
            "陑\n",
            "乧\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} )=\\int _{\\mathbf {z} }p_{\theta }(\\mathbf {x\\mid z} )p_{\theta }(\\mathbf {z} )\\,d\\mathbf {z} }\n",
            "Dans l'auto-encodeur variationnel classique, on fait l'hypothèse que \n",
            "聧\n",
            "{\\displaystyle \\mathbf {z} } est un vecteur à valeur réelles de dimension finie, et \n",
            "聝\n",
            "椃\n",
            "(\n",
            "聥\n",
            "|\n",
            "乧\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x|z} )} suit une loi normale. Par conséquent, \n",
            "聝\n",
            "椃\n",
            "(\n",
            "聥\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} )} est un mélange de distributions gaussiennes.\n",
            "\n",
            "On peut voir les relations entre les données d'entrée et leur représentation latente comme un problème d'inférence bayésienne avec\n",
            "\n",
            "聝\n",
            "椃\n",
            "(\n",
            "乧\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {z} )} représente la distribution probabilité a priori dans l'espace latent\n",
            "聝\n",
            "椃\n",
            "(\n",
            "乥\n",
            "∣\n",
            "乧\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} \\mid \\mathbf {z} )} représente la vraisemblance\n",
            "聝\n",
            "椃\n",
            "(\n",
            "乧\n",
            "∣\n",
            "乥\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {z} \\mid \\mathbf {x} )} représente la distribution de probabilité a posteriori\n",
            "Malheureusement, le calcul de \n",
            "九\n",
            "夃\n",
            "(\n",
            "聥\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} )} est au mieux coûteux, et dans la plupart des cas, impossible. Pour résoudre ce problème, il est nécessaire d'introduire une autre fonction \n",
            "聞\n",
            "Φ{\\displaystyle q_{\\Phi }} pour approximer la distribution a posteriori :\n",
            "\n",
            "聞\n",
            "Φ\n",
            "(\n",
            "聧\n",
            "∣\n",
            "聥\n",
            ")\n",
            "≈\n",
            "九\n",
            "夃\n",
            "(\n",
            "聧\n",
            "∣\n",
            "聥\n",
            ")\n",
            "{\\displaystyle q_{\\Phi }(\\mathbf {z\\mid x} )\u0007pprox p_{\theta }(\\mathbf {z\\mid x} )}\n",
            "où \n",
            "Φ{\\displaystyle \\Phi } est l'ensemble des paramètres de \n",
            "聞\n",
            "{\\displaystyle q} .\n",
            "\n",
            "Ainsi le problème est formulé pour pouvoir être appliqué dans une architecture de réseau de neurones auto-encodeur, dans lequel la distribution de vraisemblance conditionnelle \n",
            "聝\n",
            "椃\n",
            "(\n",
            "乥\n",
            "∣\n",
            "乧\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {x} \\mid \\mathbf {z} )} est représentée par un décodeur probabiliste, tandis que la distribution a posteriori approchée \n",
            "聞\n",
            "Φ\n",
            "(\n",
            "聧\n",
            "∣\n",
            "聥\n",
            ")\n",
            "{\\displaystyle q_{\\Phi }(\\mathbf {z} \\mid \\mathbf {x} )} est représentée par un codeur probabiliste. La mise en œuvre d'un VAE consistera donc à calculer les valeurs optimales des paramètres \n",
            "椃{\\displaystyle \theta } et \n",
            "Φ{\\displaystyle \\Phi } par un apprentissage automatique.\n",
            "\n",
            "Fonction de perte ELBO\n",
            "Comme dans tout problème d'apprentissage profond, il est nécessaire de définir une fonction de perte différentiable afin de mettre à jour les poids du réseau par rétropropagation lors de l'apprentissage.\n",
            "\n",
            "Pour les auto-encodeurs variationnels, l'idée est de minimiser conjointement les paramètres du modèle génératif \n",
            "餃{\\displaystyle \theta } pour réduire l'erreur de reconstruction entre l'entrée et la sortie, et \n",
            "Φ{\\displaystyle \\Phi } pour avoir \n",
            "聞\n",
            "Φ\n",
            "(\n",
            "聧\n",
            "∣\n",
            "聥\n",
            ")\n",
            "{\\displaystyle q_{\\Phi }(\\mathbf {z\\mid x} )}, la distribution postérieure approchée, le plus près possible de \n",
            "聝\n",
            "椃\n",
            "(\n",
            "乧\n",
            "∣\n",
            "乥\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {z} \\mid \\mathbf {x} )}, la vraie distribution de probabilité a posteriori.\n",
            "\n",
            "Comme fonction de coût pour la reconstruction, l'erreur quadratique moyenne et l'entropie croisée sont souvent utilisées.\n",
            "\n",
            "Pour la fonction de coût de distance entre les deux distributions, la divergence inverse de Kullback – Leibler \n",
            "耷\n",
            "举\n",
            "耿\n",
            "(\n",
            "聞\n",
            "Φ\n",
            "(\n",
            "乧\n",
            "∣\n",
            "乥\n",
            ")\n",
            "∥\n",
            "聝\n",
            "椃\n",
            "(\n",
            "乧\n",
            "∣\n",
            "乥\n",
            ")\n",
            ")\n",
            "{\\displaystyle D_{KL}(q_{\\Phi }(\\mathbf {z\\mid x} )\\parallel p_{\theta }(\\mathbf {z\\mid x} ))} est un bon choix pour pousser \n",
            "聞\n",
            "Φ\n",
            "(\n",
            "聧\n",
            "∣\n",
            "聥\n",
            ")\n",
            "{\\displaystyle q_{\\Phi }(\\mathbf {z\\mid x} )} en dessous de \n",
            "聝\n",
            "椃\n",
            "(\n",
            "乧\n",
            "∣\n",
            "乥\n",
            ")\n",
            "{\\displaystyle p_{\theta }(\\mathbf {z} \\mid \\mathbf {x} )}1,11.\n",
            "\n",
            "Reparamétrisation\n",
            "\n",
            "Le schéma de l'astuce de reparamétrisation. La variable aléatoire \n",
            "朚{\\displaystyle \\mathbf {\u000barepsilon } } est injecté dans l'espace latent, non observé, \n",
            "聧\n",
            "{\\displaystyle \\mathbf {z} } comme entrée externe. De cette manière, il est possible de rétropropager le gradient sans impliquer de variable stochastique lors de la mise à jour.\n",
            "Pour rendre la formulation ELBO adaptée à des fins d'apprentissage, il est nécessaire de modifier légèrement la formulation du problème et la structure du VAE12.\n",
            "\n",
            "L'échantillonnage stochastique est l'opération non différentiable par laquelle il est possible d'échantillonner à partir de l'espace latent et d'alimenter le décodeur probabiliste.\n",
            "\n",
            "L'hypothèse principale sur l'espace latent est qu'il peut être considéré comme un ensemble de distributions gaussiennes multivariées, et peut donc être décrit comme\n",
            "\n",
            "蹧\n",
            "∼\n",
            "聞\n",
            "Φ\n",
            "(\n",
            "聧\n",
            "∣\n",
            "聥\n",
            ")\n",
            "=\n",
            "陁\n",
            "(\n",
            "稇\n",
            ",\n",
            "椎\n",
            "2\n",
            ")\n",
            "{\\displaystyle \\mathbf {z} \\sim q_{\\Phi }(\\mathbf {z} \\mid \\mathbf {x} )={\\mathcal {N}}({\boldsymbol {\\mu }},{\boldsymbol {\\sigma }}^{2})} .\n",
            "\n",
            "Le schéma d'un auto-encodeur variationnel après l'astuce de reparamétrisation.\n",
            "Etant donné \n",
            "榤\n",
            "∼\n",
            "聁\n",
            "(\n",
            "0\n",
            ",\n",
            "耼\n",
            ")\n",
            "{\\displaystyle {\boldsymbol {\u000barepsilon }}\\sim {\\mathcal {N}}(0,{\boldsymbol {I}})} et \n",
            "⊙{\\displaystyle \\odot } défini comme le produit élément par élément, l'astuce de reparamétrisation modifie l'équation ci-dessus comme\n",
            "\n",
            "乧\n",
            "=\n",
            "万\n",
            "+\n",
            "椎\n",
            "⊙\n",
            "介\n",
            ".\n",
            "{\\displaystyle \\mathbf {z} ={\boldsymbol {\\mu }}+{\boldsymbol {\\sigma }}\\odot {\boldsymbol {\u000barepsilon }}.}\n",
            "Grâce à cette transformation (qui peut être étendue à des distributions non gaussiennes), le VAE devient entraînable et le codeur probabiliste doit apprendre à mapper une représentation compressée de l'entrée dans les deux vecteurs latents \n",
            "万{\\displaystyle {\boldsymbol {\\mu }}} et \n",
            "椎{\\displaystyle {\boldsymbol {\\sigma }}}, tandis que la stochasticité reste exclue du processus de mise à jour et est injectée dans l'espace latent en tant qu'entrée externe via le vecteur aléatoire \n",
            "朚{\\displaystyle {\boldsymbol {\u000barepsilon }}} .\n",
            "\n",
            "Références\n",
            "Diederik P. Kingma et Max Welling, « Auto-Encoding Variational Bayes », arXiv:1312.6114 [cs, stat], 0e 1er mai 2014 (lire en ligne [archive], consulté le 1er juillet 2022)\n",
            "(en) Kramer, « Nonlinear principal component analysis using autoassociative neural networks », AIChE Journal, vol. 37, no 2, 0e 1991, p. 233–243 (DOI 10.1002/aic.690370209, lire en ligne [archive])\n",
            "(en) Hinton et Salakhutdinov, « Reducing the Dimensionality of Data with Neural Networks », Science, vol. 313, no 5786, 0e 28 juillet 2006, p. 504–507 (PMID 16873662, DOI 10.1126/science.1127647, Bibcode 2006Sci...313..504H, S2CID 1658773, lire en ligne [archive])\n",
            "(en) « A Beginner's Guide to Variational Methods: Mean-Field Approximation [archive] », Eric Jang, 8 juillet 2016\n",
            "Wei-Ning Hsu, Yu Zhang et James Glass, 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), décembre 2017, 16–23 p. (ISBN 978-1-5090-4788-8, DOI 10.1109/ASRU.2017.8268911, arXiv 1707.06265, S2CID 22681625), « Unsupervised domain adaptation for robust speech recognition via variational autoencoder-based data augmentation »\n",
            "M. Ehsan Abbasnejad, Anthony Dick et Anton van den Hengel, Infinite Variational Autoencoder for Semi-Supervised Learning, 2017, 5888–5897 p. (lire en ligne [archive])\n",
            "(en) Xu, Sun, Deng et Tan, « Variational Autoencoder for Semi-Supervised Text Classification », Proceedings of the AAAI Conference on Artificial Intelligence, vol. 31, no 1, 0e 12 février 2017 (lire en ligne [archive])\n",
            "Kameoka, Li, Inoue et Makino, « Supervised Determined Source Separation with Multichannel Variational Autoencoder », Neural Computation, vol. 31, no 9, 0e 1er septembre 2019, p. 1891–1914 (PMID 31335290, DOI 10.1162/neco_a_01217, S2CID 198168155, lire en ligne [archive])\n",
            "An, J., & Cho, S. (2015). Variational autoencoder based anomaly detection using reconstruction probability. Special Lecture on IE, 2(1).\n",
            "Kingma et Welling, « An Introduction to Variational Autoencoders », Foundations and Trends in Machine Learning, vol. 12, no 4, 0e 2019, p. 307–392 (ISSN 1935-8237, DOI 10.1561/2200000056, arXiv 1906.02691, S2CID 174802445)\n",
            "(en) « From Autoencoder to Beta-VAE [archive] », Lil'Log, 12 août 2018\n",
            "Bengio, Courville et Vincent, « Representation Learning: A Review and New Perspectives », IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no 8, 0e 2013, p. 1798–1828 (ISSN 1939-3539, PMID 23787338, DOI 10.1109/TPAMI.2013.50, arXiv 1206.5538, S2CID 393948, lire en ligne [archive])\u0000\n",
            "0.9885127491079976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(__s))\n",
        "print(__e.shape)\n",
        "tf.print(__e[:4, :8], summarize=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAKttpNX2M8_",
        "outputId": "bbcc5f7c-e109-45e1-bc20-b99e86be792d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "134\n",
            "(34, 256)\n",
            "[[3.17276168 1.53056908 2.41119337 0.0258403085 1.5207386 1.66698301 2.24263883 2.11223722]\n",
            " [2.65205669 1.68546355 2.01416564 0.655108571 2.3957293 1.70228446 2.12328672 2.04205203]\n",
            " [2.4943645 0.441500723 1.79073346 2.31724644 1.87132716 1.36434507 3.37104845 2.3522613]\n",
            " [2.87078524 1.11898732 2.12827492 0.995271683 0.403087556 0.974042118 1.82035911 2.90426946]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "__t = list(set(tokun.pipeline.chunk(seq=SAMPLES[-1], size=4, repeats=False)))\n",
        "__x = tokun.pipeline.preprocess(text=''.join(__t), groups=N_TOKEN_DIM, flatten=True)\n",
        "__e = MODEL._encoder(__x)"
      ],
      "metadata": {
        "id": "dKFlLJbz3IBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlable.io.write(data=__t, path='./metadata.tsv', tsv=False)\n",
        "mlable.io.write(data=__e.numpy(), path='./embeddings.tsv', tsv=True)"
      ],
      "metadata": {
        "id": "Ttnox9Df6AEQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}