{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xXM7DoPpds1"
      },
      "source": [
        "## Import deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W24EKFXaO5yC"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets mlable tokun"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U --no-index -f '/content/libs/' tokun"
      ],
      "metadata": {
        "id": "gKU9BSWyhcX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXU-Ebl2pddk"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import functools\n",
        "import itertools\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import urllib.request\n",
        "\n",
        "import datasets as hd\n",
        "import tensorflow as tf\n",
        "\n",
        "import mlable.data\n",
        "import mlable.maths.ops\n",
        "import mlable.metrics\n",
        "import mlable.sampling\n",
        "import mlable.shaping.axes\n",
        "import mlable.shaping.hilbert\n",
        "import mlable.text\n",
        "\n",
        "import tokun.data\n",
        "import tokun.eval\n",
        "import tokun.models.meta\n",
        "import tokun.models.vqvae\n",
        "import tokun.pipeline.flat.preprocess\n",
        "import tokun.pipeline.flat.postprocess\n",
        "import tokun.pipeline.hilbert.preprocess\n",
        "import tokun.pipeline.hilbert.postprocess\n",
        "import tokun.pipeline.square.preprocess\n",
        "import tokun.pipeline.square.postprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pn1ywhSrpin9"
      },
      "outputs": [],
      "source": [
        "print(\"Tensorflow version \" + tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Setup the GPU / TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWMvVNSAJK_n"
      },
      "outputs": [],
      "source": [
        "# MIXED PRECISION ##############################################################\n",
        "\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_bfloat16')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFIMfPmgQa0h"
      },
      "outputs": [],
      "source": [
        "# DEVICES ######################################################################\n",
        "\n",
        "tf.debugging.set_log_device_placement(False)\n",
        "\n",
        "CPU = tf.config.list_logical_devices('CPU')\n",
        "GPU = tf.config.list_logical_devices('GPU')\n",
        "TPU = tf.config.list_logical_devices('TPU')\n",
        "\n",
        "if TPU:\n",
        "    RESOLVER = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(RESOLVER)\n",
        "    tf.tpu.experimental.initialize_tpu_system(RESOLVER)\n",
        "    DISTRIBUTION_STRATEGY = tf.distribute.TPUStrategy(RESOLVER)\n",
        "elif GPU:\n",
        "    DISTRIBUTION_STRATEGY = tf.distribute.MirroredStrategy(GPU)\n",
        "else:\n",
        "    DISTRIBUTION_STRATEGY = tf.distribute.MirroredStrategy(CPU)\n",
        "\n",
        "print(DISTRIBUTION_STRATEGY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9066X5EOyAX"
      },
      "source": [
        "## Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFSPMtQaO1fu"
      },
      "outputs": [],
      "source": [
        "# TOGGLE #######################################################################\n",
        "\n",
        "IMPORT = False\n",
        "DOWNLOAD = False\n",
        "TRAINING = True\n",
        "RANDOM = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t1jfsJlM3SX"
      },
      "source": [
        "## Defining The Metadata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COMMON PARAMETERS ############################################################\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    'batch_dim': 64,\n",
        "    'token_dim': 1, # T\n",
        "    'input_dim': 256, # U_i (bytes)\n",
        "    'embed_dim': 64, # E\n",
        "    'binary_dim': 8, # U_o (8 bits)\n",
        "    'order_num': 5,\n",
        "    'rank_num': 2,\n",
        "    'trainable': True,\n",
        "    'bigendian': True,\n",
        "    'epochs': 8,\n",
        "    'steps': 2 ** 14,}"
      ],
      "metadata": {
        "id": "PfHiXTM8WGE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z74MlibMWnu"
      },
      "outputs": [],
      "source": [
        "# MODEL PARAMETERS #############################################################\n",
        "\n",
        "MODEL_FACTORY = tokun.models.vqvae.QuantizedAutoEncoder\n",
        "\n",
        "MODEL_CONFIG = {\n",
        "    'token_dim': BASE_CONFIG['token_dim'], # T\n",
        "    'input_dim': BASE_CONFIG['input_dim'], # U_i (bytes)\n",
        "    'embed_dim': BASE_CONFIG['embed_dim'], # E\n",
        "    'binary_dim': BASE_CONFIG['binary_dim'], # U_o (8 bits)\n",
        "    'trainable': BASE_CONFIG['trainable'],}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DERIVED MODEL PARAMETERS #####################################################\n",
        "\n",
        "META_CONFIG = {\n",
        "    'version': '{}x{}x{}'.format(BASE_CONFIG['input_dim'], BASE_CONFIG['embed_dim'], BASE_CONFIG['token_dim']),\n",
        "    'label': '6.1',}\n",
        "\n",
        "IO_CONFIG = {\n",
        "    'url': 'https://github.com/apehex/tokun/raw/main/models/{}/{}/{}/{}.keras'.format(*META_CONFIG['version'], META_CONFIG['label']),\n",
        "    'path': 'tokun.keras',}"
      ],
      "metadata": {
        "id": "HMA0OZAGdq-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21J7WEkhSwph"
      },
      "outputs": [],
      "source": [
        "# TRAINING PARAMETERS ##########################################################\n",
        "\n",
        "TRAINING_CONFIG = {\n",
        "    'epochs': BASE_CONFIG['epochs'],\n",
        "    'batch_size': None,\n",
        "    'validation_split': None,\n",
        "    'validation_freq': list(range(1, 9)),\n",
        "    # 'class_weight': {__c: 1. if __c == 0 else 1. for __c in range(256)}, # there are 3 times more 0s than other bytes\n",
        "    'verbose': 1,}\n",
        "\n",
        "OPTIMIZER_CONFIG = {\n",
        "    'learning_rate': 0.01 * (0.1 if IMPORT else 1.0),\n",
        "    'weight_decay': 0.001,\n",
        "    'beta_1': 0.9,\n",
        "    'beta_2': 0.95,\n",
        "    'epsilon': 1e-6,\n",
        "    'clipnorm': 0.1,\n",
        "    'amsgrad': False,\n",
        "    'use_ema': False,\n",
        "    'ema_momentum': 0.99,\n",
        "    'ema_overwrite_frequency': 1024,}\n",
        "    # 'gradient_accumulation_steps': 2,\n",
        "\n",
        "SCHEDULER_CONFIG = {\n",
        "    'initial_learning_rate': OPTIMIZER_CONFIG['learning_rate'],\n",
        "    'decay_steps': TRAINING_CONFIG['epochs'] * BASE_CONFIG['steps'],\n",
        "    'alpha': 0.01,\n",
        "    'name': 'cosine_lr',\n",
        "    'warmup_target': None,\n",
        "    'warmup_steps': 0,}\n",
        "\n",
        "LOSS_CONFIG = {\n",
        "    'from_logits': True,\n",
        "    'label_smoothing': 0.0,\n",
        "    'axis': -1,\n",
        "    'reduction': 'sum_over_batch_size',\n",
        "    'name': 'ce_loss',}\n",
        "\n",
        "METRICS_CONFIG = {\n",
        "    'depth': 8,\n",
        "    'from_logits': True,}\n",
        "\n",
        "CHECKPOINT_CONFIG = {\n",
        "    'filepath': IO_CONFIG['path'],\n",
        "    'monitor': 'val_loss',\n",
        "    'mode': 'auto',\n",
        "    'save_freq': 'epoch',\n",
        "    'save_best_only': False,\n",
        "    'save_weights_only': False,\n",
        "    'verbose': 1,}\n",
        "\n",
        "TENSORBOARD_CONFIG = {\n",
        "    'log_dir': os.path.join('.logs/', META_CONFIG['version'], datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")),\n",
        "    'histogram_freq': 1,\n",
        "    'embeddings_freq': 1,\n",
        "    'profile_batch': (0, 4),\n",
        "    'write_graph': True,\n",
        "    'write_images': True,}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPROCESSING ################################################################\n",
        "\n",
        "PIPELINE_FACTORY = tokun.pipeline.flat.preprocess.factory\n",
        "\n",
        "BATCH_CONFIG = {\n",
        "    'batch_size': BASE_CONFIG['batch_dim'],\n",
        "    'drop_remainder': True,\n",
        "    'num_parallel_calls': tf.data.AUTOTUNE,}\n",
        "\n",
        "PIPELINE_CONFIG = {\n",
        "    'batch_dim': BATCH_CONFIG['batch_size'],\n",
        "    'sample_dim': BASE_CONFIG['token_dim'] * (1 << (BASE_CONFIG['order_num'] * BASE_CONFIG['rank_num'])),\n",
        "    'token_dim': BASE_CONFIG['token_dim'],\n",
        "    'drop_dim': 0,\n",
        "    # 'order_num': BASE_CONFIG['order_num'],\n",
        "    # 'rank_num': BASE_CONFIG['rank_num'],\n",
        "    'pattern': '\\x1b\\[[0-9;]*[mGKHF]',\n",
        "    'rewrite': '',\n",
        "    'separator': '\\u001d',\n",
        "    'encoding': 'UTF-8',\n",
        "    'bigendian': BASE_CONFIG['bigendian'],}"
      ],
      "metadata": {
        "id": "p4-naWbUdV4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# POSTPROCESSING ###############################################################\n",
        "\n",
        "POSTPROCESSING_FACTORY = tokun.pipeline.flat.postprocess.factory\n",
        "\n",
        "POSTPROCESSING_CONFIG = {\n",
        "    'drop_dim': PIPELINE_CONFIG['drop_dim'],\n",
        "    # 'order_num': PIPELINE_CONFIG['order_num'],\n",
        "    # 'rank_num': PIPELINE_CONFIG['rank_num'],\n",
        "    'encoding': PIPELINE_CONFIG['encoding'],\n",
        "    'bigendian': PIPELINE_CONFIG['bigendian'],\n",
        "    'threshold': 0.0,\n",
        "    'errors': 'replace',}"
      ],
      "metadata": {
        "id": "UdUxIlvYdTeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RANDOM DATASET ###############################################################\n",
        "\n",
        "RANDOM_CONFIG = {\n",
        "    'sample_count': BATCH_CONFIG['batch_size'] * BASE_CONFIG['steps'],\n",
        "    'sample_size': BASE_CONFIG['token_dim'] * (1 << (BASE_CONFIG['order_num'] * BASE_CONFIG['rank_num'])),}"
      ],
      "metadata": {
        "id": "RDLCxrmALIcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DATASETS #####################################################################\n",
        "\n",
        "DATASETS_CONFIG = {\n",
        "    # 'pt-fineweb-edu': {\n",
        "    #     'path': 'HuggingFaceFW/fineweb-edu',\n",
        "    #     'name': 'sample-10BT',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['text'],},\n",
        "    # 'pt-fineweb-kor': {\n",
        "    #     'path': 'HuggingFaceFW/fineweb-2',\n",
        "    #     'name': 'kor_Hang',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['text'],},\n",
        "    # 'pt-fineweb-fin': {\n",
        "    #     'path': 'HuggingFaceFW/fineweb-2',\n",
        "    #     'name': 'fin_Latn',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['text'],},\n",
        "    'pt-wikipedia': {\n",
        "        'path': 'wikimedia/wikipedia',\n",
        "        'name': '20231101.en',\n",
        "        'split': 'train',\n",
        "        'features': ['text'],},\n",
        "    # 'tp-wikipedia-1': {\n",
        "    #     'path': 'wikimedia/wikipedia',\n",
        "    #     'name': '20231101.en',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['text'],},\n",
        "    # 'tp-wikipedia-2': {\n",
        "    #     'path': 'wikimedia/wikipedia',\n",
        "    #     'name': '20231101.en',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['text'],},\n",
        "    # 'ft-retro-ascii-art': {\n",
        "    #     'path': 'jdpressman/retro-ascii-art-v1',\n",
        "    #     'name': None,\n",
        "    #     'train': 'train',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['prompt', 'art_aic'],},\n",
        "    # 'ft-stack-exchange': {\n",
        "    #     'path': 'Alignment-Lab-AI/Stack-Exchange-April',\n",
        "    #     'name': None,\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['question', 'answer'],},\n",
        "    # 'ft-math': {\n",
        "    #     'path': 'HuggingFaceTB/finemath',\n",
        "    #     'name': 'finemath-3plus',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['text'],},\n",
        "    # 'cot-text-dolphin': {\n",
        "    #     'path': 'cognitivecomputations/dolphin-r1',\n",
        "    #     'name': 'reasoning-deepseek',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['reasoning', 'answer'],},\n",
        "    # 'cot-text-openthoughts': {\n",
        "    #     'path': 'open-thoughts/OpenThoughts-114k',\n",
        "    #     'name': 'default',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['problem', 'solution'],},\n",
        "    # 'ft-asciiart-asciiart': {\n",
        "    #     'path': 'apehex/ascii-art',\n",
        "    #     'name': 'asciiart',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['content'],},\n",
        "    # 'ft-asciiart-copypasta': {\n",
        "    #     'path': 'apehex/ascii-art',\n",
        "    #     'name': 'copypasta',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['content'],},\n",
        "    # 'ft-asciiart-graffiti': {\n",
        "    #     'path': 'apehex/ascii-art',\n",
        "    #     'name': 'graffiti',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['content'],},\n",
        "    # 'ft-asciiart-images': {\n",
        "    #     'path': 'apehex/ascii-art',\n",
        "    #     'name': 'images',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['content'],},\n",
        "    # 'ft-asciiart-datacompdr': {\n",
        "    #     'path': 'apehex/ascii-art-datacompdr-12m',\n",
        "    #     'name': 'default',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['content'],},\n",
        "    # 'cot-math-numi': {\n",
        "    #     'path': 'AI-MO/NuminaMath-CoT',\n",
        "    #     'name': None,\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['problem', 'solution'],},\n",
        "}"
      ],
      "metadata": {
        "id": "sdzyDZfKnMw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading The Model Weights"
      ],
      "metadata": {
        "id": "dNF00bM5xj9O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfIZb86Fg0dQ"
      },
      "outputs": [],
      "source": [
        "# IMPORT #######################################################################\n",
        "\n",
        "if IMPORT and DOWNLOAD:\n",
        "    urllib.request.urlretrieve(IO_CONFIG['url'], IO_CONFIG['path'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEyFtkcFNGe4"
      },
      "source": [
        "## Downloading The Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTK1MPV8qek5"
      },
      "outputs": [],
      "source": [
        "# DOWNLOAD #####################################################################\n",
        "\n",
        "DATASETS = {\n",
        "    __name: hd.load_dataset(path=__args['path'], name=__args['name'], split=__args['split']).to_tf_dataset(shuffle=True, batch_size=None)\n",
        "    for __name, __args in DATASETS_CONFIG.items()}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RANDOM #######################################################################\n",
        "\n",
        "RANDOM_TRAIN = tokun.data.random_dataset_of_bytes(**RANDOM_CONFIG)\n",
        "RANDOM_TEST = tokun.data.random_dataset_of_bytes(**{__k: (128 if 'count' in __k else __v) for __k, __v in RANDOM_CONFIG.items()})"
      ],
      "metadata": {
        "id": "Hx48BANQKq6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STATS ########################################################################\n",
        "\n",
        "STATS = {__n: mlable.data.stats(dataset=DATASETS[__n], features=DATASETS_CONFIG[__n]['features'], count=2048) for __n in DATASETS}\n",
        "\n",
        "print(STATS)"
      ],
      "metadata": {
        "id": "hlo20AczsU1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VIZ ##########################################################################\n",
        "\n",
        "# __i = iter(DATASETS['ft-asciiart-datacompdr'])"
      ],
      "metadata": {
        "id": "9_LXytNaVsnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# __s = next(__i)\n",
        "# print(__s['caption'].numpy().decode('utf-8'), __s['labels'].numpy().decode('utf-8'), len(__s['content'].numpy().decode('utf-8')))\n",
        "# print(__s['content'].numpy().decode('utf-8'))"
      ],
      "metadata": {
        "id": "oNENi7nQV4Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cheN52OEchs"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gp2WitYVhs8I"
      },
      "outputs": [],
      "source": [
        "# ITERATE ######################################################################\n",
        "\n",
        "for __name in DATASETS:\n",
        "    # specialized preprocessing fn\n",
        "    __preprocess = PIPELINE_FACTORY(\n",
        "        features=DATASETS_CONFIG[__name]['features'],\n",
        "        **PIPELINE_CONFIG)\n",
        "    # apply\n",
        "    DATASETS[__name] = DATASETS[__name].batch(**BATCH_CONFIG).map(__preprocess, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RANDOM #######################################################################\n",
        "\n",
        "def preprocess_bytes(inputs: tf.Tensor) -> tuple:\n",
        "    __inputs = mlable.shaping.axes.divide(inputs, axis=-1, factor=BASE_CONFIG['token_dim'], insert=True, right=True)\n",
        "    __targets = mlable.shaping.axes.merge(mlable.maths.ops.expand_base(__inputs, base=2, depth=8, bigendian=BASE_CONFIG['bigendian']), axis=-1, right=False)\n",
        "    return (tf.cast(__inputs, tf.int32), tf.cast(__targets, tf.float32))\n",
        "\n",
        "RANDOM_TRAIN = RANDOM_TRAIN.batch(**BATCH_CONFIG).map(preprocess_bytes, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "RANDOM_TEST = RANDOM_TEST.batch(**BATCH_CONFIG).map(preprocess_bytes, num_parallel_calls=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "fbLxi31ZL9EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnzCjLkrlI8d"
      },
      "outputs": [],
      "source": [
        "# CONCATENATE ##################################################################\n",
        "\n",
        "DATASET_KEYS = set(DATASETS.keys()) - {'random'}\n",
        "\n",
        "DATASET_ALL = functools.reduce(lambda __l, __r: __l.concatenate(__r), [DATASETS[__n] for __n in DATASET_KEYS])\n",
        "DATASET_DIM = DATASET_ALL.cardinality().numpy()\n",
        "\n",
        "DATASET_TEST = DATASET_ALL.take(128)\n",
        "DATASET_TRAIN = DATASET_ALL.skip(128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ3quJQ4EUKf"
      },
      "outputs": [],
      "source": [
        "# INSPECT ######################################################################\n",
        "\n",
        "__X, __T = next(iter(DATASET_TRAIN.take(1)))\n",
        "\n",
        "print(DATASET_TRAIN.element_spec)\n",
        "print(DATASET_TEST.element_spec)\n",
        "\n",
        "print(RANDOM_TRAIN.element_spec)\n",
        "print(RANDOM_TEST.element_spec)\n",
        "\n",
        "print('train: {:,}'.format(DATASET_TRAIN.cardinality().numpy()))\n",
        "print('test:  {:,}'.format(DATASET_TEST.cardinality().numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VIZ ##########################################################################\n",
        "\n",
        "# __x, __y = next(iter(DATASETS['ft-asciiart-datacompdr'][0]))"
      ],
      "metadata": {
        "id": "gnKKUwcJNVdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# __i = 1\n",
        "# print(b'\\n'.join(mlable.text.decode(mlable.text.codepoint(mlable.text.untrim(__x[__i], count=PIPELINE_CONFIG['drop_dim'], outof=4)), encoding='UTF-32-BE').numpy().tolist()).decode('utf-8'))"
      ],
      "metadata": {
        "id": "pz9HSIvg94zD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbkaXey44V5Q"
      },
      "source": [
        "## Init The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEpY1-vFIFX7"
      },
      "outputs": [],
      "source": [
        "# COMPILE ######################################################################\n",
        "\n",
        "with DISTRIBUTION_STRATEGY.scope():\n",
        "    # metrics\n",
        "    byte_accuracy = mlable.metrics.BinaryGroupAccuracy(group=1, name='byte_accuracy', **METRICS_CONFIG)\n",
        "    token_accuracy = mlable.metrics.BinaryGroupAccuracy(group=BASE_CONFIG['token_dim'], name='token_accuracy', **METRICS_CONFIG)\n",
        "    # cosing LR\n",
        "    OPTIMIZER_CONFIG['learning_rate'] = tf.keras.optimizers.schedules.CosineDecay(**SCHEDULER_CONFIG)\n",
        "    # weights\n",
        "    MODEL = MODEL_FACTORY(**MODEL_CONFIG)\n",
        "    if IMPORT and os.path.isfile(IO_CONFIG['path']): MODEL = tf.keras.models.load_model(IO_CONFIG['path'], compile=False)\n",
        "    # compile\n",
        "    MODEL.compile(\n",
        "        optimizer=tf.keras.optimizers.AdamW(**OPTIMIZER_CONFIG),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(**LOSS_CONFIG),\n",
        "        weighted_metrics=[byte_accuracy, token_accuracy])\n",
        "    # build\n",
        "    MODEL(__X, training=False)\n",
        "    MODEL.compute_metrics(__X, __T, __T)\n",
        "    MODEL.compute_loss(__X, __T, __T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuRwWdjpPQBM"
      },
      "outputs": [],
      "source": [
        "# INSPECT ######################################################################\n",
        "\n",
        "MODEL.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(MODEL.compute_loss(__X, __T, MODEL(__X)))\n",
        "tf.reduce_mean(tf.keras.losses.binary_crossentropy(__T, MODEL(__X), from_logits=True, axis=-1))"
      ],
      "metadata": {
        "id": "zN5DeRPbrIT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRkNkXthBwar"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beTpALmzFdu1"
      },
      "outputs": [],
      "source": [
        "# TRAIN ########################################################################\n",
        "\n",
        "if TRAINING:\n",
        "    with DISTRIBUTION_STRATEGY.scope():\n",
        "        # callbacks\n",
        "        cp_callback = tf.keras.callbacks.ModelCheckpoint(**CHECKPOINT_CONFIG)\n",
        "        tb_callback = tf.keras.callbacks.TensorBoard(**TENSORBOARD_CONFIG)\n",
        "        tn_callback = tf.keras.callbacks.TerminateOnNaN()\n",
        "        # fit model\n",
        "        TRAINING_HISTORY = MODEL.fit(\n",
        "            x=RANDOM_TRAIN.prefetch(tf.data.AUTOTUNE),\n",
        "            validation_data=RANDOM_TEST.prefetch(tf.data.AUTOTUNE),\n",
        "            callbacks=[cp_callback, tb_callback, tn_callback],\n",
        "            **TRAINING_CONFIG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHtROW1K1R7c"
      },
      "source": [
        "## Dataviz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EMBEDDINGS ###################################################################\n",
        "\n",
        "print(tf.math.reduce_mean(MODEL._encoder._layers[0].embeddings, axis=0))\n",
        "print(tf.math.reduce_std(MODEL._encoder._layers[0].embeddings, axis=0))"
      ],
      "metadata": {
        "id": "bI6fYfJQWwu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# POSTPROCESS ##################################################################\n",
        "\n",
        "__postprocess_greedy = POSTPROCESSING_FACTORY(**POSTPROCESSING_CONFIG)\n",
        "__postprocess_sampler = POSTPROCESSING_FACTORY(temp=1.0, topp=0.9, topk=4, **POSTPROCESSING_CONFIG)\n",
        "__postprocess_probs = POSTPROCESSING_FACTORY(**{__k: (0.5 if __k == 'threshold' else __v) for __k, __v in POSTPROCESSING_CONFIG.items()})"
      ],
      "metadata": {
        "id": "KZ67zbIOdJHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST DATASET #################################################################\n",
        "\n",
        "__i = 0\n",
        "__X, __T = next(iter(DATASET_TRAIN.take(1)))\n",
        "__Y = MODEL(__X, logits=True)"
      ],
      "metadata": {
        "id": "ziA7Tq8rs5gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__O_T = mlable.text.unpack(__postprocess_probs(__T))\n",
        "__O_P = mlable.text.unpack(__postprocess_sampler(__Y))"
      ],
      "metadata": {
        "id": "tRwOXoBg3AiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('# INPUT #################################################################\\n\\n' + __O_T[__i])\n",
        "print('\\n# OUTPUT ################################################################\\n\\n' + __O_P[__i])\n",
        "print('\\n# SCORE #################################################################\\n\\n' + str(tokun.eval.compare(__O_T[__i], __O_P[__i])))"
      ],
      "metadata": {
        "id": "009P_Tm22WBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiRVgG-oSfb7"
      },
      "outputs": [],
      "source": [
        "# SPECIFIC SAMPLES #############################################################\n",
        "\n",
        "SAMPLES = [\n",
        "    \"\"\"위키백과, 우리 모두의 백과사전.\\nt-분포 확률적 임베딩(t-SNE)은 데이터의 차원 축소에 사용되는 기계 학습 알고리즘 중 하나로, 2002년 샘 로이스Sam Rowise와 제프리 힌튼에 의해 개발되었다.[1] t-SNE는 비선형 차원 축소 기법으로, 고차원 데이터를 특히 2, 3차원 등으로 줄여 가시화하는데에 유용하게 사용된다. 구체적으로 t-SNE는 비슷한 데이터는 근접한 2, 3차원의 지점으로, 다른 데이터는 멀리 떨어진 지점으로 맵핑한다.\"\"\",\n",
        "    \"\"\"class Encoder(tf.keras.models.Model):\\n    def __init__(self, depth: int, token_dim: int, encoding_dim: int, embedding_dim: int, batch_dim: int=None, attention: bool=False, **kwargs) -> None:\\n        super(Encoder, self).__init__(**kwargs)\\n        self._encoder = tf.keras.Sequential([\\n            tf.keras.Input(shape=(encoding_dim,), batch_size=batch_dim, name='input'), # (B * G ^ D, U)\\n            tf.keras.layers.Dense(units=embedding_dim, activation=None, use_bias=False, kernel_initializer='glorot_uniform', bias_initializer=None, name='embed-1'),] # (B * G ^ D, U) => (B * G ^ D, E)\\n            + [tokun.layers.TokenizeBlock(left_axis=-2, right_axis=-1, token_dim=token_dim, attention=attention, name='tokenize' + (__i + 1) * '-4') for __i in range(depth)]) # (B * G ^ i, E) => (B * G ^ (i-1), E)\\n\\n    def call(self, x: tf.Tensor) -> tf.Tensor:\\n        return self._encoder(x)\\n\"\"\",\n",
        "    \"\"\"Hilbert curve\\n\\nThe Hilbert curve (also known as the Hilbert space-filling curve) is a continuous fractal space-filling curve first described by the German mathematician David Hilbert in 1891,[1] as a variant of the space-filling Peano curves discovered by Giuseppe Peano in 1890.[2]\\n\\nBecause it is space-filling, its Hausdorff dimension is 2 (precisely, its image is the unit square, whose dimension is 2 in any definition of dimension; its graph is a compact set homeomorphic to the closed unit interval, with Hausdorff dimension 1).\\n\\nThe Hilbert curve is constructed as a limit of piecewise linear curves. The length of the {\\\\displaystyle n}th curve is {\\\\displaystyle \\\\textstyle 2^{n}-{1 \\\\over 2^{n}}}, i.e., the length grows exponentially with {\\\\displaystyle n}, even though each curve is contained in a square with area {\\\\displaystyle 1}.\\n\\nImages\\n\\nFirst six iterations of the Hilbert curve\\n\\nHilbert curve, first order\\n\\nHilbert curves, first and second orders\\n\\nHilbert curves, first to third orders\\n\\nProduction rules\\n\\nHilbert curve, construction color-coded\\n\\nA 3-D Hilbert curve with color showing progression\\n\\nVariant, first three iterations[3]\\n\\nApplications and mapping algorithms\\n\\nBoth the true Hilbert curve and its discrete approximations are useful because they give a mapping between 1D and 2D space that preserves locality fairly well.[4] This means that two data points which are close to each other in one-dimensional space are also close to each other after folding. The converse cannot always be true.\\n\\nBecause of this locality property, the Hilbert curve is widely used in computer science. For example, the range of IP addresses used by computers can be mapped into a picture using the Hilbert curve. Code to generate the image would map from 2D to 1D to find the color of each pixel, and the Hilbert curve is sometimes used because it keeps nearby IP addresses close to each other in the picture.[5] The locality property of the Hilbert curve has also been used to design algorithms for exploring regions with mobile robots[6][7] and indexing geospatial location data.[8]\\n\\nIn an algorithm called Riemersma dithering, grayscale photographs can be converted to a dithered black-and-white image using thresholding, with the leftover amount from each pixel added to the next pixel along the Hilbert curve. Code to do this would map from 1D to 2D, and the Hilbert curve is sometimes used because it does not create the distracting patterns that would be visible to the eye if the order were simply left to right across each row of pixels.[9] Hilbert curves in higher dimensions are an instance of a generalization of Gray codes, and are sometimes used for similar purposes, for similar reasons. For multidimensional databases, Hilbert order has been proposed to be used instead of Z order because it has better locality-preserving behavior. For example, Hilbert curves have been used to compress and accelerate R-tree indexes[10] (see Hilbert R-tree). They have also been used to help compress data warehouses.[11][12]\\n\\nThe linear distance of any point along the curve can be converted to coordinates in n dimensions for a given n, and vice versa, using any of several standard mathematical techniques such as Skilling\\'s method.[13][14]\\n\\nIt is possible to implement Hilbert curves efficiently even when the data space does not form a square.[15] Moreover, there are several possible generalizations of Hilbert curves to higher dimensions.[16][17]\\n\\nRepresentation as Lindenmayer system\\n\\nThe Hilbert Curve can be expressed by a rewrite system (L-system).\\n\\nDuration: 52 seconds.0:52\\nHilbert curve at its sixth iteration\\nAlphabet : A, B\\nConstants : F + −\\nAxiom : A\\nProduction rules:\\nA → +BF−AFA−FB+\\nB → −AF+BFB+FA−\\nHere, \"F\" means \"draw forward\", \"+\" means \"turn left 90°\", \"-\" means \"turn right 90°\" (see turtle graphics), and \"A\" and \"B\" are ignored during drawing.\\n\\nOther implementations\\n\\nGraphics Gems II[18][promotion?] discusses Hilbert curve coherency, and provides implementation.\\n\\nThe Hilbert Curve is commonly used among rendering images or videos. Common programs such as Blender and Cinema 4D use the Hilbert Curve to trace the objects, and render the scene.[citation needed]\\n\\nThe slicer software used to convert 3D models into toolpaths for a 3D printer typically has the Hilbert curve as an option for an infill pattern.\\n\"\"\",\n",
        "    \"\"\"Vícerozměrná náhodná proměnná nebo náhodný vektor je v teorii pravděpodobnosti a statistice seznam matematických proměnných, jehož žádná hodnota není známa, buď protože zatím nebyla pozorována, nebo protože její hodnotu neznáme přesně. Jednotlivé proměnné jsou sdružené v náhodném vektoru, protože tvoří části jednoho matematického systému – často reprezentují různé vlastnosti určité statistické jednotky. Pokud například chceme zachytit, že každá osoba má určitý věk, výšku a hmotnost, lze tyto vlastnosti blíže neurčené osoby z určité skupiny reprezentovat náhodným vektorem. Prvky náhodných vektorů jsou obvykle reálná čísla.\"\"\",]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SCALAR #######################################################################\n",
        "\n",
        "__preprocess = PIPELINE_FACTORY(batch_dim=1, token_dim=BASE_CONFIG['token_dim'], sample_dim=1024, features=['data'], encoding='UTF-8')"
      ],
      "metadata": {
        "id": "fUOAfI40qJCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEGS3dLaSW4x"
      },
      "outputs": [],
      "source": [
        "# COMPUTE ######################################################################\n",
        "\n",
        "__i = -1\n",
        "__s = {'data': tf.cast([SAMPLES[__i]], dtype=tf.string),}\n",
        "__x, __t = __preprocess(__s)\n",
        "__y = MODEL(__x, logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "__o_t = mlable.text.unpack(__postprocess_probs(__t))\n",
        "__o_p = mlable.text.unpack(__postprocess_sampler(__y))"
      ],
      "metadata": {
        "id": "Bc8nAu0Zq3XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKdaYXR5SyY9"
      },
      "outputs": [],
      "source": [
        "print('# INPUT #################################################################\\n\\n' + SAMPLES[__i])\n",
        "print('\\n# OUTPUT ################################################################\\n\\n' + __o_t[0])\n",
        "print('\\n# SCORE #################################################################\\n\\n' + str(tokun.eval.compare(__o_t[0], __o_p[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfopolmD9fNx"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eJmv4xjnTH4t"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir .logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeTbE8iK4-37"
      },
      "outputs": [],
      "source": [
        "MODEL.save('tokun.keras', overwrite=True, zipped=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}