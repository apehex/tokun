{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xXM7DoPpds1"
      },
      "source": [
        "## Import deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W24EKFXaO5yC"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets mlable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U --no-index -f '/content/libs/' tokun"
      ],
      "metadata": {
        "id": "gKU9BSWyhcX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXU-Ebl2pddk"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import functools\n",
        "import itertools\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import urllib.request\n",
        "\n",
        "import datasets as hd\n",
        "import tensorflow as tf\n",
        "\n",
        "import mlable.data\n",
        "import mlable.metrics\n",
        "import mlable.ops\n",
        "\n",
        "import tokun.data\n",
        "import tokun.models.meta\n",
        "import tokun.models.mlp\n",
        "import tokun.pipeline.evaluate\n",
        "import tokun.pipeline.preprocess\n",
        "import tokun.pipeline.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pn1ywhSrpin9"
      },
      "outputs": [],
      "source": [
        "print(\"Tensorflow version \" + tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Setup the GPU / TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWMvVNSAJK_n"
      },
      "outputs": [],
      "source": [
        "# MIXED PRECISION ##############################################################\n",
        "\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_bfloat16')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFIMfPmgQa0h"
      },
      "outputs": [],
      "source": [
        "# DEVICES ######################################################################\n",
        "\n",
        "tf.debugging.set_log_device_placement(False)\n",
        "\n",
        "CPU = tf.config.list_logical_devices('CPU')\n",
        "GPU = tf.config.list_logical_devices('GPU')\n",
        "TPU = tf.config.list_logical_devices('TPU')\n",
        "\n",
        "if TPU:\n",
        "    RESOLVER = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(RESOLVER)\n",
        "    tf.tpu.experimental.initialize_tpu_system(RESOLVER)\n",
        "    DISTRIBUTION_STRATEGY = tf.distribute.TPUStrategy(RESOLVER)\n",
        "elif GPU:\n",
        "    DISTRIBUTION_STRATEGY = tf.distribute.MirroredStrategy(GPU)\n",
        "else:\n",
        "    DISTRIBUTION_STRATEGY = tf.distribute.MirroredStrategy(CPU)\n",
        "\n",
        "print(DISTRIBUTION_STRATEGY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9066X5EOyAX"
      },
      "source": [
        "## Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFSPMtQaO1fu"
      },
      "outputs": [],
      "source": [
        "# TOGGLE #######################################################################\n",
        "\n",
        "IMPORT = True\n",
        "DOWNLOAD = False\n",
        "TRAINING = True\n",
        "RANDOM = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t1jfsJlM3SX"
      },
      "source": [
        "## Defining The Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z74MlibMWnu"
      },
      "outputs": [],
      "source": [
        "# MODEL PARAMETERS #############################################################\n",
        "\n",
        "TOKUN_CONFIG = {\n",
        "    'sequence_axis': 1,\n",
        "    'feature_axis': -1,\n",
        "    'token_dim': [4, 2, 2, 2,], # G, for each block\n",
        "    'latent_dim': [16, 32, 64, 128], # L, for each block\n",
        "    'input_dim': 256, # U_i (bytes)\n",
        "    'embed_dim': 8, # E\n",
        "    'output_dim': 8, # U_o (8 bits)\n",
        "    'activation': 'gelu',}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DERIVED MODEL PARAMETERS #####################################################\n",
        "\n",
        "VERSION_CONFIG = {\n",
        "    'token_dim': TOKUN_CONFIG['token_dim'],\n",
        "    'input_dim': TOKUN_CONFIG['input_dim'],\n",
        "    'embed_dim': TOKUN_CONFIG['latent_dim'],\n",
        "    'output_dim': TOKUN_CONFIG['output_dim'],\n",
        "    'sequence_axis': TOKUN_CONFIG['sequence_axis']}\n",
        "\n",
        "META_CONFIG = {\n",
        "    'version': tokun.models.meta.version(**VERSION_CONFIG),\n",
        "    'label': '6.1',}\n",
        "\n",
        "IO_CONFIG = {\n",
        "    'url': 'https://github.com/apehex/tokun/raw/main/models/{}/{}/{}/{}.keras'.format(*META_CONFIG['version'], META_CONFIG['label']),\n",
        "    'path': 'tokun.keras',}"
      ],
      "metadata": {
        "id": "HMA0OZAGdq-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21J7WEkhSwph"
      },
      "outputs": [],
      "source": [
        "# TRAINING PARAMETERS ##########################################################\n",
        "\n",
        "TRAINING_CONFIG = {\n",
        "    'epochs': 8,\n",
        "    'batch_size': None,\n",
        "    'validation_split': None,\n",
        "    'validation_freq': list(range(1, 9)),\n",
        "    # 'class_weight': {__c: 1. if __c == 0 else 1. for __c in range(256)}, # there are 3 times more 0s than other bytes\n",
        "    'verbose': 1,}\n",
        "\n",
        "OPTIMIZER_CONFIG = {\n",
        "    'learning_rate': 0.001 * (0.1 if IMPORT else 1.0),\n",
        "    'weight_decay': 0.001,\n",
        "    'beta_1': 0.9,\n",
        "    'beta_2': 0.95,\n",
        "    'epsilon': 1e-6,\n",
        "    'clipnorm': 0.1,\n",
        "    'amsgrad': False,\n",
        "    'use_ema': False,\n",
        "    'ema_momentum': 0.99,\n",
        "    'ema_overwrite_frequency': 1024,}\n",
        "    # 'gradient_accumulation_steps': 2,\n",
        "\n",
        "SCHEDULER_CONFIG = {\n",
        "    'initial_learning_rate': OPTIMIZER_CONFIG['learning_rate'],\n",
        "    'decay_steps': TRAINING_CONFIG['epochs'] * 1024,\n",
        "    'alpha': 0.01,\n",
        "    'name': 'cosine_lr',\n",
        "    'warmup_target': None,\n",
        "    'warmup_steps': 0,}\n",
        "\n",
        "LOSS_CONFIG = {\n",
        "    'from_logits': True,\n",
        "    'label_smoothing': 0.0,\n",
        "    'axis': -1,\n",
        "    'reduction': 'sum_over_batch_size',\n",
        "    'name': 'ce_loss',}\n",
        "\n",
        "METRICS_CONFIG = {\n",
        "    # 'factor': 256,}\n",
        "    'depth': -1,}\n",
        "\n",
        "CHECKPOINT_CONFIG = {\n",
        "    'filepath': IO_CONFIG['path'],\n",
        "    'monitor': 'val_loss',\n",
        "    'mode': 'auto',\n",
        "    'save_freq': 'epoch',\n",
        "    'save_best_only': False,\n",
        "    'save_weights_only': False,\n",
        "    'verbose': 1,}\n",
        "\n",
        "TENSORBOARD_CONFIG = {\n",
        "    'log_dir': os.path.join('.logs/', *META_CONFIG['version'], datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")),\n",
        "    'histogram_freq': 1,\n",
        "    'embeddings_freq': 1,\n",
        "    'profile_batch': (0, 4),\n",
        "    'write_graph': True,\n",
        "    'write_images': True,}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPROCESSING ################################################################\n",
        "\n",
        "BATCH_CONFIG = {\n",
        "    'batch_size': 256,\n",
        "    'drop_remainder': True,\n",
        "    'num_parallel_calls': tf.data.AUTOTUNE,}\n",
        "\n",
        "PIPELINE_CONFIG = {\n",
        "    'batch_dim': BATCH_CONFIG['batch_size'],\n",
        "    'sample_dim': 4 * 512,\n",
        "    'token_dim': math.prod(TOKUN_CONFIG['token_dim']),\n",
        "    'separator': '\\u001d',}"
      ],
      "metadata": {
        "id": "p4-naWbUdV4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DATASETS #####################################################################\n",
        "\n",
        "DATASETS_CONFIG = {\n",
        "    # 'pt-fineweb-edu': {\n",
        "    #     'path': 'HuggingFaceFW/fineweb-edu',\n",
        "    #     'name': 'sample-10BT',\n",
        "    #     'splits': [f'train[{__p}%:{__p + 10}%]' for __p in range(0, 100, 10)],\n",
        "    #     'features': ['text'],},\n",
        "    # 'pt-fineweb-kor': {\n",
        "    #     'path': 'HuggingFaceFW/fineweb-2',\n",
        "    #     'name': 'kor_Hang',\n",
        "    #     'splits': [f'train[{__p}%:{__p + 10}%]' for __p in range(0, 100, 10)],\n",
        "    #     'features': ['text'],},\n",
        "    # 'pt-fineweb-fin': {\n",
        "    #     'path': 'HuggingFaceFW/fineweb-2',\n",
        "    #     'name': 'fin_Latn',\n",
        "    #     'splits': [f'train[{__p}%:{__p + 10}%]' for __p in range(0, 100, 10)],\n",
        "    #     'features': ['text'],},\n",
        "    'pt-wikipedia': {\n",
        "        'path': 'wikimedia/wikipedia',\n",
        "        'name': '20231101.en',\n",
        "        'splits': [f'train[{__p}%:{__p + 9}%]' for __p in range(0, 80, 8)],\n",
        "        'features': ['text'],},\n",
        "    # 'tp-wikipedia-1': {\n",
        "    #     'path': 'wikimedia/wikipedia',\n",
        "    #     'name': '20231101.en',\n",
        "    #     'splits': [f'train[{__p}%:{__p + 1}%]' for __p in range(80, 90, 1)],\n",
        "    #     'features': ['text'],},\n",
        "    # 'tp-wikipedia-2': {\n",
        "    #     'path': 'wikimedia/wikipedia',\n",
        "    #     'name': '20231101.en',\n",
        "    #     'splits': [f'train[{__p}%:{__p + 1}%]' for __p in range(90, 100, 1)],\n",
        "    #     'features': ['text'],},\n",
        "    # 'ft-retro-ascii-art': {\n",
        "    #     'path': 'jdpressman/retro-ascii-art-v1',\n",
        "    #     'name': None,\n",
        "    #     'train': 'train',\n",
        "    #     'splits': [f'train[{__p}%:{__p + 10}%]+validation[{__p}%:{__p + 10}%]' for __p in range(0, 100, 10)],\n",
        "    #     'features': ['prompt', 'art_aic'],},\n",
        "    # 'ft-stack-exchange': {\n",
        "    #     'path': 'Alignment-Lab-AI/Stack-Exchange-April',\n",
        "    #     'name': None,\n",
        "    #     'splits': [f'train[{__p}%:{__p + 10}%]' for __p in range(0, 100, 10)],\n",
        "    #     'features': ['question', 'answer'],},\n",
        "    # 'ft-math': {\n",
        "    #     'path': 'HuggingFaceTB/finemath',\n",
        "    #     'name': 'finemath-3plus',\n",
        "    #     'splits': [f'train[{__p}%:{__p + 10}%]' for __p in range(0, 100, 10)],\n",
        "    #     'features': ['text'],},\n",
        "    # 'cot-text-dolphin': {\n",
        "    #     'path': 'cognitivecomputations/dolphin-r1',\n",
        "    #     'name': 'reasoning-deepseek',\n",
        "    #     'splits': [f'train[{__p}%:{__p + 10}%]' for __p in range(0, 100, 10)],\n",
        "    #     'features': ['reasoning', 'answer'],},\n",
        "    # 'cot-text-openthoughts': {\n",
        "    #     'path': 'open-thoughts/OpenThoughts-114k',\n",
        "    #     'name': 'default',\n",
        "    #     'splits': [f'train[{__p}%:{__p + 10}%]' for __p in range(0, 100, 10)],\n",
        "    #     'features': ['problem', 'solution'],},\n",
        "    # 'cot-math-numi': {\n",
        "    #     'path': 'AI-MO/NuminaMath-CoT',\n",
        "    #     'name': None,\n",
        "    #     'splits': [f'train[{__p}%:{__p + 10}%]' for __p in range(0, 100, 10)],\n",
        "    #     'features': ['problem', 'solution'],},\n",
        "}"
      ],
      "metadata": {
        "id": "sdzyDZfKnMw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading The Model Weights"
      ],
      "metadata": {
        "id": "dNF00bM5xj9O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfIZb86Fg0dQ"
      },
      "outputs": [],
      "source": [
        "# IMPORT #######################################################################\n",
        "\n",
        "if IMPORT and DOWNLOAD:\n",
        "    urllib.request.urlretrieve(IO_CONFIG['url'], IO_CONFIG['path'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEyFtkcFNGe4"
      },
      "source": [
        "## Downloading The Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTK1MPV8qek5"
      },
      "outputs": [],
      "source": [
        "# DOWNLOAD #####################################################################\n",
        "\n",
        "DATASETS = {\n",
        "    __name: [\n",
        "        hd.load_dataset(path=__args['path'], name=__args['name'], split=__s).to_tf_dataset(shuffle=True, batch_size=None)\n",
        "        for __s in __args['splits']]\n",
        "    for __name, __args in DATASETS_CONFIG.items()}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STATS #######################################################################\n",
        "\n",
        "STATS = {__n: mlable.data.stats(dataset=DATASETS[__n][0], features=DATASETS_CONFIG[__n]['features'], count=2048) for __n in DATASETS}\n",
        "\n",
        "print(STATS)"
      ],
      "metadata": {
        "id": "hlo20AczsU1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cheN52OEchs"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gp2WitYVhs8I"
      },
      "outputs": [],
      "source": [
        "# ITERATE #####################################################################\n",
        "\n",
        "for __name in DATASETS:\n",
        "    # specialized preprocessing fn\n",
        "    __preprocess = tokun.pipeline.preprocess.factory(\n",
        "        features=DATASETS_CONFIG[__name]['features'],\n",
        "        **PIPELINE_CONFIG)\n",
        "    # apply\n",
        "    for __idx in range(len(DATASETS[__name])):\n",
        "        DATASETS[__name][__idx] = DATASETS[__name][__idx].batch(**BATCH_CONFIG).map(__preprocess, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnzCjLkrlI8d"
      },
      "outputs": [],
      "source": [
        "# CONCATENATE #################################################################\n",
        "\n",
        "DATASET_KEYS = set(DATASETS.keys()) - {'ft-retro-ascii-art'}\n",
        "\n",
        "# FINE_TRAIN = functools.reduce(lambda __l, __r: __l.concatenate(__r), DATASETS['pt-fineweb-edu'][:-1])\n",
        "# FINE_TEST = DATASETS['pt-fineweb-edu'][-1]\n",
        "\n",
        "DATASET_TRAIN = functools.reduce(lambda __l, __r: __l.concatenate(__r), [DATASETS[__n][__i] for __n in DATASET_KEYS for __i in range(len(DATASETS[__n]) - 1)])\n",
        "DATASET_TEST = functools.reduce(lambda __l, __r: __l.concatenate(__r), [DATASETS[__n][-1] for __n in DATASET_KEYS])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ3quJQ4EUKf"
      },
      "outputs": [],
      "source": [
        "# INSPECT #####################################################################\n",
        "\n",
        "__X, __T = next(iter(DATASET_TRAIN.take(1)))\n",
        "\n",
        "# print(FINE_TRAIN.element_spec)\n",
        "# print(FINE_TEST.element_spec)\n",
        "\n",
        "print(DATASET_TRAIN.element_spec)\n",
        "print(DATASET_TEST.element_spec)\n",
        "\n",
        "print('train: {:,}'.format(DATASET_TRAIN.cardinality().numpy()))\n",
        "print('test:  {:,}'.format(DATASET_TEST.cardinality().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbkaXey44V5Q"
      },
      "source": [
        "## Init The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEpY1-vFIFX7"
      },
      "outputs": [],
      "source": [
        "# COMPILE ######################################################################\n",
        "\n",
        "with DISTRIBUTION_STRATEGY.scope():\n",
        "    # metrics\n",
        "    byte_accuracy = mlable.metrics.BinaryGroupAccuracy(group=1, name='byte_accuracy')\n",
        "    character_accuracy = mlable.metrics.BinaryGroupAccuracy(group=4, name='character_accuracy')\n",
        "    token_accuracy = mlable.metrics.BinaryGroupAccuracy(group=PIPELINE_CONFIG['token_dim'], name='token_accuracy')\n",
        "    # weights\n",
        "    MODEL = tokun.models.mlp.AutoEncoder(**TOKUN_CONFIG)\n",
        "    if IMPORT and os.path.isfile(IO_CONFIG['path']): MODEL = tf.keras.models.load_model(IO_CONFIG['path'], compile=False)\n",
        "    # compile\n",
        "    MODEL.compile(\n",
        "        optimizer=tf.keras.optimizers.AdamW(**OPTIMIZER_CONFIG),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(**LOSS_CONFIG),\n",
        "        weighted_metrics=[byte_accuracy, character_accuracy, token_accuracy])\n",
        "    # build\n",
        "    MODEL(__X, training=False)\n",
        "    MODEL.compute_metrics(__X, __T, __T)\n",
        "    MODEL.compute_loss(__X, __T, __T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuRwWdjpPQBM"
      },
      "outputs": [],
      "source": [
        "# INSPECT ######################################################################\n",
        "\n",
        "MODEL.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(MODEL.compute_loss(__X, __T, MODEL(__X)))\n",
        "tf.reduce_mean(tf.keras.losses.binary_crossentropy(__T, MODEL(__X), from_logits=True, axis=-1))"
      ],
      "metadata": {
        "id": "zN5DeRPbrIT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRkNkXthBwar"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beTpALmzFdu1"
      },
      "outputs": [],
      "source": [
        "# TRAIN ########################################################################\n",
        "\n",
        "if TRAINING:\n",
        "    with DISTRIBUTION_STRATEGY.scope():\n",
        "        # callbacks\n",
        "        cp_callback = tf.keras.callbacks.ModelCheckpoint(**CHECKPOINT_CONFIG)\n",
        "        tb_callback = tf.keras.callbacks.TensorBoard(**TENSORBOARD_CONFIG)\n",
        "        tn_callback = tf.keras.callbacks.TerminateOnNaN()\n",
        "        # fit model\n",
        "        TRAINING_HISTORY = MODEL.fit(\n",
        "            x=DATASET_TRAIN.take(2048).prefetch(tf.data.AUTOTUNE),\n",
        "            validation_data=DATASET_TEST.take(128).prefetch(tf.data.AUTOTUNE),\n",
        "            callbacks=[cp_callback, tb_callback, tn_callback],\n",
        "            **TRAINING_CONFIG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHtROW1K1R7c"
      },
      "source": [
        "## Dataviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiRVgG-oSfb7"
      },
      "outputs": [],
      "source": [
        "# DATA ########################################################################\n",
        "\n",
        "SAMPLES = [\n",
        "    \"\"\"위키백과, 우리 모두의 백과사전.\\nt-분포 확률적 임베딩(t-SNE)은 데이터의 차원 축소에 사용되는 기계 학습 알고리즘 중 하나로, 2002년 샘 로이스Sam Rowise와 제프리 힌튼에 의해 개발되었다.[1] t-SNE는 비선형 차원 축소 기법으로, 고차원 데이터를 특히 2, 3차원 등으로 줄여 가시화하는데에 유용하게 사용된다. 구체적으로 t-SNE는 비슷한 데이터는 근접한 2, 3차원의 지점으로, 다른 데이터는 멀리 떨어진 지점으로 맵핑한다.\"\"\",\n",
        "    \"\"\"class Encoder(tf.keras.models.Model):\\n    def __init__(self, depth: int, token_dim: int, encoding_dim: int, embedding_dim: int, batch_dim: int=None, attention: bool=False, **kwargs) -> None:\\n        super(Encoder, self).__init__(**kwargs)\\n        self._encoder = tf.keras.Sequential([\\n            tf.keras.Input(shape=(encoding_dim,), batch_size=batch_dim, name='input'), # (B * G ^ D, U)\\n            tf.keras.layers.Dense(units=embedding_dim, activation=None, use_bias=False, kernel_initializer='glorot_uniform', bias_initializer=None, name='embed-1'),] # (B * G ^ D, U) => (B * G ^ D, E)\\n            + [tokun.layers.TokenizeBlock(left_axis=-2, right_axis=-1, token_dim=token_dim, attention=attention, name='tokenize' + (__i + 1) * '-4') for __i in range(depth)]) # (B * G ^ i, E) => (B * G ^ (i-1), E)\\n\\n    def call(self, x: tf.Tensor) -> tf.Tensor:\\n        return self._encoder(x)\\n\"\"\",\n",
        "    \"\"\"class AutoEncoder(tf.keras.models.Model):\\n    def __init__(self, token_dim: int, encoding_dim: int, embedding_dim: int, batch_dim: int=None, **kwargs) -> None:\\n        super(AutoEncoder, self).__init__(**kwargs)\\n        self._encoder = Encoder(token_dim=token_dim, encoding_dim=encoding_dim, embedding_dim=embedding_dim, batch_dim=batch_dim)\\n        self._decoder = Decoder(token_dim=token_dim, encoding_dim=encoding_dim, embedding_dim=embedding_dim, batch_dim=batch_dim)\\n\\n    def call(self, x: tf.Tensor) -> tf.Tensor:\\n        return self._decoder(self._encoder(x))\"\"\",\n",
        "    \"\"\"class AutoEncoder(tf.keras.models.Model):\\n  def __init__(self, token_dim: int, encoding_dim: int, embedding_dim: int, batch_dim: int=None, **kwargs) -> None:\\n    super(AutoEncoder, self).__init__(**kwargs)\\n    self._encoder = Encoder(token_dim=token_dim, encoding_dim=encoding_dim, embedding_dim=embedding_dim, batch_dim=batch_dim)\\n    self._decoder = Decoder(token_dim=token_dim, encoding_dim=encoding_dim, embedding_dim=embedding_dim, batch_dim=batch_dim)\\n\\n  def call(self, x: tf.Tensor) -> tf.Tensor:\\n    return self._decoder(self._encoder(x))\"\"\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEGS3dLaSW4x"
      },
      "outputs": [],
      "source": [
        "# COMPUTE ######################################################################\n",
        "\n",
        "__i = 1\n",
        "__x, __e, __p, __y, __o = tokun.pipeline.text.sample(model=MODEL, text=SAMPLES[__i], token_dim=PIPELINE_CONFIG['token_dim'], threshold=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKdaYXR5SyY9"
      },
      "outputs": [],
      "source": [
        "print('# INPUT ################################################################\\n\\n' + SAMPLES[__i])\n",
        "print('\\n# OUTPUT ###############################################################\\n\\n' + __o[0])\n",
        "print('\\n# SCORE ################################################################\\n\\n' + str(tokun.pipeline.evaluate.compare(SAMPLES[__i], __o[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfopolmD9fNx"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eJmv4xjnTH4t"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir .logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeTbE8iK4-37"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}