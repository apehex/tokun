{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xXM7DoPpds1"
      },
      "source": [
        "## Import deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W24EKFXaO5yC"
      },
      "outputs": [],
      "source": [
        "!pip install mlable tokun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXU-Ebl2pddk"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import functools\n",
        "import itertools\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import urllib.request\n",
        "\n",
        "import huggingface_hub\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import mlable.data\n",
        "import mlable.layers.reshaping\n",
        "import mlable.layers.transformer\n",
        "import mlable.optimizers\n",
        "\n",
        "import tokun.data\n",
        "import tokun.evaluation\n",
        "import tokun.meta\n",
        "import tokun.pipeline\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pn1ywhSrpin9"
      },
      "outputs": [],
      "source": [
        "print(\"Tensorflow version \" + tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Setup the GPU / TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFIMfPmgQa0h"
      },
      "outputs": [],
      "source": [
        "tf.debugging.set_log_device_placement(False)\n",
        "\n",
        "CPU = tf.config.list_logical_devices('CPU')\n",
        "GPU = tf.config.list_logical_devices('GPU')\n",
        "TPU = tf.config.list_logical_devices('TPU')\n",
        "\n",
        "if TPU:\n",
        "    RESOLVER = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(RESOLVER)\n",
        "    tf.tpu.experimental.initialize_tpu_system(RESOLVER)\n",
        "    DISTRIBUTION_STRATEGY = tf.distribute.TPUStrategy(RESOLVER)\n",
        "elif GPU:\n",
        "    DISTRIBUTION_STRATEGY = tf.distribute.MirroredStrategy(GPU)\n",
        "else:\n",
        "    DISTRIBUTION_STRATEGY = tf.distribute.MirroredStrategy(CPU)\n",
        "\n",
        "print(DISTRIBUTION_STRATEGY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9066X5EOyAX"
      },
      "source": [
        "## Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFSPMtQaO1fu"
      },
      "outputs": [],
      "source": [
        "# TOGGLE ######################################################################\n",
        "\n",
        "IMPORT = False\n",
        "TRAINING = True\n",
        "RANDOM = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t1jfsJlM3SX"
      },
      "source": [
        "## Defining The Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z74MlibMWnu"
      },
      "outputs": [],
      "source": [
        "# PARAMETERS ##################################################################\n",
        "\n",
        "ACTIVATION = 'silu'\n",
        "GATE = False\n",
        "NORMALIZATION = True\n",
        "\n",
        "SEQUENCE_AXIS = 1\n",
        "FEATURE_AXIS = -1\n",
        "\n",
        "N_TOKEN_DIM = [4, 4] # G, for each block\n",
        "N_ENCODING_DIM = 256 # U\n",
        "N_EMBEDDING_DIM = N_ENCODING_DIM # E\n",
        "N_HIDDEN_DIM = 4 * N_EMBEDDING_DIM # H\n",
        "N_LATENT_DIM = N_EMBEDDING_DIM # L\n",
        "\n",
        "N_EPOCHS = 8\n",
        "N_EPOCHS_RAMPUP = 0\n",
        "N_EPOCHS_SUSTAIN = 0\n",
        "\n",
        "N_BATCH = 128 # number of samples per batch\n",
        "N_SAMPLE = 256 # number of characters per sample (=> N_TOKEN_DIM * N_SAMPLE integers per sample)\n",
        "\n",
        "R_MIN, R_MAX, R_EXP = tokun.meta.rates(pretrained=IMPORT, normalization=NORMALIZATION, base=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jm6y63XRBz07"
      },
      "outputs": [],
      "source": [
        "# DERIVED #####################################################################\n",
        "\n",
        "TOKEN_SIZES = list(itertools.accumulate(N_TOKEN_DIM, lambda x, y: x * y)) # in bytes\n",
        "OFFSET_TICKS = [2 ** __i for __i in range(int(math.log(TOKEN_SIZES[-1] // 4, 2)))] # in characters\n",
        "\n",
        "VERSION = tokun.meta.version(groups=N_TOKEN_DIM, activation=ACTIVATION, gate=GATE, normalization=NORMALIZATION)\n",
        "DATETIME = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfIZb86Fg0dQ"
      },
      "outputs": [],
      "source": [
        "# IMPORT ######################################################################\n",
        "\n",
        "LABEL = '3.1'\n",
        "URL_IMPORT = 'https://github.com/apehex/tokun/raw/main/models/{}/{}/{}/{}/{}.keras'.format(*VERSION, LABEL)\n",
        "PATH_IMPORT = 'model.keras'\n",
        "\n",
        "if IMPORT:\n",
        "    urllib.request.urlretrieve(URL_IMPORT, PATH_IMPORT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m88JtcyGiSZ1"
      },
      "outputs": [],
      "source": [
        "# EXPORT ######################################################################\n",
        "\n",
        "PATH_LOG = os.path.join('.logs/', *VERSION, DATETIME)\n",
        "PATH_EXPORT = 'model.keras'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEyFtkcFNGe4"
      },
      "source": [
        "## Loading The Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4a3SQUQHNJ6M"
      },
      "outputs": [],
      "source": [
        "# MLQA DATASET ################################################################\n",
        "\n",
        "LANG = ['ar', 'de', 'en', 'es', 'hi', 'vi', 'zh']\n",
        "MLQA_TRAIN = {__l: tfds.load('mlqa/' + __l, split='test', as_supervised=False, shuffle_files=True, data_dir='~/.cache/tensorflow/', batch_size=None) for __l in LANG}\n",
        "MLQA_TEST = {__l: tfds.load('mlqa/' + __l, split='validation', as_supervised=False, shuffle_files=True, data_dir='~/.cache/tensorflow/', batch_size=None) for __l in LANG}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTK1MPV8qek5"
      },
      "outputs": [],
      "source": [
        "# RANDOM DATASET ##############################################################\n",
        "\n",
        "RANDOM_TRAIN = tokun.data.random_dataset(size=N_BATCH * 2**10, sample_size=N_SAMPLE, lower_plane=0, upper_plane=0x40000)\n",
        "RANDOM_TEST = tokun.data.random_dataset(size=N_BATCH * 2**8, sample_size=N_SAMPLE, lower_plane=0, upper_plane=0x40000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgceZVOon0Mi"
      },
      "source": [
        "## Blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITuRAod-n6Lm"
      },
      "outputs": [],
      "source": [
        "# ENCODING BLOCKS #############################################################\n",
        "\n",
        "@tf.keras.saving.register_keras_serializable(package='blocks')\n",
        "class TokenizeBlock(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        sequence_axis: int=0,\n",
        "        feature_axis: int=-1,\n",
        "        token_dim: int=4,\n",
        "        embedding_dim: int=256,\n",
        "        hidden_dim: int=1024,\n",
        "        latent_dim: int=256,\n",
        "        normalization: bool=False,\n",
        "        gate: bool=False,\n",
        "        activation: str='silu',\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        super(TokenizeBlock, self).__init__(**kwargs)\n",
        "        # this axis is inserted and then merged\n",
        "        __temp_axis = sequence_axis + 1\n",
        "        # config\n",
        "        self._config = {\n",
        "            'sequence_axis': sequence_axis,\n",
        "            'feature_axis': feature_axis,\n",
        "            'token_dim': token_dim,\n",
        "            'embedding_dim': embedding_dim,\n",
        "            'hidden_dim': hidden_dim,\n",
        "            'latent_dim': latent_dim,\n",
        "            'normalization': normalization,\n",
        "            'gate': gate,\n",
        "            'activation': activation,}\n",
        "        # layers\n",
        "        self._normalization = tf.keras.layers.LayerNormalization(axis=feature_axis, epsilon=0.001, center=True, scale=True, name='normalization') if normalization else None # normalize each token unit independently\n",
        "        self._divide = mlable.layers.reshaping.Divide(input_axis=sequence_axis, output_axis=__temp_axis, factor=token_dim, insert=True, name='group') # (B * G, E) => (B, G, E)\n",
        "        self._gate = mlable.layers.transformer.FeedForwardGate(input_dim=embedding_dim, hidden_dim=hidden_dim, name='gate') if gate else None # (B, G, E) => (B, G, H) => (B, G, E)\n",
        "        self._merge = mlable.layers.reshaping.Merge(left_axis=__temp_axis, right_axis=feature_axis, left=False, name='merging') # (B, G, E) => (B, G * E)\n",
        "        self._dense = tf.keras.layers.Dense(units=latent_dim, activation=activation, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', name='compression') # (B, G * E) => (B, L), typically L = E\n",
        "\n",
        "    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n",
        "        __t = self._normalization(inputs) if self._normalization else inputs\n",
        "        __t = self._divide(__t)\n",
        "        __t = self._gate(__t) if self._gate else __t\n",
        "        return self._dense(self._merge(__t))\n",
        "\n",
        "    def get_config(self) -> dict:\n",
        "        __config = super(TokenizeBlock, self).get_config()\n",
        "        __config.update(self._config)\n",
        "        return __config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config) -> tf.keras.layers.Layer:\n",
        "        return cls(**config)\n",
        "\n",
        "# DECODING BLOCKS #############################################################\n",
        "\n",
        "@tf.keras.saving.register_keras_serializable(package='blocks')\n",
        "class DetokenizeBlock(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        sequence_axis: int=0,\n",
        "        feature_axis: int=-1,\n",
        "        token_dim: int=4,\n",
        "        embedding_dim: int=256,\n",
        "        hidden_dim: int=1024,\n",
        "        normalization: bool=False,\n",
        "        gate: bool=False,\n",
        "        activation: str='silu',\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        super(DetokenizeBlock, self).__init__(**kwargs)\n",
        "        # this axis is inserted and then merged\n",
        "        __temp_axis = sequence_axis + 1\n",
        "        # config\n",
        "        self._config = {\n",
        "            'sequence_axis': sequence_axis,\n",
        "            'feature_axis': feature_axis,\n",
        "            'token_dim': token_dim,\n",
        "            'embedding_dim': embedding_dim,\n",
        "            'hidden_dim': hidden_dim,\n",
        "            'normalization': normalization,\n",
        "            'gate': gate,\n",
        "            'activation': activation,}\n",
        "        # layers\n",
        "        self._dense = tf.keras.layers.Dense(units=token_dim * embedding_dim, activation=activation, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', name='decompression') # (B, L) => (B, G * E), typically L = E\n",
        "        self._divide = mlable.layers.reshaping.Divide(input_axis=feature_axis, output_axis=__temp_axis, insert=True, factor=token_dim, name='split') # (B, G * E) => (B, G, E)\n",
        "        self._gate = mlable.layers.transformer.FeedForwardGate(input_dim=embedding_dim, hidden_dim=hidden_dim, name='gate') if gate else None # (B, G, E) => (B, G, H) => (B, G, E)\n",
        "        self._merge = mlable.layers.reshaping.Merge(left_axis=sequence_axis, right_axis=__temp_axis, left=True) # (B, G, E) => (B * G, E)\n",
        "        self._normalization = tf.keras.layers.LayerNormalization(axis=feature_axis, epsilon=0.001, center=True, scale=True, name='normalization') if normalization else None # normalize each token unit independently\n",
        "\n",
        "    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n",
        "        __t = self._divide(self._dense(inputs))\n",
        "        __t = self._gate(__t) if self._gate else __t\n",
        "        __t = self._merge(__t)\n",
        "        return self._normalization(__t) if self._normalization else __t\n",
        "\n",
        "    def get_config(self) -> dict:\n",
        "        __config = super(DetokenizeBlock, self).get_config()\n",
        "        __config.update(self._config)\n",
        "        return __config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config) -> tf.keras.layers.Layer:\n",
        "        return cls(**config)\n",
        "\n",
        "# HEAD ########################################################################\n",
        "\n",
        "@tf.keras.saving.register_keras_serializable(package='blocks')\n",
        "class HeadBlock(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        feature_axis: int=-1,\n",
        "        encoding_dim: int=256,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        super(HeadBlock, self).__init__(**kwargs)\n",
        "        # config\n",
        "        self._config = {'feature_axis': feature_axis, 'encoding_dim': encoding_dim}\n",
        "        # layers\n",
        "        self._dense = tf.keras.layers.Dense(units=encoding_dim, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', name='projection') # (..., G, E) => (..., G, U), typically U = E\n",
        "        self._softmax = tf.keras.layers.Softmax(axis=feature_axis, name='softmax') # (..., G, U)\n",
        "\n",
        "    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n",
        "        return self._softmax(self._dense(inputs))\n",
        "\n",
        "    def get_config(self) -> dict:\n",
        "        __config = super(HeadBlock, self).get_config()\n",
        "        __config.update(self._config)\n",
        "        return __config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config) -> tf.keras.layers.Layer:\n",
        "        return cls(**config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S39n2JmXG6yv"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0upuSDyELmZ"
      },
      "outputs": [],
      "source": [
        "# ENCODER #####################################################################\n",
        "\n",
        "@tf.keras.saving.register_keras_serializable(package='models')\n",
        "class Encoder(tf.keras.models.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        token_dim: list,\n",
        "        encoding_dim: int,\n",
        "        embedding_dim: int,\n",
        "        hidden_dim: int,\n",
        "        latent_dim: int,\n",
        "        batch_dim: int=None,\n",
        "        normalization: bool=True,\n",
        "        gate: bool=True,\n",
        "        activation: str='silu',\n",
        "        sequence_axis: int=0,\n",
        "        feature_axis: int=-1,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        # init\n",
        "        super(Encoder, self).__init__(**kwargs)\n",
        "        # config\n",
        "        self._config = {\n",
        "            'token_dim': token_dim,\n",
        "            'encoding_dim': encoding_dim,\n",
        "            'embedding_dim': embedding_dim,\n",
        "            'hidden_dim': hidden_dim,\n",
        "            'latent_dim': latent_dim,\n",
        "            'batch_dim': batch_dim,\n",
        "            'normalization': normalization,\n",
        "            'gate': gate,\n",
        "            'activation': activation,\n",
        "            'sequence_axis': sequence_axis,\n",
        "            'feature_axis': feature_axis,}\n",
        "        # successive dimensions of the merging units\n",
        "        __token_dim = [token_dim] if isinstance(token_dim, int) else token_dim\n",
        "        # layers\n",
        "        __layers = [\n",
        "            # (B * G ^ D, U) => (B * G ^ D, E)\n",
        "            tf.keras.layers.Dense(\n",
        "                units=embedding_dim,\n",
        "                activation='linear',\n",
        "                use_bias=False,\n",
        "                kernel_initializer='glorot_uniform',\n",
        "                name='embed-1'),] + [\n",
        "            # (B * G ^ i, E) => (B * G ^ (i-1), E)\n",
        "            TokenizeBlock(\n",
        "                sequence_axis=sequence_axis,\n",
        "                feature_axis=feature_axis,\n",
        "                token_dim=__g,\n",
        "                embedding_dim=embedding_dim,\n",
        "                hidden_dim=hidden_dim,\n",
        "                latent_dim=latent_dim,\n",
        "                normalization=normalization,\n",
        "                gate=gate,\n",
        "                activation=activation,\n",
        "                name='tokenize-{}_{}'.format(__g, __i))\n",
        "            for __i, __g in enumerate(__token_dim)]\n",
        "        # model\n",
        "        self._encoder = tf.keras.Sequential(__layers)\n",
        "\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        return self._encoder(x)\n",
        "\n",
        "    def get_config(self) -> dict:\n",
        "        __config = super(Encoder, self).get_config()\n",
        "        __config.update(self._config)\n",
        "        return __config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config) -> tf.keras.layers.Layer:\n",
        "        return cls(**config)\n",
        "\n",
        "# DECODER #####################################################################\n",
        "\n",
        "@tf.keras.saving.register_keras_serializable(package='models')\n",
        "class Decoder(tf.keras.models.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        token_dim: list,\n",
        "        encoding_dim: int,\n",
        "        embedding_dim: int,\n",
        "        hidden_dim: int,\n",
        "        latent_dim: int,\n",
        "        batch_dim: int=None,\n",
        "        normalization: bool=True,\n",
        "        gate: bool=True,\n",
        "        activation: str='silu',\n",
        "        sequence_axis: int=0,\n",
        "        feature_axis: int=-1,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        # init\n",
        "        super(Decoder, self).__init__(**kwargs)\n",
        "        # config\n",
        "        self._config = {\n",
        "            'token_dim': token_dim,\n",
        "            'encoding_dim': encoding_dim,\n",
        "            'embedding_dim': embedding_dim,\n",
        "            'hidden_dim': hidden_dim,\n",
        "            'latent_dim': latent_dim,\n",
        "            'batch_dim': batch_dim,\n",
        "            'normalization': normalization,\n",
        "            'gate': gate,\n",
        "            'activation': activation,\n",
        "            'sequence_axis': sequence_axis,\n",
        "            'feature_axis': feature_axis,}\n",
        "        # successive dimensions of the dividing units\n",
        "        __token_dim = [token_dim] if isinstance(token_dim, int) else token_dim\n",
        "        # layers\n",
        "        __layers = [\n",
        "            # (B * G ^ i, E) => (B * G ^ (i+1), E)\n",
        "            DetokenizeBlock(\n",
        "                sequence_axis=sequence_axis,\n",
        "                feature_axis=feature_axis,\n",
        "                token_dim=__g,\n",
        "                embedding_dim=embedding_dim,\n",
        "                hidden_dim=hidden_dim,\n",
        "                normalization=normalization,\n",
        "                gate=gate,\n",
        "                activation=activation,\n",
        "                name='detokenize-{}_{}'.format(__g, __i))\n",
        "            for __i, __g in enumerate(__token_dim)] + [\n",
        "            # (B * G ^ D, E) => (B * G ^ D, U)\n",
        "            HeadBlock(feature_axis=feature_axis, encoding_dim=encoding_dim, name='project-head')]\n",
        "        # model\n",
        "        self._decoder = tf.keras.Sequential(__layers)\n",
        "\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        return self._decoder(x)\n",
        "\n",
        "    def get_config(self) -> dict:\n",
        "        __config = super(Decoder, self).get_config()\n",
        "        __config.update(self._config)\n",
        "        return __config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config) -> tf.keras.layers.Layer:\n",
        "        return cls(**config)\n",
        "\n",
        "# VAE #########################################################################\n",
        "\n",
        "@tf.keras.saving.register_keras_serializable(package='models')\n",
        "class AutoEncoder(tf.keras.models.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        token_dim: list,\n",
        "        encoding_dim: int,\n",
        "        embedding_dim: int,\n",
        "        hidden_dim: int,\n",
        "        latent_dim: int,\n",
        "        batch_dim: int=None,\n",
        "        normalization: bool=True,\n",
        "        gate: bool=True,\n",
        "        activation: str='silu',\n",
        "        sequence_axis: int=0,\n",
        "        feature_axis: int=-1,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        # init\n",
        "        super(AutoEncoder, self).__init__(**kwargs)\n",
        "        # layers\n",
        "        self._encoder = Encoder(token_dim=token_dim, encoding_dim=encoding_dim, embedding_dim=embedding_dim, hidden_dim=hidden_dim, latent_dim=latent_dim, batch_dim=batch_dim, gate=gate, normalization=normalization, activation=activation, sequence_axis=sequence_axis, feature_axis=feature_axis)\n",
        "        self._decoder = Decoder(token_dim=token_dim, encoding_dim=encoding_dim, embedding_dim=embedding_dim, hidden_dim=hidden_dim, latent_dim=latent_dim, batch_dim=batch_dim, gate=gate, normalization=normalization, activation=activation, sequence_axis=sequence_axis, feature_axis=feature_axis)\n",
        "\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        return self._decoder(self._encoder(x))\n",
        "\n",
        "    def get_config(self) -> dict:\n",
        "        __config = super(AutoEncoder, self).get_config()\n",
        "        __config.update(self._encoder.get_config())\n",
        "        return __config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config) -> tf.keras.layers.Layer:\n",
        "        return cls(**config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TokunModel(AutoEncoder, huggingface_hub.KerasModelHubMixin):\n",
        "    pass"
      ],
      "metadata": {
        "id": "Oo9tXQH83B7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEpY1-vFIFX7"
      },
      "outputs": [],
      "source": [
        "# INIT ########################################################################\n",
        "\n",
        "with DISTRIBUTION_STRATEGY.scope():\n",
        "    if IMPORT and os.path.isfile(PATH_IMPORT):\n",
        "        MODEL = tf.keras.models.load_model(PATH_IMPORT)\n",
        "    else:\n",
        "        MODEL = TokunModel(sequence_axis=SEQUENCE_AXIS, feature_axis=FEATURE_AXIS, token_dim=N_TOKEN_DIM, encoding_dim=N_ENCODING_DIM, embedding_dim=N_EMBEDDING_DIM, hidden_dim=N_HIDDEN_DIM, latent_dim=N_LATENT_DIM, batch_dim=None, gate=GATE, normalization=NORMALIZATION, activation=ACTIVATION)\n",
        "    MODEL.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=R_MAX),\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=0., axis=-1, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE, name='loss'),\n",
        "        metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cheN52OEchs"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gp2WitYVhs8I"
      },
      "outputs": [],
      "source": [
        "# MLQA ########################################################################\n",
        "\n",
        "PIPELINE = [\n",
        "    # offset by 1 to 15 character => (B, 1) bytes\n",
        "    *[(functools.partial(tokun.pipeline.offset, ticks=__t), False) for __t in OFFSET_TICKS], # (offsets 0, ..., (2 ^ i) - 1) + (offsets 2 ^ i, ..., 2 ^ (i+1) - 1)\n",
        "    # encode => (B, G * S,) int\n",
        "    (functools.partial(tokun.pipeline.encode, token_size=TOKEN_SIZES[-1], sample_size=N_SAMPLE), True),\n",
        "    # reshape => (B * G * S,) int\n",
        "    (functools.partial(tokun.pipeline.reshape, groups=N_TOKEN_DIM, expand=[], flatten=True), True),\n",
        "    # one-hot encoding => (B * G * S, E) int (bool)\n",
        "    (functools.partial(tf.one_hot, depth=N_ENCODING_DIM, axis=-1), True),\n",
        "    # replace sample inputs with (input, target) for supervised learning\n",
        "    ((lambda x: (x, x)), True)]\n",
        "\n",
        "OPERATIONS, REPLACE = zip(*PIPELINE)\n",
        "\n",
        "MLQA_TRAIN = {__l: mlable.data.process(dataset=__d, feature='context', pipeline=OPERATIONS, replace=REPLACE) for __l, __d in MLQA_TRAIN.items()}\n",
        "MLQA_TEST = {__l: mlable.data.process(dataset=__d, feature='context', pipeline=OPERATIONS, replace=REPLACE) for __l, __d in MLQA_TEST.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmPWQV6GzkFb"
      },
      "outputs": [],
      "source": [
        "# RANDOM ######################################################################\n",
        "\n",
        "PIPELINE = [\n",
        "    # reshape => (B * G * S,) int\n",
        "    (functools.partial(tokun.pipeline.reshape, groups=N_TOKEN_DIM, expand=[], flatten=True), True),\n",
        "    # one-hot encoding => (B * G * S, E) int (bool)\n",
        "    (functools.partial(tf.one_hot, depth=N_ENCODING_DIM, axis=-1), True),\n",
        "    # replace sample inputs with (input, target) for supervised learning\n",
        "    ((lambda x: (x, x)), True)]\n",
        "\n",
        "OPERATIONS, REPLACE = zip(*PIPELINE)\n",
        "\n",
        "RANDOM_TRAIN = mlable.data.process(dataset=RANDOM_TRAIN, feature='', pipeline=OPERATIONS, replace=REPLACE)\n",
        "RANDOM_TEST = mlable.data.process(dataset=RANDOM_TEST, feature='', pipeline=OPERATIONS, replace=REPLACE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnzCjLkrlI8d"
      },
      "outputs": [],
      "source": [
        "# TOGGLE ######################################################################\n",
        "\n",
        "DATASET_TRAIN = RANDOM_TRAIN if RANDOM else MLQA_TRAIN['ar'].concatenate(MLQA_TRAIN['en']).concatenate(MLQA_TRAIN['es']).concatenate(MLQA_TRAIN['de']).concatenate(MLQA_TRAIN['hi']).concatenate(MLQA_TRAIN['vi']).concatenate(MLQA_TRAIN['zh'])\n",
        "DATASET_TEST = MLQA_TEST['ar'].concatenate(MLQA_TEST['en']).concatenate(MLQA_TEST['es']).concatenate(MLQA_TEST['de']).concatenate(MLQA_TEST['hi']).concatenate(MLQA_TEST['vi']).concatenate(MLQA_TEST['zh'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRkNkXthBwar"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beTpALmzFdu1"
      },
      "outputs": [],
      "source": [
        "# TRAIN #######################################################################\n",
        "\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(functools.partial(mlable.optimizers.learning_rate_hokusai, lr_min=R_MIN, lr_max=R_MAX, lr_exp=R_EXP, rampup=N_EPOCHS_RAMPUP, sustain=N_EPOCHS_SUSTAIN), verbose=True)\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(PATH_EXPORT, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=PATH_LOG)\n",
        "\n",
        "if TRAINING:\n",
        "    with DISTRIBUTION_STRATEGY.scope():\n",
        "        TRAINING_HISTORY = MODEL.fit(\n",
        "            x=DATASET_TRAIN.batch(N_BATCH).prefetch(tf.data.AUTOTUNE),\n",
        "            batch_size=None,\n",
        "            epochs=N_EPOCHS,\n",
        "            validation_split=None,\n",
        "            validation_data=RANDOM_TEST.batch(N_BATCH).prefetch(tf.data.AUTOTUNE),\n",
        "            validation_freq=list(range(1, N_EPOCHS + 1, 1)),\n",
        "            verbose=2,\n",
        "            callbacks=[lr_callback, cp_callback, tb_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuRwWdjpPQBM"
      },
      "outputs": [],
      "source": [
        "MODEL.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMqMBhAidTZ9"
      },
      "source": [
        "## Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00maJbmdeeBi"
      },
      "outputs": [],
      "source": [
        "# SAMPLES #####################################################################\n",
        "\n",
        "IO = {}\n",
        "TOKENS = {__i: {} for __i in TOKEN_SIZES} # length in bytes\n",
        "EMBEDDINGS = {__i: {} for __i in TOKEN_SIZES} # same\n",
        "\n",
        "for __lang, __dataset in MLQA_TEST.items():\n",
        "    # compute predictions\n",
        "    __batch = iter(__dataset.batch(N_BATCH)) # iterate over batches of samples\n",
        "    __input = next(__batch)[0] # take input only\n",
        "    __output = MODEL(__input)\n",
        "    # sample predictions (inputs, outputs)\n",
        "    IO[__lang] = (__input, __output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEs9VCyFgKQs"
      },
      "outputs": [],
      "source": [
        "# RANDOM ######################################################################\n",
        "\n",
        "# predictions\n",
        "__batch = iter(RANDOM_TEST.batch(N_BATCH))\n",
        "__input = next(__batch)[0]\n",
        "__output = MODEL(__input)\n",
        "# sample predictions (inputs, outputs)\n",
        "IO['rd'] = (__input, __output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SIXwdQb1BPA"
      },
      "outputs": [],
      "source": [
        "# TOKENS ######################################################################\n",
        "\n",
        "# unique (G ^ i)-tokens\n",
        "for __lang, __sample in IO.items():\n",
        "    for __size in TOKENS:\n",
        "        TOKENS[__size][__lang] = tokun.pipeline.chunk(seq=tokun.pipeline.postprocess(__sample[0]), size=__size // 4, repeats=False)\n",
        "\n",
        "# unique tokens, for all languages\n",
        "for __size in TOKENS:\n",
        "    TOKENS[__size]['all'] = list(set(__t for _, __s in TOKENS[__size].items() for __t in __s))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuIEmJcDVk71"
      },
      "outputs": [],
      "source": [
        "# EMBEDDINGS ##################################################################\n",
        "\n",
        "for __depth, __size in enumerate(TOKEN_SIZES):\n",
        "    for __lang, __tokens in TOKENS[__size].items():\n",
        "        # re-encode without token repeats\n",
        "        __input = tokun.pipeline.preprocess(text=''.join(__tokens), token_size=math.prod(N_TOKEN_DIM), expand=SEQUENCE_AXIS * [1])\n",
        "        # UTF-32 embedding\n",
        "        __embedding = MODEL._encoder._encoder.layers[0](__input)\n",
        "        # iterative CNN tokenization\n",
        "        for __i in range(__depth + 1):\n",
        "            __embedding = MODEL._encoder._encoder.layers[__i + 1](__embedding)\n",
        "        # remove the (tokenized) padding\n",
        "        EMBEDDINGS[__size][__lang] = tf.squeeze(__embedding)[:len(__tokens)] # TODO squeeze?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GIVWLr2Rblq"
      },
      "outputs": [],
      "source": [
        "# NEIGHBORHOODS ###############################################################\n",
        "\n",
        "__unit = TOKEN_SIZES[-1]\n",
        "__count = 256\n",
        "\n",
        "TOKENS['local'] = {'all': []}\n",
        "EMBEDDINGS['local'] = {'all': []}\n",
        "\n",
        "for __lang, __tokens in TOKENS[__unit].items():\n",
        "    # stats on the embeddings for the current language\n",
        "    __std = tf.math.reduce_std(EMBEDDINGS[__unit][__lang], axis=0, keepdims=True)\n",
        "    __radius = 2. ** (3 - math.log(__unit, 4)) * tf.reduce_mean(__std).numpy()\n",
        "    # choose a single token\n",
        "    __t = tokun.pipeline.preprocess(text=random.choice(__tokens), token_size=math.prod(N_TOKEN_DIM), expand=SEQUENCE_AXIS * [1])\n",
        "    # encode it\n",
        "    __e = MODEL._encoder(__t)\n",
        "    # add noise to generate random neighbors\n",
        "    __n = tokun.evaluation.neighbors(point=__e, radius=__radius, count=__count)\n",
        "    # decode the noisy embeddings\n",
        "    __d = MODEL._decoder(__n)\n",
        "    # postprocess\n",
        "    __m = tokun.pipeline.chunk(seq=tokun.pipeline.postprocess(__d), size=__unit // 4, repeats=True)\n",
        "    # save\n",
        "    TOKENS['local']['all'].extend(__m)\n",
        "    EMBEDDINGS['local']['all'].append(tf.squeeze(__n))\n",
        "\n",
        "# merge all the embedding tensors\n",
        "EMBEDDINGS['local']['all'] = tf.concat(values=EMBEDDINGS['local']['all'], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzVnrexqVo5K"
      },
      "outputs": [],
      "source": [
        "# SAVE ########################################################################\n",
        "\n",
        "for __size in TOKENS:\n",
        "    mlable.data.write(data=[__c + ' ' + mlable.data.label(__c) for __c in TOKENS[__size]['all']][:8192], path='./metadata.' + str(__size) + '.label.tsv', tsv=False)\n",
        "    mlable.data.write(data=TOKENS[__size]['all'][:8192], path='./metadata.' + str(__size) + '.tsv', tsv=False)\n",
        "    mlable.data.write(data=EMBEDDINGS[__size]['all'].numpy()[:8192], path='./embeddings.' + str(__size) + '.tsv', tsv=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHtROW1K1R7c"
      },
      "source": [
        "## Dataviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiRVgG-oSfb7"
      },
      "outputs": [],
      "source": [
        "# DATA ########################################################################\n",
        "\n",
        "SAMPLES = [\n",
        "    \"\"\"위키백과, 우리 모두의 백과사전.\\nt-분포 확률적 임베딩(t-SNE)은 데이터의 차원 축소에 사용되는 기계 학습 알고리즘 중 하나로, 2002년 샘 로이스Sam Rowise와 제프리 힌튼에 의해 개발되었다.[1] t-SNE는 비선형 차원 축소 기법으로, 고차원 데이터를 특히 2, 3차원 등으로 줄여 가시화하는데에 유용하게 사용된다. 구체적으로 t-SNE는 비슷한 데이터는 근접한 2, 3차원의 지점으로, 다른 데이터는 멀리 떨어진 지점으로 맵핑한다.\"\"\",\n",
        "    \"\"\"class Encoder(tf.keras.models.Model):\\n    def __init__(self, depth: int, token_dim: int, encoding_dim: int, embedding_dim: int, latent_dim: int, batch_dim: int=None, attention: bool=False, **kwargs) -> None:\\n        super(Encoder, self).__init__(**kwargs)\\n        self._encoder = tf.keras.Sequential([\\n            tf.keras.Input(shape=(encoding_dim,), batch_size=batch_dim, name='input'), # (B * G ^ D, U)\\n            tf.keras.layers.Dense(units=embedding_dim, activation=None, use_bias=False, kernel_initializer='glorot_uniform', bias_initializer=None, name='embed-1'),] # (B * G ^ D, U) => (B * G ^ D, E)\\n            + [tokun.layers.TokenizeBlock(left_axis=-2, right_axis=-1, token_dim=token_dim, latent_dim=latent_dim, attention=attention, name='tokenize' + (__i + 1) * '-4') for __i in range(depth)]) # (B * G ^ i, E) => (B * G ^ (i-1), E)\\n\\n    def call(self, x: tf.Tensor) -> tf.Tensor:\\n        return self._encoder(x)\\n\"\"\",\n",
        "    \"\"\"class AutoEncoder(tf.keras.models.Model):\\n    def __init__(self, token_dim: int, encoding_dim: int, embedding_dim: int, latent_dim: int, batch_dim: int=None, **kwargs) -> None:\\n        super(AutoEncoder, self).__init__(**kwargs)\\n        self._encoder = Encoder(token_dim=token_dim, encoding_dim=encoding_dim, embedding_dim=embedding_dim, latent_dim=latent_dim, batch_dim=batch_dim)\\n        self._decoder = Decoder(token_dim=token_dim, encoding_dim=encoding_dim, embedding_dim=embedding_dim, latent_dim=latent_dim, batch_dim=batch_dim)\\n\\n    def call(self, x: tf.Tensor) -> tf.Tensor:\\n        return self._decoder(self._encoder(x))\"\"\",\n",
        "    \"\"\"class AutoEncoder(tf.keras.models.Model):\\n  def __init__(self, token_dim: int, encoding_dim: int, embedding_dim: int, latent_dim: int, batch_dim: int=None, **kwargs) -> None:\\n    super(AutoEncoder, self).__init__(**kwargs)\\n    self._encoder = Encoder(token_dim=token_dim, encoding_dim=encoding_dim, embedding_dim=embedding_dim, latent_dim=latent_dim, batch_dim=batch_dim)\\n    self._decoder = Decoder(token_dim=token_dim, encoding_dim=encoding_dim, embedding_dim=embedding_dim, latent_dim=latent_dim, batch_dim=batch_dim)\\n\\n  def call(self, x: tf.Tensor) -> tf.Tensor:\\n    return self._decoder(self._encoder(x))\"\"\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-iGpXb15m83"
      },
      "outputs": [],
      "source": [
        "# COMPUTE ######################################################################\n",
        "\n",
        "__i = 0\n",
        "__x, __e, __p, __y = tokun.pipeline.sample(model=MODEL, text=SAMPLES[__i], groups=N_TOKEN_DIM, expand=SEQUENCE_AXIS * [1], flatten=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKdaYXR5SyY9"
      },
      "outputs": [],
      "source": [
        "print('# INPUT ################################################################\\n\\n' + SAMPLES[__i])\n",
        "print('\\n# OUTPUT ###############################################################\\n\\n' + __y)\n",
        "print('\\n# SCORE ################################################################\\n\\n' + str(tokun.evaluation.compare(SAMPLES[__i], __y)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lK2XC4zLNco4"
      },
      "outputs": [],
      "source": [
        "# FROM DATASET ################################################################\n",
        "\n",
        "# compute\n",
        "__l = tokun.pipeline.postprocess(IO['de'][0])\n",
        "__r = tokun.pipeline.postprocess(IO['de'][1])\n",
        "\n",
        "# print\n",
        "print(__l)\n",
        "print(__r)\n",
        "print(tokun.evaluation.compare(__l, __r))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eJmv4xjnTH4t"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir .logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6KBZcFw8JXo"
      },
      "outputs": [],
      "source": [
        "print(tf.math.reduce_mean(EMBEDDINGS[TOKEN_SIZES[-1]]['en'], axis=0))\n",
        "print(tf.math.reduce_std(EMBEDDINGS[TOKEN_SIZES[-1]]['en'], axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ysd903RYTt8"
      },
      "outputs": [],
      "source": [
        "__std = tf.math.reduce_std(EMBEDDINGS[TOKEN_SIZES[-1]]['en'], axis=0)\n",
        "__noise = tf.random.normal(shape=(256,), mean=0., stddev=tf.math.reduce_mean(__std).numpy())\n",
        "\n",
        "__x = tokun.pipeline.preprocess(text='tokun to can tokens', token_size=math.prod(N_TOKEN_DIM), expand=SEQUENCE_AXIS * [1])\n",
        "__e = MODEL._encoder(__x)\n",
        "\n",
        "print(tokun.pipeline.postprocess(MODEL._decoder(__e)))\n",
        "print(tokun.pipeline.postprocess(MODEL._decoder(__e + 0.4 * __std)))\n",
        "print(tokun.pipeline.postprocess(MODEL._decoder(__e + 0.2 * __noise)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "__x = IO['en'][0]\n",
        "MODEL._encoder(__x)"
      ],
      "metadata": {
        "id": "AthSZ_luoWPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "66nzoc7m4uvd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}